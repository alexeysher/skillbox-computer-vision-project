{"cells":[{"cell_type":"markdown","id":"aae08bf2-ed8b-4294-a9eb-9b24bd47048c","metadata":{"id":"aae08bf2-ed8b-4294-a9eb-9b24bd47048c"},"source":["# Skillbox. Thesis on computer vision. Recognition of human emotions."]},{"cell_type":"markdown","id":"70cadafa-e59b-4bd4-a95d-d714314bc1e1","metadata":{"id":"70cadafa-e59b-4bd4-a95d-d714314bc1e1","tags":[]},"source":["## Settings"]},{"cell_type":"markdown","id":"c8c435e5-e282-4672-9beb-21a58c36c377","metadata":{"id":"c8c435e5-e282-4672-9beb-21a58c36c377","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Main"]},{"cell_type":"code","execution_count":null,"id":"57795e9e-b819-47ec-9299-00dbc3edc452","metadata":{"id":"57795e9e-b819-47ec-9299-00dbc3edc452"},"outputs":[],"source":["PROJECT_NAME = 'skillbox-computer-vision-project' # Project name\n","LOCAL_PROJ_PATH = f'D:/{PROJECT_NAME}' # Path to the project folder on the local computer\n","COLAB_PROJ_PATH = f'/content/{PROJECT_NAME}' # Path to the project folder in Google Colab session storage\n","LOCAL_GD_PROJ_PATH = f'G:/My Drive/{PROJECT_NAME}' # Path to the project folder on Google Drive on the local computer\n","COLAB_GD_PROJ_PATH = f'/content/drive/MyDrive/{PROJECT_NAME}' # Path to the project folder on Google Drive in Google Colab\n","TRAIN_DATASET_PATH = 'train' # Path to the original training dataset inside the project folder\n","TEST_DATASET_PATH = 'test_kaggle' # Path to the original test dataset inside the project folder\n","TRAIN_DATASET_URL = 'https://drive.google.com/file/d/1TG9P5B2k3eTbC4XDxDmEc07dyAORPC16/view?usp=sharing' # Link to the training dataset archive\n","TRAIN_DATASET_EXT = 'zip' # Type (file extension) of the training dataset archive\n","TEST_DATASET_URL = 'https://drive.google.com/file/d/12QrDrLT1F-X7UycvOoApXFqxTw3Zx93K/view?usp=sharing' # Link to the test dataset archive\n","TEST_DATASET_EXT = 'zip' # Type (file extension) of the test dataset archive\n","KAGGLE_API_TOKEN_URL = 'https://drive.google.com/file/d/1yS7Y5xFBxTYRPQd9sx1TudLWYYNOUGbu/view?usp=share_link' # Link to the token for connecting to the Kaggle platform via API\n","MAX_INFERENCE_TIME = .033 # Maximum allowed model inference time in seconds\n","INFERENCE_TIME_WEIGHT = .6 # Inference time weight when selecting a base model\n","FACE_DETECTOR_CLASSIFIER = 'haarcascade_frontalface_default.xml' # Trained XML classifier\n","FACE_DETECTOR_SCALE_FACTOR = 1.1 # Specify how much the image size is reduced at each image scale\n","FACE_DETECTOR_MIN_NEIGHBORS = 3 # Specify how many neighbors each candidate rectangle should have to retain it\n","import cv2 as cv\n","FACE_DETECTOR_FLAGS = cv.CASCADE_SCALE_IMAGE + cv.CASCADE_FIND_BIGGEST_OBJECT # Flags (It informs the classifier that the Haar features for detecting the face are applied to the image and It instructs the classifier to find the biggest face in the image)\n","FACE_DETECTOR_MIN_RATIO = 0.5 # Specify the minimum ratio of size of a face that we are expecting to detect to size of a image\n","BASE_MODEL_MAX_SIZE = 64 # Maximum allowed base model size in MB\n","BASE_MODEL_POOLINGS = 'avg' # Pooling type at the output of the base models ('avg' - average, 'max' - max)\n","MODEL_ON_TOP_DENSE_NUMS = [1, 2] # Options for the number of additional fully connected layers\n","MODEL_ON_TOP_DENSE_UNITS = [512, 1024] # Options for the number of output neurons in the additional fully connected layer\n","MODEL_ON_TOP_DROPOUT_RATES = [.0, .2] # Options for the proportion of data to drop before feeding into the fully connected layer during training\n","OPTIMIZER = 'Adam' # Name of the optimizer used to train the model\n","MODEL_ON_TOP_INITIAL_LEARNING_RATE = 1e-4 # Initial learning rate of the model on top\n","MODEL_ON_TOP_LEARNING_RATE_DECAY_RATE = 0.96 # The rate at which the learning rate of the model on top changes after each epoch\n","MODEL_INITIAL_LEARNING_RATE = 1e-5 # Initial learning rate of the model when fine-tuning\n","MODEL_LEARNING_RATE_DECAY_RATE = 0.96 # The rate at which the model's learning rate changes after each epoch when fine-tuning\n","RANDOM_FLIP = 'horizontal' # Type of random image flip\n","RANDOM_ZOOM = .2 # Maximum image zoom\n","RANDOM_ROTATION_FACTOR = .1 # Maximum image rotation (in fractions of a full rotation - 360Â°)\n","RANDOM_CONTRACT_FACTOR = .2 # Maximum contrast change (as a fraction of the original value)\n","RANDOM_BRIGHTNESS_FACTOR = .2 # Maximum brightness change (as a fraction of the original value)\n","SEED = 123 # Random number generator initializer\n","VERBOSE = 1 # Verbosity mode (0-quiet, 1-message output)"]},{"cell_type":"markdown","id":"c6b687cf-1660-4b61-9f55-8ad76741bd8f","metadata":{"id":"c6b687cf-1660-4b61-9f55-8ad76741bd8f","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Description of emotions"]},{"cell_type":"markdown","id":"ff8817dd-f674-4c78-aaa3-99ff7969ab66","metadata":{"id":"ff8817dd-f674-4c78-aaa3-99ff7969ab66"},"source":["If data is marked up using emotion ordinal numbers, the emotion names should be listed as a list or tuple.\n","\n","When using Valence-Arousal markup, the description of emotions should be presented as a dictionary. The dictionary keys should be the names of emotions. The dictionary values should be pairs of numbers characterizing the Valence and Arousal levels of emotions. Valence and Arousal levels should be numbers in the range from -1.0 to 1.0 inclusive.\n","\n","In both cases, the listing of emotions should be done in alphabetical order."]},{"cell_type":"code","execution_count":null,"id":"d2a25aa2-6a47-4240-8712-d6f1c8aed7ff","metadata":{"id":"d2a25aa2-6a47-4240-8712-d6f1c8aed7ff"},"outputs":[],"source":["EMOTIONS = (\n","    'anger', # anger, rage\n","    'contempt', # contempt\n","    'disgust', # disgust\n","    'fear', # fear\n","    'happy', # cheerful\n","    'neutral', # neutral\n","    'sad', # sadness\n","    'surprise', # astonishment\n","    'uncertain', # uncertainty\n",")"]},{"cell_type":"markdown","id":"9f34e881-22b1-4f68-a64e-65471d6f61bb","metadata":{"id":"9f34e881-22b1-4f68-a64e-65471d6f61bb","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### List of base models from Keras Applications with [reference data](https://keras.io/api/applications/):\n","- size in MB (Size (MB))\n","- prediction accuracy in % (Top-1 Accuracy)"]},{"cell_type":"code","execution_count":null,"id":"e747d29a-ef1f-4b39-a83e-218d8dfb88d0","metadata":{"id":"e747d29a-ef1f-4b39-a83e-218d8dfb88d0"},"outputs":[],"source":["KERAS_BASE_MODELS = {\n","    'MobileNet': (16, 70.40),\n","    'MobileNetV2': (14, 71.30),\n","    'NASNetMobile': (23, 74.40),\n","    'InceptionV3': (92, 77.90),\n","    'ResNet50V2': (98, 76.00),\n","    'EfficientNetB0': (29, 77.10),\n","    'ResNet50': (98, 74.90),\n","    'EfficientNetB1': (31, 79.10),\n","    'VGG16': (528, 71.30),\n","    'ResNet101V2': (171, 77.20),\n","    'DenseNet121': (33, 75.00),\n","    'EfficientNetB2': (36, 80.10),\n","    'VGG19': (549, 71.30),\n","    'ResNet101': (171, 76.40),\n","    'DenseNet169': (57, 76.20),\n","    'ResNet152V2': (232, 78.00),\n","    'Xception': (88, 79.00),\n","    'DenseNet201': (80, 77.30),\n","    'ResNet152': (232, 76.60),\n","    'InceptionResNetV2': (215, 80.30),\n","    'EfficientNetB3': (48, 81.60),\n","    'EfficientNetB4': (75, 82.90),\n","    'NASNetLarge': (343, 82.50),\n","    'EfficientNetB5': (118, 83.60),\n","    'EfficientNetB6': (166, 84.00),\n","    'EfficientNetB7': (256, 84.30),\n","    'EfficientNetV2B0': (29, 78.70),\n","    'EfficientNetV2B1': (34, 79.80),\n","    'EfficientNetV2B2': (42, 80.50),\n","    'EfficientNetV2B3': (59, 82.00),\n","    'EfficientNetV2S': (88, 83.90),\n","    'EfficientNetV2M': (220, 85.30),\n","    'EfficientNetV2L': (479, 85.70),\n","}"]},{"cell_type":"markdown","id":"18df7a45-badd-4a32-990e-7be0fb2e020d","metadata":{"id":"18df7a45-badd-4a32-990e-7be0fb2e020d","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Pipeline for gathering information about underlying models in Keras Applications"]},{"cell_type":"code","execution_count":null,"id":"c7a39ca3-5d46-43c7-a0fe-84e2efc7f893","metadata":{"id":"c7a39ca3-5d46-43c7-a0fe-84e2efc7f893"},"outputs":[],"source":["KERAS_BASE_MODELS_PROCESSING_PIPELINE = {\n","    'name': 'keras_base_models_processing',\n","    'description': 'A pipeline for collecting information about underlying models in Keras Applications',\n","    'report_csv': 'pipeline_base_models_processing.csv',\n","    'stages': [\n","        {\n","            'name': 'sizes_retrieving',\n","            'description': 'Getting information about the sizes of input images and feature vectors',\n","            'platform': 'colab', # Runs in Google Colab\n","            'params': {\n","               'result_csv': 'base_model_sizes.csv', # Path to the file with the selected models\n","            }\n","        },\n","        {\n","            'name': 'inference_time_measuring',\n","            'description': 'Measuring the inference time of models',\n","            'platform': 'colab', # Runs in Google Colab\n","            'params': {\n","                'batch_size': 1, # Batch size\n","                'batches': 1, # Number of batches in the dataset\n","                'repetitions': 100, # Number of repetitions\n","                'result_csv': 'model_inference_times.csv', # Path to the file with the selected models\n","            }\n","        },\n","        {\n","            'name': 'base_model_selection',\n","            'description': 'Selecting a base model',\n","            'platform': 'colab', # Runs in Google Colab\n","            'params': {\n","                'inference_time_weight': INFERENCE_TIME_WEIGHT, # Inference time weight when selecting a base model\n","                'top1_accuracy_weight': 1 - INFERENCE_TIME_WEIGHT, # Accuracy weight when selecting a base model\n","                'process_csv': 'base_model_selection.csv', # Path to file with base model selection process data\n","                'result_csv': 'base_model.csv', # Path to the file with the description of the selected base model\n","            }\n","        },\n","    ]\n","}"]},{"cell_type":"markdown","id":"dfdf7f1a-d8f3-48e7-80b0-7cf3af9d4c95","metadata":{"id":"dfdf7f1a-d8f3-48e7-80b0-7cf3af9d4c95","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Image Preprocessing Pipeline"]},{"cell_type":"code","execution_count":null,"id":"dbbc6f72-1b50-46cc-a8f6-3d9cad442b0c","metadata":{"id":"dbbc6f72-1b50-46cc-a8f6-3d9cad442b0c"},"outputs":[],"source":["IMAGE_PREPROCESSING_PIPELINE = {\n","    'name': 'image_preprocessing',\n","    'description': 'Image Preprocessing Pipeline',\n","    'report_csv': 'pipeline_images_preprocessing.csv',\n","    'stages':\n","    [\n","        {\n","            'name': 'train_face_extraction',\n","            'description': 'Extracting face images from training dataset',\n","            'platform': 'colab', # Runs on the local computer\n","            'params': {\n","                'path': 'train_faces', # Path to the training dataset folder with face images\n","                'classifier': FACE_DETECTOR_CLASSIFIER, # Trained XML classifier\n","                'scale_factor': FACE_DETECTOR_SCALE_FACTOR, # Specify how much the image size is reduced at each image scale\n","                'min_neighbors': FACE_DETECTOR_MIN_NEIGHBORS, # Specify how many neighbors each candidate rectangle should have to retain it\n","                'flags': FACE_DETECTOR_FLAGS, # Flags\n","                'face_min_ratio': FACE_DETECTOR_MIN_RATIO, # Specify the minimum size of a face that we are expecting to detect\n","                'process_csv': 'train_face_extraction_process.csv', # Path to file with detailed information\n","                'result_csv': 'train_face_extraction.csv', # Path to the results file\n","            },\n","        },\n","        {\n","            'name': 'test_face_extraction',\n","            'description': 'Extracting face images from test dataset',\n","            'platform': 'colab', # Runs on the local computer\n","            'params': {\n","                'path': 'test_faces', # Path to the training dataset folder with face images\n","                'classifier': FACE_DETECTOR_CLASSIFIER, # Trained XML classifier\n","                'scale_factor': FACE_DETECTOR_SCALE_FACTOR, # Specify how much the image size is reduced at each image scale\n","                'min_neighbors': FACE_DETECTOR_MIN_NEIGHBORS, # Specify how many neighbors each candidate rectangle should have to retain it\n","                'flags': FACE_DETECTOR_FLAGS, # Flags\n","                'face_min_ratio': FACE_DETECTOR_MIN_RATIO, # Specify the minimum size of a face that we are expecting to detect\n","                'process_csv': 'test_face_extraction_process.csv', # Path to file with detailed information\n","                'result_csv': 'test_face_extraction.csv', # Path to the results file\n","            },\n","        },\n","        {\n","            'name': 'train_face_feature_extraction',\n","            'description': 'Extracting features from training fadataset',\n","            'platform': 'colab', # Runs in Google Colab\n","            'params': {\n","                'path': 'train_features', # Path to the folder with the batch files of extracted features\n","                'batch_size': 64, # Batch size\n","                'buffer_size': 10, # Buffer size\n","            }\n","        },\n","        {\n","            'name': 'test_face_feature_extraction',\n","            'description': 'Extracting features from a test dataset',\n","            'platform': 'colab', # Runs in Google Colab\n","            'params': {\n","                'path': 'test_features', # Path to the folder with batch files of extracted features\n","                'batch_size': 64, # Batch size\n","                'buffer_size': 10, # Buffer size\n","            }\n","        },\n","        {\n","            'name': 'train_cleaning',\n","            'description': 'Additional cleaning of the training dataset',\n","            'platform': 'colab', # Runs on the local computer\n","            'params': {\n","                'features_path': 'train_clean_features', # Path to the training dataset folder with face images\n","                'dataset_path': 'train_clean_faces', # Path to the folder of the cleaned training dataset\n","                'process_csv': 'train_cleaning_process.csv', # Path to file with detailed information\n","                'result_csv': 'train_cleaning.csv', # Name of the file with results\n","            },\n","        },\n","    ]\n","}"]},{"cell_type":"markdown","id":"c90b4edc-a3d9-4848-a510-610045255406","metadata":{"id":"c90b4edc-a3d9-4848-a510-610045255406","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Model creation pipeline"]},{"cell_type":"code","execution_count":null,"id":"237aed04-8759-40aa-b216-cdb0c60be812","metadata":{"id":"237aed04-8759-40aa-b216-cdb0c60be812"},"outputs":[],"source":["MODEL_BUILDING_PIPELINE = {\n","    'name': 'model_building',\n","    'description': 'Model creation pipeline',\n","    'report_csv': 'pipeline_model_building.csv',\n","    'stages': [\n","        {\n","            'name': 'model_on_top_selection',\n","            'description': 'Selecting the best model on top',\n","            'platform': 'colab', # Runs on the local computer\n","            'params': {\n","                'path': 'model_on_top_selection', # Path to the folder with logs and weights of the model on top\n","                'batch_size': 64, # Batch size\n","                'optimizer_name': OPTIMIZER, # Optimizer,\n","                'initial_learning_rate': MODEL_ON_TOP_INITIAL_LEARNING_RATE, # Initial learning rate\n","                'learning_rate_decay_rate': MODEL_ON_TOP_LEARNING_RATE_DECAY_RATE, # Learning rate decay rate\n","                'epochs': 20, # Number of epochs when measuring inference time\n","                'patience': 3, # Max epochs without accuracy improvement\n","                'process_csv': 'model_on_top_selection.csv', # Path with model training results\n","                'result_csv': 'selected_model_on_top.csv', # Path to file with description of selected base model\n","            }\n","        },\n","        {\n","            'name': 'model_fine_tuning',\n","            'description': 'Fine-tuning the model',\n","            'platform': 'colab', # Runs in Google Colab\n","            'params': {\n","                'path': 'model_fine_tuning', # Path to the folder with logs and weights of the model on top\n","                'flip': RANDOM_FLIP, # Randomly flip the image\n","                'rotation_factor': RANDOM_ROTATION_FACTOR, # Random rotation factor (counter-clockwise or clockwise) of the image during augmentation, fraction of 360Â°\n","                'zoom_factor': RANDOM_ZOOM, # Factor of random zooming in or out of the image during augmentation\n","                'contrast_factor': RANDOM_CONTRACT_FACTOR, # Factor for randomly changing image contrast\n","                'brightness_factor': RANDOM_BRIGHTNESS_FACTOR, # Factor for randomly changing the brightness of an image\n","                'batch_size': 32, # Batch size\n","                'buffer_size': 100, # Buffer size\n","                'optimizer_name': OPTIMIZER, # Optimizer\n","                'initial_learning_rate': MODEL_INITIAL_LEARNING_RATE, # Initial learning rate\n","                'learning_rate_decay_rate': MODEL_LEARNING_RATE_DECAY_RATE, # Learning rate decay rate\n","                'epochs': 50, # Number of epochs to learn\n","                'epochs_per_run': 10, # Number of training epochs per run\n","                'patience': 3, # Max epochs without accuracy improvement\n","                'process_csv': 'model_fine_tuning.csv', # Path to file with model fine tuning process data\n","                'result_csv': 'tuned_model.csv', # Path to file with resulting model validation\n","            }\n","        },\n","        {\n","            'name': 'model_test',\n","            'description': 'Testing the model',\n","            'platform': 'colab', # Runs in Google Colab\n","            'params': {\n","                'path': 'model_test',\n","                'batch_size': 32, # Batch size\n","                'buffer_size': 100, # Buffer size\n","                'process_csv': 'test_prediction.csv', # Path to file with prediction data\n","                'result_csv': 'test_scoring.csv', # Path to file with resulting model score\n","            }\n","        },\n","    ]\n","}"]},{"cell_type":"markdown","id":"7679fe0c-42a1-4daa-9fd9-f7a85a68fe3c","metadata":{"id":"7679fe0c-42a1-4daa-9fd9-f7a85a68fe3c","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["## Preparation"]},{"cell_type":"markdown","id":"dc104087-26ee-412e-a6b6-b1722688198c","metadata":{"id":"dc104087-26ee-412e-a6b6-b1722688198c","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Determining the platform on which the notebook is running (local computer or Google Colab)"]},{"cell_type":"code","execution_count":null,"id":"6c767ade-0d37-4373-9779-2fef2f3410fa","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1743952726724,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"6c767ade-0d37-4373-9779-2fef2f3410fa","outputId":"5c1f5f99-6aaf-4c6d-ec8f-3e06e283120c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Notebook is running on Google Colab.\n"]}],"source":["if 'google.colab' in str(get_ipython()):\n","    platform = 'colab'\n","    print('Notebook is running on Google Colab.')\n","else:\n","    platform = 'local'\n","    print(f'Notebook is running locally.')"]},{"cell_type":"markdown","id":"ef68c4e6-28af-4c55-aaa5-b3d87e9b7b6e","metadata":{"id":"ef68c4e6-28af-4c55-aaa5-b3d87e9b7b6e"},"source":["### Installing and loading the necessary libraries"]},{"cell_type":"markdown","id":"f58b030a-5149-45c5-a662-9e45c0156485","metadata":{"id":"f58b030a-5149-45c5-a662-9e45c0156485"},"source":["#### Installing required libraries if they are missing"]},{"cell_type":"code","execution_count":null,"id":"1aae3472-cf9d-47e5-9113-068587c1fae9","metadata":{"id":"1aae3472-cf9d-47e5-9113-068587c1fae9","scrolled":true},"outputs":[],"source":["from importlib.util import find_spec\n","\n","# List of packages\n","packages = [\n","    'validators',\n","    'ipyparallel',\n","    'tqdm',\n","    'numpy',\n","    'pandas',\n","    'gdown',\n","    'matplotlib',\n","    ('scikit-learn', 'sklearn'),\n","    'kaggle',\n","    ('opencv-python', 'cv2')\n","]\n","if platform == 'local':\n","    packages.append(('tensorflow-cpu', 'tensorflow'))\n","    packages.append('ipywidgets')\n","else:\n","    packages.append(('tensorflow-gpu', 'tensorflow'))\n","\n","# Installing packages\n","for package in packages:\n","    if isinstance(package, str):\n","        space = package\n","    else:\n","        package, space = package\n","    if not find_spec(space):\n","        print(f'Installing {package}...')\n","        !pip install {package}"]},{"cell_type":"markdown","id":"b56b4b1d-6d0e-4f7c-9970-b9c8dde5428a","metadata":{"id":"b56b4b1d-6d0e-4f7c-9970-b9c8dde5428a"},"source":["#### Importing required libraries"]},{"cell_type":"code","execution_count":null,"id":"ff8fffb7-ddc5-4460-8c89-fed1bf19fc5d","metadata":{"id":"ff8fffb7-ddc5-4460-8c89-fed1bf19fc5d"},"outputs":[],"source":["from typing import Optional, Union, Tuple, List, Dict\n","import inspect\n","import itertools\n","import validators\n","from time import sleep\n","from datetime import datetime\n","from timeit import timeit\n","from pathlib import Path\n","import shutil\n","import gdown\n","import ipyparallel as ipp\n","from tqdm.notebook import tqdm, trange\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import cv2 as cv\n","from sklearn.metrics.pairwise import cosine_similarity\n","from copy import deepcopy\n","from PIL import Image, ImageOps\n","import tensorflow as tf\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","from tensorflow.keras import models, layers, activations, optimizers, metrics, losses, callbacks, utils, applications, initializers\n","if platform == 'colab':\n","    from psutil import virtual_memory\n","    from google.colab import output\n","    output.enable_custom_widget_manager()"]},{"cell_type":"markdown","id":"8983f9af-389b-4256-9b6b-69e039729cbc","metadata":{"id":"8983f9af-389b-4256-9b6b-69e039729cbc"},"source":["#### Importing required extensions"]},{"cell_type":"code","execution_count":null,"id":"JYwWF9NlmZBg","metadata":{"id":"JYwWF9NlmZBg"},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"markdown","id":"358bc4a2-69c7-4da7-b901-d6db50a6ed28","metadata":{"id":"358bc4a2-69c7-4da7-b901-d6db50a6ed28","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Connecting Google Drive when running in Google Colab"]},{"cell_type":"code","execution_count":null,"id":"e84caef0-66ae-4807-a6d4-2a4861ec474b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1490,"status":"ok","timestamp":1743952732947,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"e84caef0-66ae-4807-a6d4-2a4861ec474b","outputId":"6320b900-5e2d-4075-e186-996f335376b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["if platform == 'colab':\n","    from google.colab import drive\n","    drive.mount('/content/drive')"]},{"cell_type":"markdown","id":"93df5dcf-949a-45a8-8a05-6fe9833fa6dc","metadata":{"id":"93df5dcf-949a-45a8-8a05-6fe9833fa6dc","tags":[]},"source":["### Checking basic settings"]},{"cell_type":"code","execution_count":null,"id":"06ae989b-463d-47d9-96ea-299696daed67","metadata":{"id":"06ae989b-463d-47d9-96ea-299696daed67"},"outputs":[],"source":["assert isinstance(EMOTIONS, (list, tuple)) or isinstance(EMOTIONS, dict), 'Emotions must be a list or tuple of emotion names, or a dictionary whose keys are emotion names and whose values ââare value-arousal pairs.'\n","assert len(EMOTIONS) > 1, 'The number of emotions must be greater than 1.'\n","assert all([isinstance(emotion, str) for emotion in EMOTIONS]), 'Emotion names must be strings.'\n","if isinstance(EMOTIONS, dict):\n","    assert all(isinstance(value, (list, tuple)) for value in EMOTIONS.values()), 'Values ââvalue-arousal must be specified as a list or tuple of numbers.'\n","    assert all((isinstance(x, (int, float)) for x in value) for value in EMOTIONS.values()), 'Values ââof value-arousal must be numbers.'\n","    assert all(len(value)==2 for value in EMOTIONS.values()), 'There must be two elements in the list or tuple of values ââvalue-arousal.'"]},{"cell_type":"code","execution_count":null,"id":"af7b0c6b-d74d-44ff-97bb-fd62e25fd6a0","metadata":{"id":"af7b0c6b-d74d-44ff-97bb-fd62e25fd6a0"},"outputs":[],"source":["assert isinstance(PROJECT_NAME, str), 'Project name must be a string.'\n","assert PROJECT_NAME != '', 'Project name cannot be an empty string.'"]},{"cell_type":"code","execution_count":null,"id":"24184f14-ee28-4a83-955d-c5d69c2d5063","metadata":{"id":"24184f14-ee28-4a83-955d-c5d69c2d5063"},"outputs":[],"source":["if platform == 'colab':\n","    PROJ_PATH = COLAB_PROJ_PATH\n","else:\n","    PROJ_PATH = LOCAL_PROJ_PATH\n","assert isinstance(PROJ_PATH, str), 'The project folder path must be a string.'\n","proj_path = Path(PROJ_PATH)\n","assert Path(proj_path).parent.exists(), 'There is no folder to place the project folder.'"]},{"cell_type":"code","execution_count":null,"id":"f708e46c-9199-4715-b011-b44382703a18","metadata":{"id":"f708e46c-9199-4715-b011-b44382703a18"},"outputs":[],"source":["if platform == 'colab':\n","    GD_PROJ_PATH = COLAB_GD_PROJ_PATH\n","else:\n","    GD_PROJ_PATH = LOCAL_GD_PROJ_PATH\n","assert isinstance(GD_PROJ_PATH, str), 'The path to the project folder on Google Drive must be a string.'\n","gd_proj_path = Path(GD_PROJ_PATH)\n","assert Path(gd_proj_path).parent.exists(), 'There is no folder on Google Drive to contain the project folder.'"]},{"cell_type":"code","execution_count":null,"id":"450ab112-cdb2-4194-812e-345d65540c8b","metadata":{"id":"450ab112-cdb2-4194-812e-345d65540c8b"},"outputs":[],"source":["assert isinstance(TRAIN_DATASET_URL, str), 'The training dataset archive URL must be a string.'\n","assert validators.url(TRAIN_DATASET_URL), 'The training dataset archive link is in an invalid format.'"]},{"cell_type":"code","execution_count":null,"id":"71aaf83d-bae2-400e-97b0-3ab8618411c8","metadata":{"id":"71aaf83d-bae2-400e-97b0-3ab8618411c8"},"outputs":[],"source":["assert TRAIN_DATASET_EXT in (\"zip\", \"tar\", \"tar.gz\", \"tgz\", \"tar.bz2\", \"tbz\"), \\\n","'The archive file of the original training dataset must have any of the following extensions: *.zip, *.tar, *.tar.gz, *.tgz, *.tar.bz2, *.tbz'"]},{"cell_type":"code","execution_count":null,"id":"7cafca1b-d40d-43c2-8850-824f44ae6e66","metadata":{"id":"7cafca1b-d40d-43c2-8850-824f44ae6e66"},"outputs":[],"source":["assert isinstance(TRAIN_DATASET_PATH, str), 'The path to the original training dataset inside the project folder must be a string.'\n","try:\n","    Path(TRAIN_DATASET_PATH).exists()\n","    valid = True\n","except OSError as e:\n","    valid = False\n","assert valid, 'Syntax error in path to source training dataset inside project folder.'"]},{"cell_type":"code","execution_count":null,"id":"15b4fb08-e47e-44cb-b61c-0d7060a7257f","metadata":{"id":"15b4fb08-e47e-44cb-b61c-0d7060a7257f"},"outputs":[],"source":["assert isinstance(TEST_DATASET_URL, str), 'The link to the test dataset archive must be specified as a string.'\n","assert validators.url(TEST_DATASET_URL), 'The link to the test dataset archive has an invalid format.'"]},{"cell_type":"code","execution_count":null,"id":"07ac2047-420f-4f37-b77c-5ecaaeac85e2","metadata":{"id":"07ac2047-420f-4f37-b77c-5ecaaeac85e2"},"outputs":[],"source":["assert TEST_DATASET_EXT in (\"zip\", \"tar\", \"tar.gz\", \"tgz\", \"tar.bz2\", \"tbz\"), \\\n","'The archive file of the original test dataset must have any of the following extensions: *.zip, *.tar, *.tar.gz, *.tgz, *.tar.bz2, *.tbz'"]},{"cell_type":"code","execution_count":null,"id":"39d92b78-ac57-4cfa-9b09-c81a96e3e474","metadata":{"id":"39d92b78-ac57-4cfa-9b09-c81a96e3e474"},"outputs":[],"source":["assert isinstance(TEST_DATASET_PATH, str), 'The path to the original test dataset inside the project folder must be a string.'\n","try:\n","    Path(TEST_DATASET_PATH).exists()\n","    valid = True\n","except OSError as e:\n","    valid = False\n","assert valid, 'Syntax error in path to source test dataset inside project folder.'"]},{"cell_type":"code","execution_count":null,"id":"6b728a8c-dfff-4256-80a8-a7d1e5772015","metadata":{"id":"6b728a8c-dfff-4256-80a8-a7d1e5772015"},"outputs":[],"source":["assert isinstance(KAGGLE_API_TOKEN_URL, str), 'The token URL for connecting to the Kaggle platform via API must be specified as a string.'\n","assert validators.url(KAGGLE_API_TOKEN_URL), 'The token link for connecting to the Kaggle platform via API is not in the correct format.'"]},{"cell_type":"code","execution_count":null,"id":"26ee57b1-09be-46a2-960f-38311313114f","metadata":{"id":"26ee57b1-09be-46a2-960f-38311313114f"},"outputs":[],"source":["assert isinstance(MAX_INFERENCE_TIME, (int, float)), 'The maximum allowed model inference time in seconds must be a floating point number.'\n","assert MAX_INFERENCE_TIME > 0., 'The maximum allowed model inference time in seconds must be a positive floating point number.'"]},{"cell_type":"code","execution_count":null,"id":"ea760826-1f8c-4039-8aad-01dd248a2f7f","metadata":{"id":"ea760826-1f8c-4039-8aad-01dd248a2f7f"},"outputs":[],"source":["assert isinstance(INFERENCE_TIME_WEIGHT, (int, float)), 'The inference time weight of a model when selecting a base model must be a number.'\n","assert INFERENCE_TIME_WEIGHT >= 0.0 and INFERENCE_TIME_WEIGHT <= 1.0, 'The inference time weight when selecting the base model must be in the range [0., 1.].'"]},{"cell_type":"code","execution_count":null,"id":"7f2cbe7c-da10-469d-9671-80bf5631f130","metadata":{"id":"7f2cbe7c-da10-469d-9671-80bf5631f130"},"outputs":[],"source":["assert isinstance(BASE_MODEL_MAX_SIZE, (int, float)), 'The maximum allowed base model size in MB must be a number.'\n","assert BASE_MODEL_MAX_SIZE > 0.0, 'The maximum allowed base model size in MB must be a positive number.'"]},{"cell_type":"code","execution_count":null,"id":"7bcfee4d-6f36-4668-9fc9-0f6b3990b8e2","metadata":{"id":"7bcfee4d-6f36-4668-9fc9-0f6b3990b8e2"},"outputs":[],"source":["assert BASE_MODEL_POOLINGS in ('avg', 'max'), 'The pooling type at the base model output must be either \"avg\" (average) or \"max\" (max).'"]},{"cell_type":"code","execution_count":null,"id":"608ae2e3-74b3-48fd-a8c4-c1c7797f4888","metadata":{"id":"608ae2e3-74b3-48fd-a8c4-c1c7797f4888"},"outputs":[],"source":["assert isinstance(MODEL_ON_TOP_DENSE_NUMS, (list, tuple)), 'Options for the number of additional fully connected layers must be specified as a list or tuple.'\n","assert all(isinstance(num, int) for num in MODEL_ON_TOP_DENSE_NUMS), 'The number of additional fully connected layers must be an integer.'\n","assert all(num > 0 for num in MODEL_ON_TOP_DENSE_NUMS), 'The number of additional fully connected layers must be a positive number.'\n","assert len(MODEL_ON_TOP_DENSE_NUMS) >= 1, 'At least one option for the number of additional fully connected layers must be specified.'"]},{"cell_type":"code","execution_count":null,"id":"85809e66-a7c2-4551-b794-9e30b0c5fc8a","metadata":{"id":"85809e66-a7c2-4551-b794-9e30b0c5fc8a"},"outputs":[],"source":["assert isinstance(MODEL_ON_TOP_DENSE_UNITS, (list, tuple)), 'The options for the number of output neurons in the additional fully connected layer must be specified by a list or a tuple.'\n","assert all(isinstance(num, int) for num in MODEL_ON_TOP_DENSE_UNITS), 'The number of output neurons in the additional fully connected layer must be an integer.'\n","assert all(num > 0 for num in MODEL_ON_TOP_DENSE_UNITS), 'The number of output neurons in the additional fully connected layer must be a positive number.'\n","assert len(MODEL_ON_TOP_DENSE_UNITS) >= 1, 'At least one option for the number of output neurons in the additional fully connected layer must be specified.'"]},{"cell_type":"code","execution_count":null,"id":"9ff5b360-21a9-42b4-9bb9-f74981bd1c6b","metadata":{"id":"9ff5b360-21a9-42b4-9bb9-f74981bd1c6b"},"outputs":[],"source":["assert isinstance(MODEL_ON_TOP_DROPOUT_RATES, (list, tuple)), 'The rates of data to drop out before feeding into the fully connected layer during training must be specified as a list or tuple.'\n","assert all(isinstance(num, float) for num in MODEL_ON_TOP_DROPOUT_RATES), 'The dropout rates before feeding into the fully connected layer during training must be an integer.'\n","assert all((num >= 0. and num < 1.0) for num in MODEL_ON_TOP_DROPOUT_RATES), 'The rates of data dropped before feeding into the fully connected layer during training must be a positive number.'\n","assert len(MODEL_ON_TOP_DROPOUT_RATES) >= 1, 'At least one dropout rate must be specified before feeding into the fully connected layer during training.'"]},{"cell_type":"code","execution_count":null,"id":"24dfb3ca-2d6e-4239-8410-abdeeeec8958","metadata":{"id":"24dfb3ca-2d6e-4239-8410-abdeeeec8958"},"outputs":[],"source":["assert isinstance(MODEL_ON_TOP_INITIAL_LEARNING_RATE, float), 'The initial learning rate of the model on top must be a number.'\n","assert MODEL_ON_TOP_INITIAL_LEARNING_RATE > 0.0, 'The inference time weight when selecting the base model must be a positive number.'"]},{"cell_type":"code","execution_count":null,"id":"b4d02361-a617-41cb-989f-faaab463c0e8","metadata":{"id":"b4d02361-a617-41cb-989f-faaab463c0e8"},"outputs":[],"source":["assert isinstance(MODEL_ON_TOP_LEARNING_RATE_DECAY_RATE, float), 'The rate at which the model on top\\'s learning rate changes after each epoch must be a number.'\n","assert MODEL_ON_TOP_LEARNING_RATE_DECAY_RATE > 0.0, 'The rate at which the learning rate of the model on top changes after each epoch must be a positive number.'"]},{"cell_type":"code","execution_count":null,"id":"82edd369-8a24-4779-b1da-e3bece81cf1b","metadata":{"id":"82edd369-8a24-4779-b1da-e3bece81cf1b"},"outputs":[],"source":["assert isinstance(MODEL_INITIAL_LEARNING_RATE, float), 'The initial learning rate of the model during fine-tuning must be a number.'\n","assert MODEL_INITIAL_LEARNING_RATE > 0.0, 'The initial learning rate of the model during fine-tuning must be a positive number.'"]},{"cell_type":"code","execution_count":null,"id":"c10fe2c4-f4eb-4de3-81c3-4fc38e79bc4a","metadata":{"id":"c10fe2c4-f4eb-4de3-81c3-4fc38e79bc4a"},"outputs":[],"source":["assert isinstance(MODEL_LEARNING_RATE_DECAY_RATE, float), 'The rate by which the model\\'s learning rate changes after each epoch during fine-tuning must be a number.'\n","assert MODEL_LEARNING_RATE_DECAY_RATE > 0.0, 'The rate at which the model\\'s learning rate changes after each epoch during fine-tuning must be a positive number.'"]},{"cell_type":"code","execution_count":null,"id":"b0004e10-7733-48e4-98d1-5023e47adcb0","metadata":{"id":"b0004e10-7733-48e4-98d1-5023e47adcb0"},"outputs":[],"source":["assert hasattr(optimizers, OPTIMIZER), f'Optimizer {OPTIMIZER} is not present in the Keras library.'"]},{"cell_type":"code","execution_count":null,"id":"db91a82f-2aae-4d5c-b031-75df5aef11f2","metadata":{"id":"db91a82f-2aae-4d5c-b031-75df5aef11f2"},"outputs":[],"source":["assert VERBOSE in (0, 1), f'The verbosity mode {VERBOSE} is specified incorrectly. The value must be 0 (quiet) or 1 (message output).'"]},{"cell_type":"code","execution_count":null,"id":"a62d0c44-dbc3-41b4-b608-1b50951316ff","metadata":{"id":"a62d0c44-dbc3-41b4-b608-1b50951316ff"},"outputs":[],"source":["assert isinstance(SEED, int) or SEED is None, 'The random number generator initializer must be an integer or None.'"]},{"cell_type":"markdown","id":"30284640-db8e-4595-8406-078ce392c1ce","metadata":{"id":"30284640-db8e-4595-8406-078ce392c1ce","tags":[]},"source":["### Preparing for the first launch"]},{"cell_type":"markdown","id":"19065f4c-9f9f-43f5-82ff-91445b07886a","metadata":{"id":"19065f4c-9f9f-43f5-82ff-91445b07886a"},"source":["#### Creating a project folder"]},{"cell_type":"code","execution_count":null,"id":"87612d3e-61f7-4ff3-b940-a137c3837a9a","metadata":{"id":"87612d3e-61f7-4ff3-b940-a137c3837a9a"},"outputs":[],"source":["if not proj_path.exists():\n","    proj_path.mkdir()"]},{"cell_type":"markdown","id":"ab454133-6e9b-49be-bad9-28f5c4d0202b","metadata":{"id":"ab454133-6e9b-49be-bad9-28f5c4d0202b"},"source":["#### Creating a Project Folder on Google Drive"]},{"cell_type":"code","execution_count":null,"id":"-WoKoo_3oQ8s","metadata":{"id":"-WoKoo_3oQ8s"},"outputs":[],"source":["if not gd_proj_path.exists():\n","    gd_proj_path.mkdir()"]},{"cell_type":"markdown","id":"30606fd6-a486-4d41-84c1-4b52386be9ad","metadata":{"id":"30606fd6-a486-4d41-84c1-4b52386be9ad"},"source":["#### Copying kaggle token"]},{"cell_type":"code","execution_count":null,"id":"eCNWlRtR2ne8","metadata":{"id":"eCNWlRtR2ne8"},"outputs":[],"source":["if platform == 'colab':\n","    kaggle_path = Path('/root/.kaggle/kaggle.json')\n","else:\n","    kaggle_path = Path('.kaggle/kaggle.json')\n","if not kaggle_path.parent.exists():\n","    kaggle_path.parent.mkdir()\n","if not kaggle_path.exists():\n","    gdown.download(KAGGLE_API_TOKEN_URL, kaggle_path.as_posix(), fuzzy=True)"]},{"cell_type":"markdown","id":"1b21d034-8b1f-4f10-a4cd-15838e29dec0","metadata":{"id":"1b21d034-8b1f-4f10-a4cd-15838e29dec0","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Setting the project folder as the working directory"]},{"cell_type":"code","execution_count":null,"id":"GolLrxTPMDBc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1743952733107,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"GolLrxTPMDBc","outputId":"91f1899e-bc11-49ce-dcc2-b78f720a323c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/skillbox-computer-vision-project\n"]}],"source":["%cd {proj_path}"]},{"cell_type":"markdown","id":"677f97c5-6cee-4267-9987-d42c91c15b9a","metadata":{"id":"677f97c5-6cee-4267-9987-d42c91c15b9a","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Displaying information about the dedicated GPU and available virtual memory in Google Colab"]},{"cell_type":"code","execution_count":null,"id":"3f4ec89b-3e52-41ce-9155-b8bfe7edd90b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92,"status":"ok","timestamp":1743952733188,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"3f4ec89b-3e52-41ce-9155-b8bfe7edd90b","outputId":"05732e32-aa34-4c42-b3f0-c79848f71492"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Apr  6 15:18:53 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   54C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n","\n","Your runtime has 13.3 gigabytes of available RAM\n"]}],"source":["if platform == 'colab':\n","    gpu_info = !nvidia-smi\n","    gpu_info = '\\n'.join(gpu_info)\n","    if gpu_info.find('failed') >= 0:\n","        print('Not connected to a GPU')\n","    else:\n","        print(gpu_info)\n","    ram_gb = virtual_memory().total / 1.024e9\n","    print(f'\\nYour runtime has {ram_gb:.1f} gigabytes of available RAM')"]},{"cell_type":"markdown","id":"227088bf-3158-4743-9576-4309423063c8","metadata":{"id":"227088bf-3158-4743-9576-4309423063c8","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["## Helper functions and classes"]},{"cell_type":"markdown","id":"f23cbf0c-fccb-4ea8-b5c2-13691425e855","metadata":{"id":"f23cbf0c-fccb-4ea8-b5c2-13691425e855","tags":[]},"source":["### Building and training models"]},{"cell_type":"markdown","id":"697900d4-4b99-462d-bb60-7e769741ebf1","metadata":{"id":"697900d4-4b99-462d-bb60-7e769741ebf1","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["#### Building an augmentation model"]},{"cell_type":"code","execution_count":null,"id":"f99810cc-0b65-471d-b27f-e838307daa91","metadata":{"id":"f99810cc-0b65-471d-b27f-e838307daa91"},"outputs":[],"source":["def build_augment_model(image_size: int,\n","                        flip: Optional[str]=None,\n","                        rotation_factor: Optional[float]=None,\n","                        zoom_factor: Optional[float]=None,\n","                        contrast_factor: Optional[float]=None,\n","                        brightness_factor: Optional[float]=None,\n","                        training: bool=False,\n","                        seed=SEED) -> models.Model:\n","    '''Creates an augmentation model of a square input image. Augmentation is achieved by randomly mirroring, rotating, scaling, changing the contrast and brightness of the original image.\n","    The model works only in inference mode and only during training, provided that the training flag is set.\n","\n","    Arguments:\n","    - image_size: image size,\n","    - flip: mirroring type: 'horizonal', 'vertical', if None, mirroring is not performed.\n","    - rotation_factor: the value of the rotation angle (clockwise or counterclockwise) of the image in fractions of a full rotation; if None, then no rotation is performed.\n","    - zoom_factor: the maximum change (increase or decrease) of the image in fractions of the original size; if None, the scale does not change.\n","    - contrast_factor: the maximum change (increase or decrease) in image contrast as a fraction of the original contrast; if None, the contrast does not change.\n","    - brightness_factor: the value of the maximum change (increase or decrease) in the image brightness as a fraction of the original brightness; if None, the brightness does not change.\n","    - training: if False, the model does not work, if True, the model works during training.\n","    - seed: random number generator initializer.'''\n","    i = layers.Input(shape=(image_size, image_size, 3), name='original_image_input')\n","    x = i\n","    if flip is not None:\n","        x = layers.RandomFlip(flip, seed=seed, name='random_flip')(x, training=training)\n","    if rotation_factor is not None:\n","        x = layers.RandomRotation(rotation_factor, seed=seed, name='random_rotation')(x, training=training)\n","    if zoom_factor is not None:\n","        x = layers.RandomZoom(zoom_factor, seed=seed, name='random_zoom')(x, training=training)\n","    if contrast_factor is not None:\n","        x = layers.RandomContrast(contrast_factor, seed=seed, name='random_contrast')(x, training=training)\n","    if brightness_factor is not None:\n","        x = layers.RandomBrightness(brightness_factor, seed=seed, name='random_brightness')(x, training=training)\n","    o = x\n","    model = models.Model(inputs=[i], outputs=[o], name='augmentation_model')\n","    return model"]},{"cell_type":"markdown","id":"cb7e54b8-4425-469c-a758-ca804591944f","metadata":{"id":"cb7e54b8-4425-469c-a758-ca804591944f","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["#### Building a Basic Model"]},{"cell_type":"code","execution_count":null,"id":"8317594b-b9ad-4031-9d00-9c53c724c305","metadata":{"id":"8317594b-b9ad-4031-9d00-9c53c724c305"},"outputs":[],"source":["def build_base_model(name: str,\n","                     weights: Optional[str]='imagenet',\n","                     image_size: Optional[int]=None,\n","                     pooling: str='avg',\n","                     include_preprocess_input:bool=False,\n","                     training: bool=False) -> models.Model:\n","    '''Creates a basic model from the Keras Applications library. The model processes a square-shaped image.\n","\n","    Arguments:\n","    - name: name of the base model.\n","    - weights: initial weights of the model: None (random), 'imagenet' (obtained during training on the ImageNet dataset) or path to a file with weights.\n","    - image_size: input image size.\n","    - pooling: output layer pooling type: 'avg' (average) or 'max' (max).\n","    - include_preprocess_input: Add image preprocessing layers.\n","    - training: using the model for training.'''\n","\n","    # We specify the input image shape only if the input image size is specified\n","    input_shape = (image_size, image_size, 3)\n","\n","    # Create the core of the basic model\n","    bild_core = getattr(applications, name)\n","    core = bild_core(include_top=False, weights=weights, input_shape=input_shape, pooling=pooling)\n","\n","    # Create an input layer for the image as an array of uint8\n","    i = layers.Input(input_shape, name=f'image_input')\n","\n","    # Add preprocessing\n","    if include_preprocess_input:\n","        app_module = inspect.getmodule(bild_core)\n","        build_preprocess_input = getattr(app_module, 'preprocess_input')\n","        x = build_preprocess_input(i)\n","    else:\n","        x = i\n","\n","    # Add a base model\n","    o = core(x, training=training)\n","\n","    # We combine everything into one model\n","    model = models.Model(inputs=[i], outputs=[o], name=core.name + ('_with' if include_preprocess_input else '_without') + '_preprocessing')\n","\n","    # Return the received model\n","    return model"]},{"cell_type":"markdown","id":"143536d9-de32-4b6a-a5e3-5c53d735ac96","metadata":{"id":"143536d9-de32-4b6a-a5e3-5c53d735ac96","tags":[]},"source":["#### Building the model on top"]},{"cell_type":"code","execution_count":null,"id":"fb72bc58-7fb2-4b9c-bb75-a5f5623bf262","metadata":{"id":"fb72bc58-7fb2-4b9c-bb75-a5f5623bf262"},"outputs":[],"source":["def build_model_on_top(\n","    feature_size: int,\n","    config: Union[List[Tuple[float, int]], Tuple[Tuple[float, int]]],\n","    emotions: Union[Union[List[str], Tuple[str]], Dict[str, Tuple[float, float]]]=EMOTIONS,\n","    training: bool=False,\n","    seed: int=SEED,\n",") -> models.Model:\n","    '''Creates a model from fully connected layers. The 'Dropout' method is used for regularization during training.\n","    If the model predicts emotions, the number of output neurons in the output corresponds to the number of emotions and the activation type 'Softmax' is applied.\n","    If the model predicts valance-arousal of emotions, then the output layer has only 2 output neurons and the 'ReLU' activation type is used with the output values ââlimited to 2.\n","    To bring the output valance-arousal values ââcloser, a final layer is added, ensuring that the valance-arousal values ââare reduced by 1.\n","\n","    Arguments:\n","    - feature_size: feature size.\n","    - config: configuration of regularization layers and fully connected layers (except output) in the format: [(regularization coefficient, fully connected layer size), ...]\n","    - emotions: description of predicted emotions.\n","    - training: for training.\n","    - seed: initializer for the random number generator of regularization layers.'''\n","\n","    # Create the input layer of the model\n","    i = layers.Input(shape=(feature_size,), name='feature_input')\n","    x = i\n","\n","    # Initialize the kernel\n","    initializer = initializers.GlorotUniform(seed=seed)\n","\n","    # Add fully connected layers according to the configuration\n","    for index, (dropout_rate, dense_units) in enumerate(config):\n","        if dropout_rate > 0.:\n","            x = layers.Dropout(dropout_rate, seed=seed, name=f'dropout_{index}')(x, training=training)\n","        if dense_units > 0:\n","            x = layers.Dense(dense_units, kernel_initializer=initializer, activation=\"relu\", name=f'dense_{index}')(x, training=training)\n","\n","    # Add output fully connected layer\n","    if isinstance(emotions, (list, tuple)):\n","        # In the usual classification\n","        o = layers.Dense(len(emotions), kernel_initializer=initializer, activation=\"softmax\", name='probs')(x, training=training)\n","    else:\n","        # When using the \"valence-arousal\" emotion decomposition\n","        x = layers.Dense(2, kernel_initializer=initializer, name='dense_valence_arousal')(x, training=training)\n","        x = layers.ReLU(max_value=2.0)(x)\n","        o = layers.Lambda(lambda x: x - 1.0)(x)\n","\n","    # Create a model\n","    model = models.Model(inputs=i, outputs=o, name='model_on_top')\n","\n","    return model"]},{"cell_type":"markdown","id":"4921d950-5597-43f6-a43d-4a75b72133c3","metadata":{"id":"4921d950-5597-43f6-a43d-4a75b72133c3","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["#### Building a model"]},{"cell_type":"code","execution_count":null,"id":"1c4e792f-58ff-4477-adc2-05e900f243a9","metadata":{"id":"1c4e792f-58ff-4477-adc2-05e900f243a9"},"outputs":[],"source":["def build_model(\n","    augment_model: Optional[models.Model]=None,\n","    base_model: Optional[models.Model]=None,\n","    model_on_top: Optional[models.Model]=None,\n",") -> models.Model:\n","    '''Creates an emotion prediction model from three models, sequentially connected: an image augmentation model, a base model, and a model on top.\n","\n","    Arguments:\n","    - augment_model: image augmentation model (see function build_augment_model).\n","    - base_model: base model (see function build_base_model).\n","    - model_on_top: model on top (see function build_on_top_model).\n","    '''\n","    models_list = []\n","    if augment_model is not None:\n","        models_list.append(augment_model)\n","    if base_model is not None:\n","        models_list.append(base_model)\n","    if model_on_top is not None:\n","        models_list.append(model_on_top)\n","    if len(models_list) == 0:\n","        return\n","    elif len(models_list) == 1:\n","        model = models_list[0]\n","    else:\n","        model = models.Sequential(models_list)\n","    return model"]},{"cell_type":"markdown","id":"288c21cf-0c1d-42d4-98b9-c8b391269c61","metadata":{"id":"288c21cf-0c1d-42d4-98b9-c8b391269c61","tags":[]},"source":["#### Stop training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KBUfcmrnqtCn"},"outputs":[],"source":["class EarlyStoppingAtBestMetric(callbacks.Callback):\n","    \"\"\"Stops training an emotion learning model when the model's loss on validation data stops decreasing.\"\"\"\n","\n","    def __init__(\n","        self,\n","        model: models.Model,\n","        metric: str,\n","        mode: str,\n","        patience: int=0,\n","        restore_best_weights: bool=False,\n","        verbose: int=VERBOSE,\n","        best_epoch: int=-1,\n","        best_loss: float=0.,\n","        best_metric: float=0.,\n","        best_weights: Optional[List[np.array]]=None,\n","        wait: int=0\n","    ):\n","        '''Arguments:\n","\n","        - model: trainable model\n","        - metric: target metric name\n","        - monitor: \"max\" - monitor metric increasing, \"min\" - monitor metric desccreasing\n","        - patience: the number of epochs during which the score does not improve, then training stops.\n","        - verboes: verbosity mode: 0 (quiet) or 1 (message output).\n","        - best_epoch: the number of the training epoch with the best score (at the end of the previous iteration).\n","        - best_loss: best loss on training dataset (at the end of the previous iteration).\n","        - best_metric: best metric on training dataset (at the end of the previous iteration).\n","        - best_weights: weights at the end of the epoch with the best model score (at the end of the previous iteration).\n","        - wait: the number of epochs during which the estimate did not improve (after the end of the previous iteration).'''\n","\n","        super().__init__()\n","\n","        self.__model = model\n","        self.__metric = metric\n","        self.__mode = mode\n","        self.__patience = patience\n","        self.__restore_best_weights = restore_best_weights\n","        self.__verbose = verbose\n","        self.__best_epoch = best_epoch\n","        self.__best_loss = best_loss\n","        self.__best_metric = best_metric\n","        self.__best_weights = best_weights\n","        self.__wait = wait\n","\n","    @property\n","    def best_epoch(self):\n","        return self.__best_epoch\n","\n","    @property\n","    def best_loss(self):\n","        return self.__best_loss\n","\n","    @property\n","    def best_metric(self):\n","        return self.__best_metric\n","\n","    @property\n","    def best_weights(self):\n","        return self.__best_weights\n","\n","    def on_train_begin(self, logs=None):\n","        self.__stopped_epoch = -1\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        # End of the era of learning\n","        current = logs[self.__metric]\n","        if self.__best_epoch == -1:\n","            new_best = True\n","        elif self.__mode == 'max':\n","            new_best = current > self.__best_metric\n","        elif self.__mode == 'min':\n","            new_best = current < self.__best_metric\n","\n","        if new_best:\n","            # Loss descreased - we understand the value and weights of the model\n","            self.__best_loss = logs['loss']\n","            self.__best_metric = current\n","            self.__best_epoch = epoch\n","            self.__wait = 0\n","            self.__best_weights = self.__model.get_weights()\n","            if self.__verbose:\n","                print(f'Epoch #{self.__best_epoch + 1}: metric has been improved ({current:.4f}).')\n","        else:\n","            # Loss did not descreas - wait for a specified number of epochs, then stop training\n","            self.__wait += 1\n","            if self.__wait >= self.__patience:\n","                self.__stopped_epoch = epoch\n","                self.__model.stop_training = True\n","\n","    def on_train_end(self, logs=None):\n","        # Stop learning\n","        if self.__stopped_epoch >= 0:\n","            if self.__verbose:\n","                print(f\"Epoch #{self.__stopped_epoch + 1}: early stopping.\")\n","\n","        # Restore the scales of a better era\n","        if self.__restore_best_weights:\n","            if self.__verbose:\n","                print(f\"Restoring model weights from the end of the best epoch (#{self.__best_epoch + 1}).\")\n","            self.__model.set_weights(self.__best_weights)"],"id":"KBUfcmrnqtCn"},{"cell_type":"code","execution_count":null,"id":"3bd8f37a-99bd-4c97-877f-3c9acdd4da98","metadata":{"id":"3bd8f37a-99bd-4c97-877f-3c9acdd4da98"},"outputs":[],"source":["class EarlyStoppingAtMinValLoss(callbacks.Callback):\n","    \"\"\"Stops training an emotion learning model when the model's loss on validation data stops decreasing.\"\"\"\n","\n","    def __init__(\n","        self,\n","        model: models.Model,\n","        metric: str,\n","        patience: int=0,\n","        restore_best_weights: bool=False,\n","        verbose: int=VERBOSE,\n","        best_epoch: int=-1,\n","        best_val_loss: float=0.,\n","        best_val_metric: float=0.,\n","        best_weights: Optional[List[np.array]]=None,\n","        wait: int=0\n","    ):\n","        '''Arguments:\n","\n","        - model: trainable model\n","        - metric: target metric name\n","        - patience: the number of epochs during which the score does not improve, then training stops.\n","        - verboes: verbosity mode: 0 (quiet) or 1 (message output).\n","        - best_epoch: the number of the training epoch with the best score (at the end of the previous iteration).\n","        - best_val_loss: best loss on validation dataset (at the end of the previous iteration).\n","        - best_val_metric: best metric on validation dataset (at the end of the previous iteration).\n","        - best_weights: weights at the end of the epoch with the best model score (at the end of the previous iteration).\n","        - wait: the number of epochs during which the estimate did not improve (after the end of the previous iteration).'''\n","\n","        super().__init__()\n","\n","        self.__model = model\n","        self.__metric = metric\n","        self.__patience = patience\n","        self.__restore_best_weights = restore_best_weights\n","        self.__verbose = verbose\n","        self.__best_epoch = best_epoch\n","        self.__best_val_loss = best_val_loss\n","        self.__best_val_metric = best_val_metric\n","        self.__best_weights = best_weights\n","        self.__wait = wait\n","\n","    @property\n","    def best_epoch(self):\n","        return self.__best_epoch\n","\n","    @property\n","    def best_val_loss(self):\n","        return self.__best_val_loss\n","\n","    @property\n","    def best_val_metric(self):\n","        return self.__best_val_metric\n","\n","    @property\n","    def best_weights(self):\n","        return self.__best_weights\n","\n","    def on_train_begin(self, logs=None):\n","        self.__stopped_epoch = -1\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        # End of the era of learning\n","        current = logs[\"val_loss\"]\n","        if (current < self.__best_val_loss) or (self.__best_val_loss == 0.):\n","            # Loss descreased - we understand the value and weights of the model\n","            self.__best_val_loss = current\n","            self.__best_val_metric = logs[f\"val_{self.__metric}\"]\n","            self.__best_epoch = epoch\n","            self.__wait = 0\n","            self.__best_weights = self.__model.get_weights()\n","            if self.__verbose:\n","                print(f'Epoch #{self.__best_epoch + 1}: val loss has been improved ({current:.4f}).')\n","        else:\n","            # Loss did not descreas - wait for a specified number of epochs, then stop training\n","            self.__wait += 1\n","            if self.__wait >= self.__patience:\n","                self.__stopped_epoch = epoch\n","                self.__model.stop_training = True\n","\n","    def on_train_end(self, logs=None):\n","        # Stop learning\n","        if self.__stopped_epoch >= 0:\n","            if self.__verbose:\n","                print(f\"Epoch #{self.__stopped_epoch + 1}: early stopping.\")\n","\n","        # Restore the scales of a better era\n","        if self.__restore_best_weights:\n","            if self.__verbose:\n","                print(f\"Restoring model weights from the end of the best epoch (#{self.__best_epoch + 1}).\")\n","            self.__model.set_weights(self.__best_weights)"]},{"cell_type":"code","execution_count":null,"id":"5b310bbd-3854-4277-9c9c-623959bbfc24","metadata":{"id":"5b310bbd-3854-4277-9c9c-623959bbfc24"},"outputs":[],"source":["class EarlyStoppingAtMaxTestScore(callbacks.Callback):\n","    \"\"\"Stops training an emotion learning model when the model's prediction score on the Kaggle test data stops increasing.\"\"\"\n","\n","    def __init__(\n","        self,\n","        model: models.Model,\n","        test_dataset: tf.data.Dataset,\n","        test_image_paths: str,\n","        patience: int=0,\n","        restore_best_weights: bool=False,\n","        verbose: int=VERBOSE,\n","        emotions: Union[Union[List[str], Tuple[str]], Dict[str, Tuple[float, float]]]=EMOTIONS,\n","        best_epoch: int=-1,\n","        best_test_score: float=0.,\n","        best_weights: Optional[List[np.array]]=None,\n","        wait: int=0\n","    ):\n","        '''Arguments:\n","\n","        - model: trainable model\n","        - test_dataset: dataset of test images.\n","        - test_image_paths: list of paths to test dataset files.\n","        - patience: the number of epochs during which the score does not improve, then training stops.\n","        - verboes: verbosity mode: 0 (quiet) or 1 (message output).\n","        - best_epoch: the number of the training epoch with the best score (at the end of the previous iteration).\n","        - best_test_score: best score (at the end of the previous iteration).\n","        - best_weights: weights at the end of the epoch with the best model score (at the end of the previous iteration).\n","        - wait: the number of epochs during which the estimate did not improve (after the end of the previous iteration).'''\n","\n","        super(EarlyStoppingAtMaxTestScore, self).__init__()\n","\n","        self.__model = model\n","        self.__emotions = emotions\n","\n","        self.__description = f'test_{model.name}'\n","        self.__file_path = f'test_{model.name}.csv'\n","        self.__keras = Kaggle()\n","\n","        self.__test_dataset = test_dataset\n","        self.__test_result = pd.DataFrame(columns=['image_path', 'emotion'])\n","        self.__test_result['image_path'] = test_image_paths\n","\n","        self.__restore_best_weights = restore_best_weights\n","        self.__patience = patience\n","        self.__verbose = verbose\n","        self.__best_epoch = best_epoch\n","        self.__best_test_score = best_test_score\n","        self.__best_weights = best_weights\n","        self.__wait = wait\n","\n","    @property\n","    def best_epoch(self):\n","        return self.__best_epoch\n","\n","    @property\n","    def best_test_score(self):\n","        return self.__best_test_score\n","\n","    @property\n","    def best_weights(self):\n","        return self.__best_weights\n","\n","    def on_train_begin(self, logs=None):\n","        self.__stopped_epoch = -1\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        # End of the era of learning\n","\n","        # Checking the accuracy of model predictions in Kaggle\n","        predicts = self.__model.predict(test_dataset, verbose=self.__verbose)\n","        if isinstance(EMOTIONS, (list, tuple)):\n","            labels = predicts.argmax(axis=1).tolist()\n","            self.__test_result['emotion'] = [self.__emotions[label] for label in labels]\n","        else:\n","            dists = np.apply_along_axis(lambda a: np.linalg.norm(a - np.array(list(self.__emotions.values())), axis=1), arr=predicts, axis=1)\n","            labels = dists.argmin(axis=1).tolist()\n","            self.__test_result['emotion'] = [list(self.__emotions)[label] for label in labels]\n","        self.__test_result.to_csv(self.__file_path, index=False)\n","        self.__keras.send_submission_files(descriptions=[self.__description], file_paths=[self.__file_path])\n","        Path(self.__file_path).unlink()\n","        test_scores = self.__keras.receive_submission_scores(descriptions=[self.__description]).loc[0, ['publicScore', 'privateScore']]\n","        current = (test_scores['publicScore'] + test_scores['privateScore']) / 2\n","        logs['test_public_score'] = test_scores['publicScore']\n","        logs['test_private_score'] = test_scores['privateScore']\n","        logs['test_score'] = current\n","\n","        if current > self.__best_test_score:\n","            # Accuracy has improved - we understand the value and weights of the model\n","            self.__best_test_score = current\n","            self.__best_epoch = epoch\n","            self.__wait = 0\n","            self.__best_weights = self.__model.get_weights()\n","            if self.__verbose:\n","                print(f'Epoch #{self.__best_epoch + 1}: accuracy has been improved ({current:.4f}).')\n","        else:\n","            # Accuracy has not improved - wait for a specified number of epochs, then stop training\n","            self.__wait += 1\n","            if self.__wait >= self.__patience:\n","                self.__stopped_epoch = epoch\n","                self.__model.stop_training = True\n","\n","    def on_train_end(self, logs=None):\n","        # Stop learning\n","        if self.__stopped_epoch >= 0:\n","            if self.__verbose:\n","                print(f\"Epoch #{self.__stopped_epoch + 1}: early stopping.\")\n","\n","        # Restore the scales of a better era\n","        if self.__restore_best_weights:\n","            if self.__verbose:\n","                print(f\"Restoring model weights from the end of the best epoch (#{self.__best_epoch + 1}).\")\n","            self.__model.set_weights(self.__best_weights)"]},{"cell_type":"markdown","id":"06340386-6d60-4172-bdb2-495db9218690","metadata":{"id":"06340386-6d60-4172-bdb2-495db9218690","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["#### Exponential Decay of Learning Rate"]},{"cell_type":"code","execution_count":null,"id":"6a2018c0-4650-4cdd-bf3a-4d93d69a0c64","metadata":{"id":"6a2018c0-4650-4cdd-bf3a-4d93d69a0c64"},"outputs":[],"source":["class LearningRateExpDecayScheduler(callbacks.LearningRateScheduler):\n","    '''Exponential Decay of Learning Rate.'''\n","    def __init__(self, decay_rate: float=1., verbose: int=VERBOSE):\n","        '''Arguments:\n","        - decay_rate - decay rate [0.0, 1.0].\n","        - verbose - verbose mode: 0-quiet, 1-output messages about changes in learning speed.'''\n","        self.__decay_rate = decay_rate\n","        super(LearningRateExpDecayScheduler, self).__init__(self.__scheduler)\n","\n","    def __scheduler(self, epoch, lr) -> float:\n","        return lr * self.__decay_rate"]},{"cell_type":"markdown","id":"cf515457-5acc-47de-af87-6d6732ccb0ff","metadata":{"id":"cf515457-5acc-47de-af87-6d6732ccb0ff","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["#### Model for implementation"]},{"cell_type":"code","execution_count":null,"id":"786df45b-ea02-487b-8cc2-6957af2990bd","metadata":{"id":"786df45b-ea02-487b-8cc2-6957af2990bd"},"outputs":[],"source":["class FaceEmotionRecognitionNet():\n","\n","    def __init__(self, file_path: str, emotions: Union[Union[List[str], Tuple[str]], Dict[str, Tuple[float, float]]]=EMOTIONS):\n","        '''\n","        file_path: path to the saved model file.\n","        emotions: predicted emotions.\n","        '''\n","        # Loading the model\n","        self.__model = models.load_model(filepath=file_path, compile=False, safe_mode=False)\n","        self.__emotions = emotions\n","\n","    def predict(self, face_image: np.array) -> Union[Tuple[str, float], Tuple[str, float, float, float]]:\n","        '''Predicting a person's emotion based on their facial image.\n","\n","        Arguments:\n","        - face_image: image of a person's face.\n","        '''\n","        image = Image.fromarray(face_image)\n","        size = max(image.width, image.height)\n","        # Make the image square\n","        padded_image = ImageOps.pad(image, (size, size))\n","        # Adjust the image size\n","        resized_image = padded_image.resize(self.__model.input_shape[1:3])\n","        # We get a prediction\n","        tensor = np.asarray(resized_image)[None, ...]\n","        predicts = self.__model.predict(tensor, verbose=0)[0]\n","        # Preparing the resulting data\n","        if isinstance(self.__emotions, (list, tuple)):\n","            probability = predicts.max()\n","            label = predicts.argmax()\n","            emotion = self.__emotions[label]\n","        else:\n","            valence, arousal = predicts\n","            dists = np.apply_along_axis(lambda a: np.linalg.norm(a - np.array(list(self.__emotions.values())), axis=1), arr=predicts[None, ...], axis=1)\n","            error = dists.min()\n","            label = dists.argmin()\n","            emotion = list(self.__emotions.keys())[label]\n","        # Return the result\n","        if isinstance(self.__emotions, (list, tuple)):\n","            return emotion, probability\n","        else:\n","            return emotion, error, valence, arousal"]},{"cell_type":"markdown","id":"63779dd2-66d3-4c83-a8c0-3c287b619d4a","metadata":{"id":"63779dd2-66d3-4c83-a8c0-3c287b619d4a","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Dataset"]},{"cell_type":"markdown","id":"9476d4c9-4da8-4e62-847e-13ea5550ca9a","metadata":{"id":"9476d4c9-4da8-4e62-847e-13ea5550ca9a","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["#### Function of face extraction from a batch of images with saving to files"]},{"cell_type":"raw","id":"6a37cd8e-0a5d-4983-887e-e37d146d0b74","metadata":{"id":"6a37cd8e-0a5d-4983-887e-e37d146d0b74"},"source":["def extract_face(detector: MTCNN, image_path: Path):\n","    '''Extracts faces from images.\n","\n","    Arguments:\n","    detector - MTCNN detector\n","    image_path - path to the source image\n","    '''\n","\n","    # Load image\n","    image = load_image(image_path)\n","\n","    # Find faces in the image\n","    faces = detector.detect_faces(image)\n","\n","    # Return result\n","    if len(faces) == 0:\n","        return\n","\n","    return faces[0]['box']"]},{"cell_type":"markdown","id":"d66f6ba7-a65a-42b0-8c0a-63379c2c3a36","metadata":{"id":"d66f6ba7-a65a-42b0-8c0a-63379c2c3a36","tags":[]},"source":["#### Function for creating a feature dataset"]},{"cell_type":"code","execution_count":null,"id":"bbf3d919-6314-45a4-92a2-69a5aa5706cb","metadata":{"id":"bbf3d919-6314-45a4-92a2-69a5aa5706cb"},"outputs":[],"source":["def build_feature_dataset(\n","    file_path: str,\n","    emotions: Union[Union[List[str], Tuple[str]], Dict[str, Tuple[float, float]]]=EMOTIONS,\n","    labeled: bool=True,\n","    batch_size: int=1,\n","    shuffle: bool=True,\n","    reshuffle_each_iteration: bool=True,\n","    seed: int=SEED,\n","    validation_split: Optional[float]=None,\n","    test_split: Optional[float]=None\n",") -> Union[\n","    tf.data.Dataset,\n","    Tuple[tf.data.Dataset, tf.data.Dataset],\n","    Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]\n","]:\n","    '''Loads a dataset of image features from an array file.\n","    Returns a dataset split into training, validation (optional) and test (optional) parts.\n","    Optionally, data can be moved randomly.\n","    The dataset contains markup by default, but it can be optionally excluded from it.\n","    Dataset(s) are divided into batches of a given size.\n","\n","    Arguments:\n","    - file_path: path to the data file.\n","    - emotions: description of emotions in the dataset.\n","    - batch_size: batch size.\n","    - shuffle: shuffle data.\n","    - reshuffle_each_iteration: reshuffle data at each iteration.\n","    - seed: initializer for random number generator during shuffling.'''\n","\n","    # Create training and validation datasets\n","    with np.load(file_path, allow_pickle=True) as data:\n","        if not labeled:\n","            dataset = tf.data.Dataset.from_tensor_slices(data['features'])\n","        elif isinstance(EMOTIONS, (list, tuple)):\n","            dataset = tf.data.Dataset.from_tensor_slices((data['features'], data['labels']))\n","        else:\n","            labels = np.apply_along_axis(lambda label: list(EMOTIONS.values())[int(label)], axis=1, arr=data['labels'])\n","            dataset = tf.data.Dataset.from_tensor_slices((data['features'], labels))\n","    size = len(dataset)\n","\n","    # Stirring\n","    if shuffle:\n","        dataset = dataset.shuffle(size, seed, reshuffle_each_iteration=reshuffle_each_iteration)\n","\n","    # Returning the full dataset\n","    if validation_split is None and test_split is None:\n","        return dataset.batch(batch_size)\n","\n","    # Return the dataset, divided into training and testing parts\n","    if test_split is None:\n","        train_size = int(size * (1-validation_split))\n","        val_size = size - train_size\n","        train_dataset = dataset.take(train_size).batch(batch_size)\n","        val_dataset = dataset.skip(train_size).take(val_size).batch(batch_size)\n","        return train_dataset, val_dataset\n","\n","    # Return the dataset, divided into training, validation and test parts\n","    train_size = int(size * (1-validation_split-test_split))\n","    val_size = int(size * validation_split)\n","    test_size = size - train_size - val_size\n","    train_dataset = dataset.take(train_size).batch(batch_size)\n","    val_dataset = dataset.skip(train_size).take(val_size).batch(batch_size)\n","    test_dataset = dataset.skip(train_size + val_size).take(test_size).batch(batch_size)\n","    return train_dataset, val_dataset, test_dataset"]},{"cell_type":"markdown","id":"bfe3225a-5008-404f-8c7e-44c69bcd306b","metadata":{"id":"bfe3225a-5008-404f-8c7e-44c69bcd306b","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Pipeline"]},{"cell_type":"code","execution_count":null,"id":"1a35a9c9-17b8-4f01-a46c-5517d77c540a","metadata":{"id":"1a35a9c9-17b8-4f01-a46c-5517d77c540a"},"outputs":[],"source":["class Pipeline():\n","    '''Model creation pipeline.'''\n","\n","    def __init__(self, config: dict, proj_path: Path, is_prev_complete: bool, platform: str):\n","        '''Arguments:\n","        - config[dict]: stage configuration.\n","        - proj_path[Path]: path to the project folder.\n","        - is_prev_complete[bool]: Is the previous step completed?\n","        - platform[str]: platform on which the pipeline is executed: 'colab' (Goggle Colab), 'local' (local).\n","        '''\n","        self.__name = config['name']\n","        self.__description = config['description']\n","        self.__stages = (stage for stage in config['stages'])\n","        self.__path = proj_path / config['name']\n","        if not self.__path.exists():\n","            self.__path.mkdir()\n","        self.__report_path = self.__path / config['report_csv']\n","        if self.__report_path.exists():\n","            self.__report = pd.read_csv(self.__report_path, index_col='stage')\n","        else:\n","            self.__report = pd.DataFrame(\n","                columns = [\n","                    'stage',\n","                    'params',\n","                    'platform',\n","                    'start_time',\n","                    'update_time',\n","                    'state',\n","                ]\n","            )\n","            self.__report['stage'] = [stage['name'] for stage in config['stages']]\n","            self.__report.set_index('stage', inplace=True)\n","        self.__is_prev_complete = is_prev_complete\n","        self.__platform = platform\n","        self.__stage = None\n","\n","    def next_stage(self):\n","        '''Returns the next stage of the pipeline.'''\n","        self.__stage = next(self.__stages)\n","\n","        # If the stage has already been completed, then we skip its execution\n","        if self.__report.loc[self.__stage['name'], 'state'] == 'complete':\n","            return\n","\n","        # Stage not yet completed\n","        self.__report.loc[self.__stage['name'], 'platform'] = self.__platform\n","        self.__report.loc[self.__stage['name'], 'params'] = str(self.__stage['params'])\n","\n","        # If the previous pipeline has not yet been fully executed, or all previous stages of this pipeline have not yet been executed\n","        # then we skip the stage\n","        if not self.__is_prev_complete or (self.__report.iloc[:self.__report.index.get_loc(self.__stage['name'])]['state'] != 'complete').any():\n","            if self.__report.loc[self.__stage['name'], 'state'] != 'skipped (not ready)':\n","                self.__report.loc[self.__stage['name'], 'update_time'] = datetime.now()\n","                self.__report.loc[self.__stage['name'], 'state'] = 'skipped (not ready)'\n","            return\n","\n","        # If the runtime does not match the required one, then skip the stage\n","        if self.__platform != self.__stage['platform']:\n","            if self.__report.loc[self.__stage['name'], 'state'] != 'skipped (platform)':\n","                self.__report.loc[self.__stage['name'], 'update_time'] = datetime.now()\n","                self.__report.loc[self.__stage['name'], 'state'] = 'skipped (platform)'\n","            return\n","\n","        # If the stage is performed in several iterations, then we move on to the next iteration\n","        if self.__report.loc[self.__stage['name'], 'state'] == 'run complete':\n","            self.__report.loc[self.__stage['name'], 'update_time'] = datetime.now()\n","            self.__report.loc[self.__stage['name'], 'state'] = 'run started'\n","            return\n","\n","        # Remember the start time of the stage execution\n","        self.__report.loc[self.__stage['name'], 'start_time'] = datetime.now()\n","        self.__report.loc[self.__stage['name'], 'update_time'] = self.__report.loc[self.__stage['name'], 'start_time']\n","        self.__report.loc[self.__stage['name'], 'state'] = 'started'\n","\n","    @property\n","    def name(self) -> str:\n","        '''Pipeline name.'''\n","        return self.__name\n","\n","    @property\n","    def description(self) -> str:\n","        '''Pipeline Description.'''\n","        return self.__description\n","\n","    @property\n","    def report(self) -> pd.DataFrame:\n","        '''Pipeline Execution Report.'''\n","        return self.__report\n","\n","    @property\n","    def stage_name(self) -> str:\n","        '''The name of the current stage.'''\n","        return self.__stage['name']\n","\n","    @property\n","    def is_complete(self) -> bool:\n","        '''Is the current pipeline complete?'''\n","        return (self.__report['state'] == 'complete').all()\n","\n","    @property\n","    def is_stage_failed(self) -> bool:\n","        return self.__report.loc[self.__stage['name'], 'state'] == 'failed'\n","\n","    @property\n","    def is_stage_complete(self) -> bool:\n","        '''Has the current stage been completed?'''\n","        return self.__report.loc[self.__stage['name'], 'state'] == 'complete'\n","\n","    @property\n","    def is_stage_started(self) -> bool:\n","        '''Is the current stage running?'''\n","        return self.__report.loc[self.__stage['name'], 'state'] == 'started'\n","\n","    @property\n","    def is_stage_skipped(self) -> bool:\n","        '''Is the current stage skipped?'''\n","        return not self.__report.loc[self.__stage['name'], 'state'].find('started') >= 0\n","\n","    @property\n","    def stage_params(self) -> dict:\n","        '''Parameters of the current stage.'''\n","        return self.__stage['params']\n","\n","    @property\n","    def stage_description(self) -> str:\n","        '''Stage Description.'''\n","        return self.__stage['description']\n","\n","    def __save(self):\n","        '''Saving a pipeline to a file.'''\n","        self.__report.to_csv(self.__report_path)\n","\n","    def complete_stage_run(self):\n","        '''End of stage iteration.'''\n","        self.__report.loc[self.__stage['name'], 'update_time'] = datetime.now()\n","        self.__report.loc[self.__stage['name'], 'state'] = 'run complete'\n","        self.__save()\n","\n","    def fail_stage(self):\n","        self.__report.loc[self.__stage['name'], 'update_time'] = datetime.now()\n","        self.__report.loc[self.__stage['name'], 'state'] = 'failed'\n","        self.__save()\n","\n","    def complete_stage(self):\n","        '''End of stage.'''\n","        self.__report.loc[self.__stage['name'], 'update_time'] = datetime.now()\n","        self.__report.loc[self.__stage['name'], 'state'] = 'complete'\n","        self.__save()\n","\n","    def save_stage_processing(self, result: pd.DataFrame):\n","        '''Saving the stage execution log to a file.'''\n","        result.to_csv(self.__path / self.__stage['params']['process_csv'])\n","\n","    def load_stage_processing(self) -> pd.DataFrame:\n","        '''Loading a stage execution log from a file.'''\n","        return pd.read_csv(self.__path / self.__stage['params']['process_csv'])\n","\n","    def save_stage_result(self, result: pd.DataFrame):\n","        '''Saving the result of the stage execution to a file.'''\n","        result.to_csv(self.__path / self.__stage['params']['result_csv'])\n","\n","    def load_stage_result(self) -> pd.DataFrame:\n","        '''Loading the result of a stage execution from a file.'''\n","        return pd.read_csv(self.__path / self.__stage['params']['result_csv'])"]},{"cell_type":"markdown","id":"a39e259b-b105-4cf1-9c80-1e6743615500","metadata":{"id":"a39e259b-b105-4cf1-9c80-1e6743615500","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Kaggle"]},{"cell_type":"code","execution_count":null,"id":"adca8134-fe7c-44ae-9335-d90b59c68122","metadata":{"id":"adca8134-fe7c-44ae-9335-d90b59c68122"},"outputs":[],"source":["class Kaggle():\n","    '''Interaction with the Kaggle platform.'''\n","\n","    __COLUMNS = ['fileName', 'date', 'description', 'status', 'publicScore', 'privateScore']\n","\n","    def __init__(self, competition: str=PROJECT_NAME, verbose: int=VERBOSE):\n","        '''Initializing interaction with the Kaggle platform.\n","\n","        Arguments:\n","        - competition: name of the competition.\n","        - verbose: verbose mode: 0 (silent) or 1 (message output).'''\n","\n","        self.__competition = competition\n","        self.__verbose = verbose\n","\n","    def send_submission_files(self, descriptions: Union[List[str], Tuple[str]], file_paths: Union[List[str], Tuple[str]]):\n","        '''Submitting solution files for review.\n","\n","        Arguments:\n","        - descriptions: list of solution descriptions.\n","        - file_paths: list of paths to solution files.'''\n","        for file_path, description in zip(file_paths, descriptions):\n","            cmd = f'kaggle competitions submit -c {self.__competition} -f \"{file_path}\" -m \"{description}\" -q'\n","            lines = !{cmd}\n","            print(lines)\n","            if self.__verbose:\n","                print(f'Sended file {file_path} of submission {description} to competition {self.__competition}.')\n","\n","    def receive_submission_scores(self, descriptions: Union[List[str], Tuple[str]]) -> pd.DataFrame:\n","        '''Receiving the results of decision verification.\n","\n","        Arguments:\n","        - descriptions: list of solution descriptions.'''\n","\n","        # Request a list of results\n","        cmd = f'kaggle competitions submissions -c {self.__competition}'\n","        while True:\n","            lines = !{cmd}\n","            if all([line.find('pending') == -1 for line in lines]):\n","                break\n","            sleep(1)\n","\n","        if self.__verbose:\n","            descriptions_str = ', '.join(descriptions)\n","            print(f'Received scores of submissions {descriptions_str} from competition {self.__competition}.')\n","\n","        # Find the header line\n","        for index, line in enumerate(lines):\n","            if line.split() == self.__COLUMNS:\n","                break\n","\n","        # Find the position of columns in the text\n","        header_start_positions = [line.find(column) for column in self.__COLUMNS]\n","        header_end_positions = header_start_positions[1:]\n","        header_end_positions.append(len(line))\n","\n","        # Leave the lines with the results\n","        lines = lines[index+2:]\n","\n","        # Extract data from result rows\n","        data = [\n","            [\n","                line[header_start_position: header_end_position].strip()\n","                for header_start_position, header_end_position in zip(header_start_positions, header_end_positions)\n","            ] for line in lines\n","        ]\n","\n","        # Create a dataset from the obtained results\n","        result = pd.DataFrame(data, columns=self.__COLUMNS)\n","        result['publicScore'] = pd.to_numeric(result['publicScore'], errors='coerce')\n","        result['privateScore'] = pd.to_numeric(result['privateScore'], errors='coerce')\n","        result['date'] = pd.to_datetime(result['date'])\n","\n","        # We leave only the results of the sent predictions\n","        result = result[result['description'].isin(descriptions)]\n","\n","        # Since there may be files with the same name, we take the latest ones\n","        indexes = sorted([indexes[0] for indexes in result.groupby('description').groups.values()])\n","        result = result.iloc[indexes]\n","\n","        # Sort the results in the order they were sent\n","        result = result.sort_values('date').reset_index(drop=True)\n","\n","        # Output the results\n","        if self.__verbose:\n","            print(result)\n","\n","        # Return the result\n","        return result"]},{"cell_type":"markdown","id":"1d83193c-d48e-4eca-99a5-cef351571004","metadata":{"id":"1d83193c-d48e-4eca-99a5-cef351571004","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["## Pipeline for collecting information about underlying models in Keras Applications"]},{"cell_type":"markdown","id":"4dfd97d2-2407-478e-844e-7cfa69cd0663","metadata":{"id":"4dfd97d2-2407-478e-844e-7cfa69cd0663"},"source":["### Create/download pipeline from Google Drive"]},{"cell_type":"code","execution_count":null,"id":"cb259eb1-19d1-4e31-afd6-9d7a8af3f6c8","metadata":{"id":"cb259eb1-19d1-4e31-afd6-9d7a8af3f6c8"},"outputs":[],"source":["pipeline = Pipeline(config=KERAS_BASE_MODELS_PROCESSING_PIPELINE, proj_path=gd_proj_path, is_prev_complete=True, platform=platform)"]},{"cell_type":"markdown","id":"01a519fd-8773-42fa-bd3a-7fd84ee04ea7","metadata":{"id":"01a519fd-8773-42fa-bd3a-7fd84ee04ea7"},"source":["### Getting information about the sizes of input images and feature vectors"]},{"cell_type":"code","execution_count":null,"id":"ce78d0ea-c9e5-4589-b411-898daf1764ad","metadata":{"id":"ce78d0ea-c9e5-4589-b411-898daf1764ad"},"outputs":[],"source":["pipeline.next_stage()\n","skip = pipeline.is_stage_skipped\n","params = pipeline.stage_params"]},{"cell_type":"markdown","id":"7dab8f49-b265-4b1c-9ef9-e98c6d37d060","metadata":{"id":"7dab8f49-b265-4b1c-9ef9-e98c6d37d060"},"source":["#### Creating a results table"]},{"cell_type":"code","execution_count":null,"id":"15179b86-7852-4f51-b364-121c09581dc6","metadata":{"id":"15179b86-7852-4f51-b364-121c09581dc6"},"outputs":[],"source":["if not skip:\n","\n","    base_models_sizes = pd.DataFrame(columns=['base_model_name', 'image_size', 'feature_size'])\n","    base_models_sizes['base_model_name'] = [base_model_name for base_model_name, (base_model_size, _) in KERAS_BASE_MODELS.items() if base_model_size <= BASE_MODEL_MAX_SIZE]\n","    base_models_sizes.set_index('base_model_name', inplace=True)"]},{"cell_type":"markdown","id":"560cd465-c102-495e-8adf-21a7905294b0","metadata":{"id":"560cd465-c102-495e-8adf-21a7905294b0"},"source":["#### Getting information about image sizes and model features"]},{"cell_type":"code","execution_count":null,"id":"230b4174-b3fa-4264-868c-4790f3925584","metadata":{"id":"230b4174-b3fa-4264-868c-4790f3925584"},"outputs":[],"source":["if not skip:\n","\n","    with tqdm(base_models_sizes.index, unit='model') as t:\n","        for base_model_name in t:\n","            t.set_description(f'{base_model_name}')\n","\n","            # Get the size of the input image\n","            base_model = getattr(applications, base_model_name)(include_top=True, weights=None)\n","            image_size = base_model.input_shape[1]\n","            if image_size is None:\n","                image_size = 299\n","\n","            # Get the size of the feature vector\n","            base_model = getattr(applications, base_model_name)(include_top=False, weights=None)\n","            feature_size = base_model.output_shape[-1]\n","\n","            # We enter the results into the table\n","            base_models_sizes.loc[base_model_name] = image_size, feature_size"]},{"cell_type":"markdown","id":"Q4PJh5fWqjOA","metadata":{"id":"Q4PJh5fWqjOA"},"source":[]},{"cell_type":"markdown","id":"d9db7aca-4590-4d80-a1d5-e08bc5137912","metadata":{"id":"d9db7aca-4590-4d80-a1d5-e08bc5137912"},"source":["#### Saving results to Google Drive"]},{"cell_type":"code","execution_count":null,"id":"d4ded5d6-e722-4c4f-abe0-14fe1931a4bd","metadata":{"id":"d4ded5d6-e722-4c4f-abe0-14fe1931a4bd"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.save_stage_result(base_models_sizes)"]},{"cell_type":"markdown","id":"374c442d-0739-4036-afc1-692cbeb43c0f","metadata":{"id":"374c442d-0739-4036-afc1-692cbeb43c0f"},"source":["#### Fixing the completion of the stage"]},{"cell_type":"code","execution_count":null,"id":"4798604b-24a9-463d-ae3f-cf043f479a2c","metadata":{"id":"4798604b-24a9-463d-ae3f-cf043f479a2c"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.complete_stage()"]},{"cell_type":"markdown","id":"01a8d9ee-26a6-4290-a633-3b5eb6540bfa","metadata":{"id":"01a8d9ee-26a6-4290-a633-3b5eb6540bfa"},"source":["#### Loading results from Google Drive (done when step is skipped)"]},{"cell_type":"code","execution_count":null,"id":"9cfcb9d9-566e-402f-870e-f8ee78d3f275","metadata":{"id":"9cfcb9d9-566e-402f-870e-f8ee78d3f275"},"outputs":[],"source":["if skip:\n","\n","    if pipeline.is_stage_complete:\n","        base_models_sizes = pipeline.load_stage_result().set_index('base_model_name')"]},{"cell_type":"markdown","id":"1c66258b-cbf1-4a38-8847-e3a040e90e8a","metadata":{"id":"1c66258b-cbf1-4a38-8847-e3a040e90e8a"},"source":["#### Output of results"]},{"cell_type":"code","execution_count":null,"id":"e4d7de51-1621-459d-8670-70496242c17e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1743952733391,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"e4d7de51-1621-459d-8670-70496242c17e","outputId":"c413c1be-42eb-405c-c940-78fbac532902"},"outputs":[{"output_type":"display_data","data":{"text/plain":["                  image_size  feature_size\n","base_model_name                           \n","MobileNet                224          1024\n","MobileNetV2              224          1280\n","NASNetMobile             224          1056\n","EfficientNetB0           224          1280\n","EfficientNetB1           240          1280\n","DenseNet121              224          1024\n","EfficientNetB2           260          1408\n","DenseNet169              224          1664\n","EfficientNetB3           300          1536\n","EfficientNetV2B0         224          1280\n","EfficientNetV2B1         240          1280\n","EfficientNetV2B2         260          1408\n","EfficientNetV2B3         300          1536"],"text/html":["\n","  <div id=\"df-6174098d-6045-4fb6-b966-bc2fee64a5ab\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_size</th>\n","      <th>feature_size</th>\n","    </tr>\n","    <tr>\n","      <th>base_model_name</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>MobileNet</th>\n","      <td>224</td>\n","      <td>1024</td>\n","    </tr>\n","    <tr>\n","      <th>MobileNetV2</th>\n","      <td>224</td>\n","      <td>1280</td>\n","    </tr>\n","    <tr>\n","      <th>NASNetMobile</th>\n","      <td>224</td>\n","      <td>1056</td>\n","    </tr>\n","    <tr>\n","      <th>EfficientNetB0</th>\n","      <td>224</td>\n","      <td>1280</td>\n","    </tr>\n","    <tr>\n","      <th>EfficientNetB1</th>\n","      <td>240</td>\n","      <td>1280</td>\n","    </tr>\n","    <tr>\n","      <th>DenseNet121</th>\n","      <td>224</td>\n","      <td>1024</td>\n","    </tr>\n","    <tr>\n","      <th>EfficientNetB2</th>\n","      <td>260</td>\n","      <td>1408</td>\n","    </tr>\n","    <tr>\n","      <th>DenseNet169</th>\n","      <td>224</td>\n","      <td>1664</td>\n","    </tr>\n","    <tr>\n","      <th>EfficientNetB3</th>\n","      <td>300</td>\n","      <td>1536</td>\n","    </tr>\n","    <tr>\n","      <th>EfficientNetV2B0</th>\n","      <td>224</td>\n","      <td>1280</td>\n","    </tr>\n","    <tr>\n","      <th>EfficientNetV2B1</th>\n","      <td>240</td>\n","      <td>1280</td>\n","    </tr>\n","    <tr>\n","      <th>EfficientNetV2B2</th>\n","      <td>260</td>\n","      <td>1408</td>\n","    </tr>\n","    <tr>\n","      <th>EfficientNetV2B3</th>\n","      <td>300</td>\n","      <td>1536</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6174098d-6045-4fb6-b966-bc2fee64a5ab')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-6174098d-6045-4fb6-b966-bc2fee64a5ab button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-6174098d-6045-4fb6-b966-bc2fee64a5ab');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-2fe25c38-283f-4943-9cbf-178752b43d8e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2fe25c38-283f-4943-9cbf-178752b43d8e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-2fe25c38-283f-4943-9cbf-178752b43d8e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_78ef640b-7037-4cdb-b596-79dcb7a47f48\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('base_models_sizes')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_78ef640b-7037-4cdb-b596-79dcb7a47f48 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('base_models_sizes');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"base_models_sizes","summary":"{\n  \"name\": \"base_models_sizes\",\n  \"rows\": 13,\n  \"fields\": [\n    {\n      \"column\": \"base_model_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"EfficientNetV2B2\",\n          \"EfficientNetV2B0\",\n          \"MobileNet\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 224,\n        \"max\": 300,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          240,\n          300,\n          224\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 199,\n        \"min\": 1024,\n        \"max\": 1664,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1024,\n          1280,\n          1536\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}],"source":["if pipeline.is_stage_complete:\n","    display(base_models_sizes)"]},{"cell_type":"markdown","id":"82bf49f9-16ea-4636-8026-70a73f295630","metadata":{"id":"82bf49f9-16ea-4636-8026-70a73f295630"},"source":["### Measuring the inference time of models"]},{"cell_type":"code","execution_count":null,"id":"6436e109-74d0-4954-805d-27c538a3a700","metadata":{"id":"6436e109-74d0-4954-805d-27c538a3a700"},"outputs":[],"source":["pipeline.next_stage()\n","skip = pipeline.is_stage_skipped\n","params = pipeline.stage_params"]},{"cell_type":"markdown","id":"7c563964-f8fe-4ab1-b4b0-43a1e13e052e","metadata":{"id":"7c563964-f8fe-4ab1-b4b0-43a1e13e052e"},"source":["#### Creating a results table"]},{"cell_type":"code","execution_count":null,"id":"74ae1e20-2e2d-439f-8915-749f98372857","metadata":{"id":"74ae1e20-2e2d-439f-8915-749f98372857"},"outputs":[],"source":["if not skip:\n","\n","    model_inference_times = pd.DataFrame(columns=['base_model_name', 'inference_time'])\n","    model_inference_times['base_model_name'] = [base_model_name for base_model_name, (base_model_size, _) in KERAS_BASE_MODELS.items() if base_model_size <= BASE_MODEL_MAX_SIZE]\n","    model_inference_times.set_index('base_model_name', inplace=True)"]},{"cell_type":"markdown","id":"e91cdd4f-7eab-4db8-a2d7-4205a2823a85","metadata":{"id":"e91cdd4f-7eab-4db8-a2d7-4205a2823a85"},"source":["#### We form the \"heaviest\" configuration of the model on top"]},{"cell_type":"code","execution_count":null,"id":"bc45557b-5dd4-4024-a642-36cd594565d7","metadata":{"id":"bc45557b-5dd4-4024-a642-36cd594565d7"},"outputs":[],"source":["if not skip:\n","\n","    model_on_top_config = [(max(MODEL_ON_TOP_DROPOUT_RATES), max(MODEL_ON_TOP_DENSE_UNITS))] * max(MODEL_ON_TOP_DENSE_NUMS)"]},{"cell_type":"markdown","id":"7e9d668c-4519-4bf9-8ccb-7aa0aad52790","metadata":{"id":"7e9d668c-4519-4bf9-8ccb-7aa0aad52790"},"source":["#### Measuring the inference time of models"]},{"cell_type":"code","execution_count":null,"id":"e5d9b05f-0fac-4e36-8877-007e2114eec9","metadata":{"id":"e5d9b05f-0fac-4e36-8877-007e2114eec9"},"outputs":[],"source":["if not skip:\n","\n","    with tqdm(model_inference_times.index, unit='model') as t:\n","        for base_model_name in t:\n","            t.set_description(f'{base_model_name}')\n","\n","            image_size, feature_size = base_models_sizes.loc[base_model_name, ['image_size', 'feature_size']]\n","\n","            # Create a model with random weights to measure inference time\n","            base_model = build_base_model(base_model_name, image_size=image_size, weights=None, pooling=BASE_MODEL_POOLINGS, include_preprocess_input=True)\n","            model_on_top = build_model_on_top(feature_size, model_on_top_config)\n","            model = build_model(base_model=base_model, model_on_top=model_on_top)\n","            model.trainable = False\n","\n","            # Create a dataset for measuring inference time\n","            dataset = tf.data.Dataset.from_tensor_slices(np.random.randint(0, 255, (params['batch_size'], image_size, image_size, 3)))\n","            dataset = dataset.batch(params['batch_size'])\n","            dataset = dataset.repeat(params['batches'])\n","\n","            # Warming up the model on one batch\n","            model.predict(dataset.take(1))\n","\n","            # Measure the total inference time\n","            inf_time = timeit(\n","                lambda: model.predict(dataset), number=params['repetitions']\n","            )\n","\n","            # We enter the results into the table\n","            model_inference_times.loc[base_model_name, 'inference_time'] = inf_time\n","\n","    # Calculate the average inference time at one step\n","    steps_total = params['batch_size'] * params['batches'] * params['repetitions']\n","    model_inference_times['inference_time'] /= steps_total"]},{"cell_type":"markdown","id":"0334070a-7597-4fe6-a5bf-b9dd280d403a","metadata":{"id":"0334070a-7597-4fe6-a5bf-b9dd280d403a"},"source":["#### Saving results to Google Drive"]},{"cell_type":"code","execution_count":null,"id":"3a9c9119-4eb7-4cef-aecc-f56d2de54422","metadata":{"id":"3a9c9119-4eb7-4cef-aecc-f56d2de54422"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.save_stage_result(model_inference_times)"]},{"cell_type":"markdown","id":"71e8b9fa-2ab1-45ca-bfc2-9e97c3d0d0bf","metadata":{"id":"71e8b9fa-2ab1-45ca-bfc2-9e97c3d0d0bf"},"source":["#### Fixing the completion of the stage"]},{"cell_type":"code","execution_count":null,"id":"872e8818-ca9b-4c82-bb4a-fbd87245eda6","metadata":{"id":"872e8818-ca9b-4c82-bb4a-fbd87245eda6"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.complete_stage()"]},{"cell_type":"markdown","id":"42129f3a-49a7-4b43-8eeb-fe3e6b2ea4d7","metadata":{"id":"42129f3a-49a7-4b43-8eeb-fe3e6b2ea4d7"},"source":["#### Loading results from Google Drive (done when step is skipped)"]},{"cell_type":"code","execution_count":null,"id":"e0cf42ff-ebbe-4ce3-a739-508d16988373","metadata":{"id":"e0cf42ff-ebbe-4ce3-a739-508d16988373"},"outputs":[],"source":["if skip:\n","\n","    if pipeline.is_stage_complete:\n","        model_inference_times = pipeline.load_stage_result().set_index('base_model_name')"]},{"cell_type":"markdown","id":"a0021e34-5451-4f7d-95c4-6b03309a4804","metadata":{"id":"a0021e34-5451-4f7d-95c4-6b03309a4804"},"source":["#### Output of results"]},{"cell_type":"code","execution_count":null,"id":"20b1a622-765c-486c-b74c-02d1df26b752","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":128,"status":"ok","timestamp":1743952733548,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"20b1a622-765c-486c-b74c-02d1df26b752","outputId":"fcc25279-15fd-48ce-bada-e07fa134a0d4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["                  inference_time\n","base_model_name                 \n","MobileNet               0.009052\n","MobileNetV2             0.010616\n","NASNetMobile            0.029175\n","EfficientNetB0          0.013702\n","EfficientNetB1          0.015081\n","DenseNet121             0.020741\n","EfficientNetB2          0.018812\n","DenseNet169             0.027100\n","EfficientNetB3          0.020881\n","EfficientNetV2B0        0.013438\n","EfficientNetV2B1        0.015272\n","EfficientNetV2B2        0.022372\n","EfficientNetV2B3        0.030711"],"text/html":["\n","  <div id=\"df-4dd2a657-9474-43fa-a153-ac73147fdd06\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>inference_time</th>\n","    </tr>\n","    <tr>\n","      <th>base_model_name</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>MobileNet</th>\n","      <td>0.009052</td>\n","    </tr>\n","    <tr>\n","      <th>MobileNetV2</th>\n","      <td>0.010616</td>\n","    </tr>\n","    <tr>\n","      <th>NASNetMobile</th>\n","      <td>0.029175</td>\n","    </tr>\n","    <tr>\n","      <th>EfficientNetB0</th>\n","      <td>0.013702</td>\n","    </tr>\n","    <tr>\n","      <th>EfficientNetB1</th>\n","      <td>0.015081</td>\n","    </tr>\n","    <tr>\n","      <th>DenseNet121</th>\n","      <td>0.020741</td>\n","    </tr>\n","    <tr>\n","      <th>EfficientNetB2</th>\n","      <td>0.018812</td>\n","    </tr>\n","    <tr>\n","      <th>DenseNet169</th>\n","      <td>0.027100</td>\n","    </tr>\n","    <tr>\n","      <th>EfficientNetB3</th>\n","      <td>0.020881</td>\n","    </tr>\n","    <tr>\n","      <th>EfficientNetV2B0</th>\n","      <td>0.013438</td>\n","    </tr>\n","    <tr>\n","      <th>EfficientNetV2B1</th>\n","      <td>0.015272</td>\n","    </tr>\n","    <tr>\n","      <th>EfficientNetV2B2</th>\n","      <td>0.022372</td>\n","    </tr>\n","    <tr>\n","      <th>EfficientNetV2B3</th>\n","      <td>0.030711</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4dd2a657-9474-43fa-a153-ac73147fdd06')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-4dd2a657-9474-43fa-a153-ac73147fdd06 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-4dd2a657-9474-43fa-a153-ac73147fdd06');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0de24f91-b744-4d57-9193-47847dc127b3\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0de24f91-b744-4d57-9193-47847dc127b3')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0de24f91-b744-4d57-9193-47847dc127b3 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_43f7e597-095e-49f4-aca8-159243963e3e\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('model_inference_times')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_43f7e597-095e-49f4-aca8-159243963e3e button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('model_inference_times');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"model_inference_times","summary":"{\n  \"name\": \"model_inference_times\",\n  \"rows\": 13,\n  \"fields\": [\n    {\n      \"column\": \"base_model_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"EfficientNetV2B2\",\n          \"EfficientNetV2B0\",\n          \"MobileNet\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inference_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0069627270402202995,\n        \"min\": 0.0090523544899997,\n        \"max\": 0.0307107081800006,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.0223719520300005,\n          0.0134379604599996,\n          0.0090523544899997\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}],"source":["if pipeline.is_stage_complete:\n","    display(model_inference_times)"]},{"cell_type":"markdown","id":"238e2d09-fdd6-4915-8938-37e8ae90758a","metadata":{"id":"238e2d09-fdd6-4915-8938-37e8ae90758a"},"source":["### Selecting a base model"]},{"cell_type":"code","execution_count":null,"id":"ca33b4fc-b936-4f9c-84c0-02d074838d88","metadata":{"id":"ca33b4fc-b936-4f9c-84c0-02d074838d88"},"outputs":[],"source":["pipeline.next_stage()\n","skip = pipeline.is_stage_skipped\n","params = pipeline.stage_params"]},{"cell_type":"markdown","id":"9458f5ee-188f-4aab-9b54-c13f4136d38c","metadata":{"id":"9458f5ee-188f-4aab-9b54-c13f4136d38c"},"source":["#### Selection of models by the ratio of accuracy and max. inference time"]},{"cell_type":"code","execution_count":null,"id":"a96720d9-1000-4865-8170-1310c572189b","metadata":{"id":"a96720d9-1000-4865-8170-1310c572189b"},"outputs":[],"source":["if not skip:\n","\n","    base_model_selection = pd.DataFrame()\n","    base_model_selection[['base_model_name', 'top1_accuracy']] = [(base_model_name, top1_accuracy) for base_model_name, (base_model_size, top1_accuracy) in KERAS_BASE_MODELS.items() if base_model_size <= BASE_MODEL_MAX_SIZE]\n","    base_model_selection.set_index('base_model_name', inplace=True)\n","    base_model_selection = base_model_selection.merge(model_inference_times['inference_time'], on='base_model_name')\n","    base_model_selection = base_model_selection.loc[base_model_selection['inference_time'] <= MAX_INFERENCE_TIME]"]},{"cell_type":"markdown","id":"1c81247e-fff2-4f6f-800a-0d5bd75f9349","metadata":{"id":"1c81247e-fff2-4f6f-800a-0d5bd75f9349"},"source":["#### Ranking of selected models by the ratio of accuracy and max. inference time"]},{"cell_type":"code","execution_count":null,"id":"06ca46df-0d8c-4c6f-82ca-f818903245fa","metadata":{"id":"06ca46df-0d8c-4c6f-82ca-f818903245fa"},"outputs":[],"source":["if not skip:\n","\n","    base_model_selection['top1_accuracy_score'] = (base_model_selection['top1_accuracy'] - min(base_model_selection['top1_accuracy'])) / (max(base_model_selection['top1_accuracy']) - min(base_model_selection['top1_accuracy']))\n","    base_model_selection['inference_time_score'] = (max(base_model_selection['inference_time']) - base_model_selection['inference_time']) / (max(base_model_selection['inference_time']) - min(base_model_selection['inference_time']))\n","    base_model_selection['weighted_score'] = base_model_selection['top1_accuracy_score']*params['top1_accuracy_weight'] + base_model_selection['inference_time_score']*params['inference_time_weight']\n","    base_model_selection['rank'] = base_model_selection['weighted_score'].rank(ascending=False).astype(int)\n","    base_model_selection.sort_values('rank', inplace=True)\n","    display(base_model_selection)"]},{"cell_type":"markdown","id":"90c099c3-0632-43e8-9ab9-9e6cf6075ab7","metadata":{"id":"90c099c3-0632-43e8-9ab9-9e6cf6075ab7"},"source":["#### Selecting the most suitable model"]},{"cell_type":"code","execution_count":null,"id":"6e55ab01-01c0-40ba-bc77-70fa21c425c2","metadata":{"id":"6e55ab01-01c0-40ba-bc77-70fa21c425c2"},"outputs":[],"source":["if not skip:\n","\n","    base_model_info = base_models_sizes.loc[base_model_selection.index[0]: base_model_selection.index[0]]"]},{"cell_type":"markdown","id":"1ef00c1f-b995-4508-a6f4-86ba97b8cfe6","metadata":{"id":"1ef00c1f-b995-4508-a6f4-86ba97b8cfe6"},"source":["#### Saving results to Google Drive"]},{"cell_type":"code","execution_count":null,"id":"81f9c7c1-0d97-4b7a-86d9-14c6620f2c1d","metadata":{"id":"81f9c7c1-0d97-4b7a-86d9-14c6620f2c1d"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.save_stage_processing(base_model_selection)\n","    pipeline.save_stage_result(base_model_info)"]},{"cell_type":"markdown","id":"e4058e38-8303-408b-848c-b4d1b493199b","metadata":{"id":"e4058e38-8303-408b-848c-b4d1b493199b"},"source":["#### Fixing the completion of the stage"]},{"cell_type":"code","execution_count":null,"id":"10f8468d-04d6-4aed-ae8e-3de9417152fe","metadata":{"id":"10f8468d-04d6-4aed-ae8e-3de9417152fe"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.complete_stage()"]},{"cell_type":"markdown","id":"baae31ad-bfc7-4076-9233-6cbeedf282d6","metadata":{"id":"baae31ad-bfc7-4076-9233-6cbeedf282d6"},"source":["#### Loading results from Google Drive (done when step is skipped)"]},{"cell_type":"code","execution_count":null,"id":"ae30e59d-53e0-41f5-ac07-a56b66ced9b0","metadata":{"id":"ae30e59d-53e0-41f5-ac07-a56b66ced9b0"},"outputs":[],"source":["if skip:\n","\n","    if pipeline.is_stage_complete:\n","        base_model_info = pipeline.load_stage_result().set_index('base_model_name')"]},{"cell_type":"markdown","id":"f7292fbb-881d-4b75-a466-64e3fdeac88c","metadata":{"id":"f7292fbb-881d-4b75-a466-64e3fdeac88c"},"source":["#### Output of results"]},{"cell_type":"code","execution_count":null,"id":"4f903222-23e6-4a3b-bf29-8316a74526ca","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1743952733627,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"4f903222-23e6-4a3b-bf29-8316a74526ca","outputId":"203afd60-b979-47df-97ed-c8c1ba5ae711"},"outputs":[{"output_type":"display_data","data":{"text/plain":["                  image_size  feature_size\n","base_model_name                           \n","EfficientNetV2B0         224          1280"],"text/html":["\n","  <div id=\"df-3692ff35-3a3e-4fc6-a6b5-51e3a60c024c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_size</th>\n","      <th>feature_size</th>\n","    </tr>\n","    <tr>\n","      <th>base_model_name</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>EfficientNetV2B0</th>\n","      <td>224</td>\n","      <td>1280</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3692ff35-3a3e-4fc6-a6b5-51e3a60c024c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3692ff35-3a3e-4fc6-a6b5-51e3a60c024c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3692ff35-3a3e-4fc6-a6b5-51e3a60c024c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_9ef2f64a-c657-40e7-a553-3fee0a4d7a9e\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('base_model_info')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_9ef2f64a-c657-40e7-a553-3fee0a4d7a9e button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('base_model_info');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"base_model_info","summary":"{\n  \"name\": \"base_model_info\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"base_model_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"EfficientNetV2B0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 224,\n        \"max\": 224,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          224\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1280,\n        \"max\": 1280,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1280\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}],"source":["if pipeline.is_stage_complete:\n","    base_model_name = base_model_info.index[0]\n","    image_size, feature_size = base_model_info.iloc[0]\n","    display(base_model_info)"]},{"cell_type":"markdown","id":"576d4baa-f44d-4f7d-8497-227f3d810a08","metadata":{"id":"576d4baa-f44d-4f7d-8497-227f3d810a08"},"source":["### Pipeline Execution Report"]},{"cell_type":"code","execution_count":null,"id":"fc1c55b3-ab18-4602-8b90-aeaafc9bffe2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1743952733661,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"fc1c55b3-ab18-4602-8b90-aeaafc9bffe2","outputId":"2e5b4e3a-a2ca-49e5-efd5-e22353d32e06"},"outputs":[{"output_type":"display_data","data":{"text/plain":["                                                                     params  \\\n","stage                                                                         \n","sizes_retrieving                     {'result_csv': 'base_model_sizes.csv'}   \n","inference_time_measuring  {'batch_size': 1, 'batches': 1, 'repetitions':...   \n","base_model_selection      {'inference_time_weight': 0.6, 'top1_accuracy_...   \n","\n","                         platform                  start_time  \\\n","stage                                                           \n","sizes_retrieving            colab  2025-03-31 10:22:43.463421   \n","inference_time_measuring    colab  2025-03-31 10:23:23.113022   \n","base_model_selection        colab  2025-03-31 10:26:02.493997   \n","\n","                                         update_time     state  \n","stage                                                           \n","sizes_retrieving          2025-03-31 10:23:23.066135  complete  \n","inference_time_measuring  2025-03-31 10:26:02.449111  complete  \n","base_model_selection      2025-03-31 10:26:02.583442  complete  "],"text/html":["\n","  <div id=\"df-e437b8f2-88a3-4282-b41a-2fedcf45480e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>params</th>\n","      <th>platform</th>\n","      <th>start_time</th>\n","      <th>update_time</th>\n","      <th>state</th>\n","    </tr>\n","    <tr>\n","      <th>stage</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>sizes_retrieving</th>\n","      <td>{'result_csv': 'base_model_sizes.csv'}</td>\n","      <td>colab</td>\n","      <td>2025-03-31 10:22:43.463421</td>\n","      <td>2025-03-31 10:23:23.066135</td>\n","      <td>complete</td>\n","    </tr>\n","    <tr>\n","      <th>inference_time_measuring</th>\n","      <td>{'batch_size': 1, 'batches': 1, 'repetitions':...</td>\n","      <td>colab</td>\n","      <td>2025-03-31 10:23:23.113022</td>\n","      <td>2025-03-31 10:26:02.449111</td>\n","      <td>complete</td>\n","    </tr>\n","    <tr>\n","      <th>base_model_selection</th>\n","      <td>{'inference_time_weight': 0.6, 'top1_accuracy_...</td>\n","      <td>colab</td>\n","      <td>2025-03-31 10:26:02.493997</td>\n","      <td>2025-03-31 10:26:02.583442</td>\n","      <td>complete</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e437b8f2-88a3-4282-b41a-2fedcf45480e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e437b8f2-88a3-4282-b41a-2fedcf45480e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e437b8f2-88a3-4282-b41a-2fedcf45480e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-b5f1dfea-2464-4bc6-a519-88a8d483f489\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b5f1dfea-2464-4bc6-a519-88a8d483f489')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b5f1dfea-2464-4bc6-a519-88a8d483f489 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(pipeline\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"stage\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"sizes_retrieving\",\n          \"inference_time_measuring\",\n          \"base_model_selection\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"params\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"{'result_csv': 'base_model_sizes.csv'}\",\n          \"{'batch_size': 1, 'batches': 1, 'repetitions': 100, 'result_csv': 'model_inference_times.csv'}\",\n          \"{'inference_time_weight': 0.6, 'top1_accuracy_weight': 0.4, 'process_csv': 'base_model_selection.csv', 'result_csv': 'base_model.csv'}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"platform\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"colab\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start_time\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2025-03-31 10:22:43.463421\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"update_time\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2025-03-31 10:23:23.066135\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"state\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"complete\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}],"source":["display(pipeline.report)"]},{"cell_type":"markdown","id":"8978435c-9fce-4f31-a19e-e1c7d6a81a8e","metadata":{"id":"8978435c-9fce-4f31-a19e-e1c7d6a81a8e","tags":[]},"source":["## Image Preprocessing Pipeline"]},{"cell_type":"markdown","id":"0232b656-9d83-4a79-ac00-65728a4b3103","metadata":{"id":"0232b656-9d83-4a79-ac00-65728a4b3103","jp-MarkdownHeadingCollapsed":true},"source":["### Create/download pipeline from Google Drive"]},{"cell_type":"code","execution_count":null,"id":"9cbeada1-6a12-433a-a03d-babf279d2227","metadata":{"id":"9cbeada1-6a12-433a-a03d-babf279d2227"},"outputs":[],"source":["pipeline = Pipeline(config=IMAGE_PREPROCESSING_PIPELINE, proj_path=gd_proj_path, is_prev_complete=pipeline.is_complete, platform=platform)"]},{"cell_type":"markdown","id":"4ffc5be3-1536-40ef-be38-57693bc26d68","metadata":{"id":"4ffc5be3-1536-40ef-be38-57693bc26d68","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Extracting face images from the training dataset"]},{"cell_type":"code","execution_count":null,"id":"57c77039-7954-4824-8408-316fa7c2ecc6","metadata":{"id":"57c77039-7954-4824-8408-316fa7c2ecc6"},"outputs":[],"source":["pipeline.next_stage()\n","skip = pipeline.is_stage_skipped\n","params = pipeline.stage_params\n","train_faces_dataset_path = Path(params['path'])"]},{"cell_type":"markdown","id":"050f80aa-3b70-4a1c-9bff-36ad1fd0c53c","metadata":{"id":"050f80aa-3b70-4a1c-9bff-36ad1fd0c53c"},"source":["#### Creating the dataset folder structure"]},{"cell_type":"code","execution_count":null,"id":"0d796d5d-8d92-4cbd-874c-1830fd114de5","metadata":{"id":"0d796d5d-8d92-4cbd-874c-1830fd114de5"},"outputs":[],"source":["if not skip:\n","\n","    if train_faces_dataset_path.exists():\n","        shutil.rmtree(train_faces_dataset_path)\n","    train_faces_dataset_path.mkdir()\n","    for emotion in list(EMOTIONS):\n","        (train_faces_dataset_path / emotion).mkdir()"]},{"cell_type":"markdown","id":"7cac7ab6-e01a-4c09-a9a9-7bf5a0281078","metadata":{"id":"7cac7ab6-e01a-4c09-a9a9-7bf5a0281078"},"source":["#### Loading the original training dataset"]},{"cell_type":"code","execution_count":null,"id":"9c162a11-1f21-4a0d-b3b6-9eef63a794b9","metadata":{"id":"9c162a11-1f21-4a0d-b3b6-9eef63a794b9"},"outputs":[],"source":["if not skip:\n","\n","    train_dataset_path = Path(TRAIN_DATASET_PATH)\n","    if not train_dataset_path.exists():\n","        gdown.cached_download(\n","            url=TRAIN_DATASET_URL,\n","            path=f'temp.{TRAIN_DATASET_EXT}',\n","            postprocess=gdown.extractall,\n","            fuzzy=True)\n","        Path(f'temp.{TRAIN_DATASET_EXT}').unlink()"]},{"cell_type":"markdown","id":"bd034179-b872-4aa0-82c1-d92ed4e42630","metadata":{"id":"bd034179-b872-4aa0-82c1-d92ed4e42630"},"source":["#### Getting a list of image files in the training dataset"]},{"cell_type":"code","execution_count":null,"id":"19bb2654-96e1-4286-a7fb-facc904c02b0","metadata":{"id":"19bb2654-96e1-4286-a7fb-facc904c02b0"},"outputs":[],"source":["if not skip:\n","\n","    file_paths = [Path(file_path).relative_to(train_dataset_path).as_posix()\n","                  for file_path in utils.image_dataset_from_directory(train_dataset_path, shuffle=False, batch_size=1).file_paths]"]},{"cell_type":"markdown","id":"66708202-6473-4142-8ad5-996314b611a7","metadata":{"id":"66708202-6473-4142-8ad5-996314b611a7"},"source":["#### Creating a process table"]},{"cell_type":"code","execution_count":null,"id":"bed488a1-fd8f-4a57-b500-6e7e7a29eed0","metadata":{"id":"bed488a1-fd8f-4a57-b500-6e7e7a29eed0"},"outputs":[],"source":["if not skip:\n","\n","    processing = pd.DataFrame(\n","        columns = ['file_path', 'min_size', 'face_box']\n","    )\n","    processing['file_path'] = file_paths"]},{"cell_type":"markdown","id":"8ae04f10-dcdd-4241-b85a-71cd473c27c5","metadata":{"id":"8ae04f10-dcdd-4241-b85a-71cd473c27c5","jp-MarkdownHeadingCollapsed":true},"source":["#### Creating a results table"]},{"cell_type":"code","execution_count":null,"id":"e88cc764-4349-4ff0-ba2e-6462a12020cb","metadata":{"id":"e88cc764-4349-4ff0-ba2e-6462a12020cb"},"outputs":[],"source":["if not skip:\n","\n","    train_face_extraction = pd.DataFrame(\n","        columns = [\n","            'emotion',\n","            'failed_images',\n","            'detected_faces',\n","        ],\n","    )\n","    train_face_extraction['emotion'] = list(EMOTIONS)\n","    train_face_extraction.set_index('emotion', inplace=True)"]},{"cell_type":"markdown","id":"3bc02057-9107-4d3b-a636-a204a5cc0f61","metadata":{"id":"3bc02057-9107-4d3b-a636-a204a5cc0f61"},"source":["#### Creating face detector"]},{"cell_type":"code","execution_count":null,"id":"349e6efa-af0b-45f6-a06e-d84dd3330120","metadata":{"id":"349e6efa-af0b-45f6-a06e-d84dd3330120"},"outputs":[],"source":["if not skip:\n","\n","    # Create a face detector\n","    face_detector = cv.CascadeClassifier(Path(cv.data.haarcascades) / params['classifier'])"]},{"cell_type":"markdown","id":"c8a972ba-5d9a-4938-9e1d-e99b36485b25","metadata":{"id":"c8a972ba-5d9a-4938-9e1d-e99b36485b25"},"source":["#### Detecting faces"]},{"cell_type":"code","execution_count":null,"id":"d9ac2bf7-444c-46db-8b90-12d9208e55af","metadata":{"id":"d9ac2bf7-444c-46db-8b90-12d9208e55af"},"outputs":[],"source":["if not skip:\n","\n","    with trange(processing.shape[0], unit='file') as t:\n","\n","        min_sizes = []\n","        face_boxes = []\n","\n","        for file_path in processing['file_path']:\n","\n","            # Load a image\n","            image_path = (train_dataset_path / file_path).as_posix()\n","            image = cv.imread(image_path)\n","\n","            # convert to gray scale\n","            grayscaled_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n","\n","            # calculate min size of face\n","            min_size = int(image.shape[0] * params['face_min_ratio']), int(image.shape[1] * params['face_min_ratio'])\n","\n","            # Detects faces of different sizes in the input image\n","            face_box = face_detector.detectMultiScale(\n","                grayscaled_image,\n","                params['scale_factor'], params['min_neighbors'], params['flags'], min_size\n","            )\n","\n","            min_sizes.append(min_size)\n","            face_boxes.append(face_box)\n","\n","            # Updating the Progress Bar counter\n","            t.update()\n","\n","        # Enter the result into the process table\n","        processing['min_size'] = min_sizes\n","        processing['face_box'] = face_boxes"]},{"cell_type":"markdown","id":"9679b7bf-fa52-4ee7-8b11-97f0eab24832","metadata":{"id":"9679b7bf-fa52-4ee7-8b11-97f0eab24832"},"source":["#### Saving train faces dataset"]},{"cell_type":"code","execution_count":null,"id":"cb5dcc32-c0cc-455c-8bc0-58678af42340","metadata":{"id":"cb5dcc32-c0cc-455c-8bc0-58678af42340","scrolled":true},"outputs":[],"source":["if not skip:\n","\n","    with trange(processing.shape[0], unit='file') as t:\n","\n","        for _, (file_path, face_box) in processing[['file_path', 'face_box']].iterrows():\n","\n","            # Load a image\n","            with Image.open(train_dataset_path / file_path) as image:\n","\n","                if len(face_box) == 0:\n","                    # If the detector did not find a face image,\n","                    # save orginal image\n","                    image.save(train_faces_dataset_path / file_path)\n","                else:\n","                    # Extract and savthe image of the first face found\n","                    x1, y1, w, h = face_box[0]\n","                    x2 = x1 + w\n","                    y2 = y1 + h\n","                    face_image = image.crop((x1, y1, x2, y2))\n","                    face_image.save(train_faces_dataset_path / file_path)\n","\n","                t.update()"]},{"cell_type":"markdown","id":"7892e9b3-193e-4586-bad0-133892c5e4ea","metadata":{"id":"7892e9b3-193e-4586-bad0-133892c5e4ea"},"source":["#### Saving training dataset face images to Google Drive archive"]},{"cell_type":"code","execution_count":null,"id":"06bbcd14-1263-4c58-a926-4f594ea873f4","metadata":{"id":"06bbcd14-1263-4c58-a926-4f594ea873f4"},"outputs":[],"source":["if not skip:\n","\n","    if not pipeline.is_stage_failed:\n","        shutil.make_archive(gd_proj_path / train_faces_dataset_path.name, 'zip', train_faces_dataset_path)"]},{"cell_type":"markdown","id":"bc70230f-4975-4f73-9c50-008f2571c98e","metadata":{"id":"bc70230f-4975-4f73-9c50-008f2571c98e"},"source":["#### Saving results to Google Drive"]},{"cell_type":"code","execution_count":null,"id":"1121d630-45ff-469e-9d80-757845ee573e","metadata":{"id":"1121d630-45ff-469e-9d80-757845ee573e"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.save_stage_processing(processing)\n","    if not pipeline.is_stage_failed:\n","        for emotion in EMOTIONS:\n","            indices = processing.index[processing['file_path'].str.contains(emotion)]\n","            images_num = indices.size\n","            detected_num = sum(processing.loc[indices, 'face_box'].apply(len) > 0)\n","            failed_num = images_num - detected_num\n","            train_face_extraction.loc[emotion] = failed_num, detected_num\n","        train_face_extraction = train_face_extraction.astype(int)\n","        pipeline.save_stage_result(train_face_extraction)"]},{"cell_type":"markdown","id":"f39bc921-ef0c-4d8b-a107-4f6a75256c7c","metadata":{"id":"f39bc921-ef0c-4d8b-a107-4f6a75256c7c"},"source":["#### Fixing the completion of the stage"]},{"cell_type":"code","execution_count":null,"id":"9794df16-668d-4dd8-928d-bd69ab7cae33","metadata":{"id":"9794df16-668d-4dd8-928d-bd69ab7cae33"},"outputs":[],"source":["if not skip and not pipeline.is_stage_failed:\n","\n","    pipeline.complete_stage()"]},{"cell_type":"markdown","id":"3d7ef9a4-15f3-4238-a231-07ce5abcd56a","metadata":{"id":"3d7ef9a4-15f3-4238-a231-07ce5abcd56a"},"source":["#### Loading results from Google Drive (done when step is skipped)"]},{"cell_type":"code","execution_count":null,"id":"f9dc8620-c135-475b-b1da-60ab85b0b11a","metadata":{"id":"f9dc8620-c135-475b-b1da-60ab85b0b11a"},"outputs":[],"source":["if skip:\n","\n","    if pipeline.is_stage_complete:\n","        train_face_extraction = pipeline.load_stage_result().set_index('emotion')"]},{"cell_type":"markdown","id":"1dc32205-7add-4c8b-80fd-d3bc92a6926e","metadata":{"id":"1dc32205-7add-4c8b-80fd-d3bc92a6926e"},"source":["#### Output of results"]},{"cell_type":"code","execution_count":null,"id":"72dcc8c7-cbdb-4363-a377-e54fa6f3a87e","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1743952733790,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"72dcc8c7-cbdb-4363-a377-e54fa6f3a87e","outputId":"6503c6ad-551a-42b2-ebbe-797fe29a2dd8"},"outputs":[{"output_type":"display_data","data":{"text/plain":["           failed_images  detected_faces\n","emotion                                 \n","anger                275            6748\n","contempt              62            3023\n","disgust              109            3046\n","fear                 242            4802\n","happy                168            5787\n","neutral              249            6546\n","sad                  374            6366\n","surprise             315            6008\n","uncertain            257            5670"],"text/html":["\n","  <div id=\"df-9d7d3812-21d9-4eb6-822e-858c229435c4\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>failed_images</th>\n","      <th>detected_faces</th>\n","    </tr>\n","    <tr>\n","      <th>emotion</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>anger</th>\n","      <td>275</td>\n","      <td>6748</td>\n","    </tr>\n","    <tr>\n","      <th>contempt</th>\n","      <td>62</td>\n","      <td>3023</td>\n","    </tr>\n","    <tr>\n","      <th>disgust</th>\n","      <td>109</td>\n","      <td>3046</td>\n","    </tr>\n","    <tr>\n","      <th>fear</th>\n","      <td>242</td>\n","      <td>4802</td>\n","    </tr>\n","    <tr>\n","      <th>happy</th>\n","      <td>168</td>\n","      <td>5787</td>\n","    </tr>\n","    <tr>\n","      <th>neutral</th>\n","      <td>249</td>\n","      <td>6546</td>\n","    </tr>\n","    <tr>\n","      <th>sad</th>\n","      <td>374</td>\n","      <td>6366</td>\n","    </tr>\n","    <tr>\n","      <th>surprise</th>\n","      <td>315</td>\n","      <td>6008</td>\n","    </tr>\n","    <tr>\n","      <th>uncertain</th>\n","      <td>257</td>\n","      <td>5670</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d7d3812-21d9-4eb6-822e-858c229435c4')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-9d7d3812-21d9-4eb6-822e-858c229435c4 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-9d7d3812-21d9-4eb6-822e-858c229435c4');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-8cfd391f-9ddf-46b2-a23d-d0a5b5c23e4c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8cfd391f-9ddf-46b2-a23d-d0a5b5c23e4c')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-8cfd391f-9ddf-46b2-a23d-d0a5b5c23e4c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_af892fce-9b2c-4480-9443-b18fefb33179\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train_face_extraction')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_af892fce-9b2c-4480-9443-b18fefb33179 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('train_face_extraction');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"train_face_extraction","summary":"{\n  \"name\": \"train_face_extraction\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"emotion\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"surprise\",\n          \"contempt\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"failed_images\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 98,\n        \"min\": 62,\n        \"max\": 374,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          315,\n          62,\n          249\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"detected_faces\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1421,\n        \"min\": 3023,\n        \"max\": 6748,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          6008,\n          3023,\n          6546\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}],"source":["if pipeline.is_stage_complete:\n","    display(train_face_extraction)"]},{"cell_type":"markdown","id":"74a38e44-f564-4814-90f5-5dae88f7fd93","metadata":{"id":"74a38e44-f564-4814-90f5-5dae88f7fd93","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Extracting face images from the test dataset"]},{"cell_type":"code","execution_count":null,"id":"fb585ea5-1d73-457a-84a6-ab4f64b82583","metadata":{"id":"fb585ea5-1d73-457a-84a6-ab4f64b82583"},"outputs":[],"source":["pipeline.next_stage()\n","skip = pipeline.is_stage_skipped\n","params = pipeline.stage_params\n","test_faces_dataset_path = Path(params['path'])"]},{"cell_type":"markdown","id":"0d641810-4ff6-4f9c-81fa-9dafc639bf22","metadata":{"id":"0d641810-4ff6-4f9c-81fa-9dafc639bf22"},"source":["#### Loading the original test dataset"]},{"cell_type":"code","execution_count":null,"id":"ddd97340-cfd2-43e0-8818-a6a6ffeba5a0","metadata":{"id":"ddd97340-cfd2-43e0-8818-a6a6ffeba5a0"},"outputs":[],"source":["if not skip:\n","\n","    test_dataset_path = Path(TEST_DATASET_PATH)\n","    if not test_dataset_path.exists():\n","        gdown.cached_download(\n","            url=TEST_DATASET_URL,\n","            path=f'temp.{TEST_DATASET_EXT}',\n","            postprocess=gdown.extractall,\n","            fuzzy=True)\n","        Path(f'temp.{TEST_DATASET_EXT}').unlink()"]},{"cell_type":"markdown","id":"5aac3fa5-9a53-4895-ad70-ad32138ccf14","metadata":{"id":"5aac3fa5-9a53-4895-ad70-ad32138ccf14"},"source":["#### Creating a dataset folder"]},{"cell_type":"code","execution_count":null,"id":"5dce23eb-9299-41f5-b9f9-e5a0662ccb0a","metadata":{"id":"5dce23eb-9299-41f5-b9f9-e5a0662ccb0a"},"outputs":[],"source":["if not skip:\n","\n","    if test_faces_dataset_path.exists():\n","        shutil.rmtree(test_faces_dataset_path)\n","    test_faces_dataset_path.mkdir()"]},{"cell_type":"markdown","id":"fd582638-bf43-4e65-9a22-3566e658dd42","metadata":{"id":"fd582638-bf43-4e65-9a22-3566e658dd42"},"source":["#### Getting a list of image files in the source dataset"]},{"cell_type":"code","execution_count":null,"id":"53b09d46-844a-45a8-9c17-8e27151a5b6e","metadata":{"id":"53b09d46-844a-45a8-9c17-8e27151a5b6e"},"outputs":[],"source":["if not skip:\n","\n","    file_paths = [Path(file_path).relative_to(test_dataset_path).as_posix()\n","                  for file_path in utils.image_dataset_from_directory(test_dataset_path, labels=None, shuffle=False, batch_size=1).file_paths]"]},{"cell_type":"markdown","id":"a1ad6275-b43a-4ef8-9880-b87b47e9b003","metadata":{"id":"a1ad6275-b43a-4ef8-9880-b87b47e9b003"},"source":["#### Creating a process table"]},{"cell_type":"code","execution_count":null,"id":"3c7cc3da-2e49-4964-9311-505c3a263968","metadata":{"id":"3c7cc3da-2e49-4964-9311-505c3a263968"},"outputs":[],"source":["if not skip:\n","\n","    processing = pd.DataFrame(\n","        columns = ['file_path', 'face_box']\n","    )\n","    processing['file_path'] = file_paths"]},{"cell_type":"markdown","id":"2d0a1a30-da90-4b29-b4df-dc7accdc9317","metadata":{"id":"2d0a1a30-da90-4b29-b4df-dc7accdc9317"},"source":["#### Creating a results table"]},{"cell_type":"code","execution_count":null,"id":"3938940d-c742-4ef5-8405-c571f7832c25","metadata":{"id":"3938940d-c742-4ef5-8405-c571f7832c25"},"outputs":[],"source":["if not skip:\n","\n","    test_face_extraction = pd.DataFrame(\n","        columns = [\n","            'emotion',\n","            'failed_images',\n","            'detected_faces',\n","        ],\n","    )\n","    test_face_extraction['emotion'] = ['all']\n","    test_face_extraction.set_index('emotion', inplace=True)"]},{"cell_type":"markdown","id":"d3dd5015-b3d5-4c69-ab98-c65e40ee06fb","metadata":{"id":"d3dd5015-b3d5-4c69-ab98-c65e40ee06fb"},"source":["#### Creating face detector"]},{"cell_type":"code","execution_count":null,"id":"54417c9f-594d-4774-a99e-61458c25053b","metadata":{"id":"54417c9f-594d-4774-a99e-61458c25053b"},"outputs":[],"source":["if not skip:\n","\n","    # Create a face detector\n","    face_detector = cv.CascadeClassifier(Path(cv.data.haarcascades) / params['classifier'])"]},{"cell_type":"markdown","id":"9ac8d13b-5894-4e7d-ac6d-01c1b6b3836e","metadata":{"id":"9ac8d13b-5894-4e7d-ac6d-01c1b6b3836e"},"source":["#### Extracting face images from the test dataset"]},{"cell_type":"code","execution_count":null,"id":"35c9c529-c71e-4702-bfe0-089d54506b5a","metadata":{"id":"35c9c529-c71e-4702-bfe0-089d54506b5a","scrolled":true},"outputs":[],"source":["if not skip:\n","\n","    with trange(processing.shape[0], unit='file') as t:\n","\n","        min_sizes = []\n","        face_boxes = []\n","\n","        for file_path in processing['file_path']:\n","\n","            # Load a image\n","            image_path = (test_dataset_path / file_path).as_posix()\n","            image = cv.imread(image_path)\n","\n","            # convert to gray scale\n","            grayscaled_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n","\n","            # calculate min size of face\n","            min_size = int(image.shape[0] * params['face_min_ratio']), int(image.shape[1] * params['face_min_ratio'])\n","\n","            # Detects faces of different sizes in the input image\n","            face_box = face_detector.detectMultiScale(\n","                grayscaled_image,\n","                params['scale_factor'], params['min_neighbors'], params['flags'], min_size\n","            )\n","\n","            min_sizes.append(min_size)\n","            face_boxes.append(face_box)\n","\n","            # Updating the Progress Bar counter\n","            t.update()\n","\n","        # Enter the result into the process table\n","        processing['min_size'] = min_sizes\n","        processing['face_box'] = face_boxes"]},{"cell_type":"markdown","id":"232db2d2-fd02-4fad-a900-3a7dc367e09d","metadata":{"id":"232db2d2-fd02-4fad-a900-3a7dc367e09d"},"source":["#### Saving test faces dataset"]},{"cell_type":"code","execution_count":null,"id":"1cb85745-717c-47a0-8ecc-66138d43dc8f","metadata":{"id":"1cb85745-717c-47a0-8ecc-66138d43dc8f"},"outputs":[],"source":["if not skip:\n","\n","    with trange(processing.shape[0], unit='file') as t:\n","\n","        for _, (file_path, face_box) in processing[['file_path', 'face_box']].iterrows():\n","\n","            # Load a image\n","            with Image.open(test_dataset_path / file_path) as image:\n","\n","                if len(face_box) == 0:\n","                    # If the detector did not find a face image,\n","                    # save orginal image\n","                    image.save(test_faces_dataset_path / file_path)\n","                else:\n","                    # Extract and savthe image of the first face found\n","                    x1, y1, w, h = face_box[0]\n","                    x2 = x1 + w\n","                    y2 = y1 + h\n","                    face_image = image.crop((x1, y1, x2, y2))\n","                    face_image.save(test_faces_dataset_path / file_path)\n","\n","                t.update()"]},{"cell_type":"markdown","id":"a6e27849-160e-41ac-b544-6d281747d793","metadata":{"id":"a6e27849-160e-41ac-b544-6d281747d793"},"source":["#### Saving test dataset face images to Google Drive archive"]},{"cell_type":"code","execution_count":null,"id":"d12343f9-624c-4404-aed4-0632bf55f68d","metadata":{"id":"d12343f9-624c-4404-aed4-0632bf55f68d"},"outputs":[],"source":["if not skip:\n","\n","    if not pipeline.is_stage_failed:\n","        shutil.make_archive(gd_proj_path / test_faces_dataset_path.name, 'zip', test_faces_dataset_path)"]},{"cell_type":"markdown","id":"b4d200ab-1133-44f6-92dc-f224ff026796","metadata":{"id":"b4d200ab-1133-44f6-92dc-f224ff026796"},"source":["#### Saving results to Google Drive"]},{"cell_type":"code","execution_count":null,"id":"6da9f696-00c8-4602-92e6-98f3ca833197","metadata":{"id":"6da9f696-00c8-4602-92e6-98f3ca833197"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.save_stage_processing(processing)\n","    if not pipeline.is_stage_failed:\n","        images_num = processing.shape[0]\n","        detected_num = sum(processing['face_box'].apply(len) > 0)\n","        failed_num = images_num - detected_num\n","        test_face_extraction.loc['all'] = failed_num, detected_num\n","        test_face_extraction = test_face_extraction.astype(int)\n","        pipeline.save_stage_result(test_face_extraction)"]},{"cell_type":"markdown","id":"790cae71-a262-4517-8e83-b6b20e9367e0","metadata":{"id":"790cae71-a262-4517-8e83-b6b20e9367e0"},"source":["#### Fixing the completion of the stage"]},{"cell_type":"code","execution_count":null,"id":"e0b117e4-0aaf-4216-9821-c643d7b5327a","metadata":{"id":"e0b117e4-0aaf-4216-9821-c643d7b5327a"},"outputs":[],"source":["if not skip and not pipeline.is_stage_failed:\n","\n","    pipeline.complete_stage()"]},{"cell_type":"markdown","id":"2d0fe797-abc5-4080-baed-03e727b13711","metadata":{"id":"2d0fe797-abc5-4080-baed-03e727b13711"},"source":["#### Loading results from Google Drive (done when step is skipped)"]},{"cell_type":"code","execution_count":null,"id":"5bb21bdf-4430-4907-b1c7-213e642001c2","metadata":{"id":"5bb21bdf-4430-4907-b1c7-213e642001c2"},"outputs":[],"source":["if skip:\n","\n","    if pipeline.is_stage_complete:\n","        test_face_extraction = pipeline.load_stage_result().set_index('emotion')"]},{"cell_type":"markdown","id":"3946c528-02fc-40c5-8d9b-9882ae9d4619","metadata":{"id":"3946c528-02fc-40c5-8d9b-9882ae9d4619"},"source":["#### Output of results"]},{"cell_type":"code","execution_count":null,"id":"475aa029-f87f-4e70-84d1-caad74603fa9","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1743952733892,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"475aa029-f87f-4e70-84d1-caad74603fa9","outputId":"2ddf7e13-31d4-40bc-aa73-986e5f07438f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["         failed_images  detected_faces\n","emotion                               \n","all                175            4825"],"text/html":["\n","  <div id=\"df-76ee3744-e3d0-43df-af74-6b7ab2e1a2b9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>failed_images</th>\n","      <th>detected_faces</th>\n","    </tr>\n","    <tr>\n","      <th>emotion</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>all</th>\n","      <td>175</td>\n","      <td>4825</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76ee3744-e3d0-43df-af74-6b7ab2e1a2b9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-76ee3744-e3d0-43df-af74-6b7ab2e1a2b9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-76ee3744-e3d0-43df-af74-6b7ab2e1a2b9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_4f777ad9-afc2-4031-909b-c95870871427\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_face_extraction')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_4f777ad9-afc2-4031-909b-c95870871427 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('test_face_extraction');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"test_face_extraction","summary":"{\n  \"name\": \"test_face_extraction\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"emotion\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"all\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"failed_images\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 175,\n        \"max\": 175,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          175\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"detected_faces\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 4825,\n        \"max\": 4825,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4825\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}],"source":["if pipeline.is_stage_complete:\n","    display(test_face_extraction)"]},{"cell_type":"markdown","id":"0c47a32c-a1b8-4593-93e3-dc14085875db","metadata":{"id":"0c47a32c-a1b8-4593-93e3-dc14085875db","tags":[]},"source":["### Extracting features from training dataset"]},{"cell_type":"code","execution_count":null,"id":"4f8cab31-e5a5-4c7c-81f4-d7059b34b8a9","metadata":{"id":"4f8cab31-e5a5-4c7c-81f4-d7059b34b8a9"},"outputs":[],"source":["pipeline.next_stage()\n","skip = pipeline.is_stage_skipped\n","params = pipeline.stage_params\n","train_features_dataset_path = Path(params['path'])"]},{"cell_type":"markdown","id":"772ad2a0-4fb8-4396-9e15-eba9054ebb15","metadata":{"id":"772ad2a0-4fb8-4396-9e15-eba9054ebb15","jp-MarkdownHeadingCollapsed":true},"source":["#### Reading training dataset of face images from archive on Google Drive"]},{"cell_type":"code","execution_count":null,"id":"62994dec-1485-48c0-a0eb-2c0d9aede988","metadata":{"id":"62994dec-1485-48c0-a0eb-2c0d9aede988"},"outputs":[],"source":["if not skip:\n","\n","    if not train_faces_dataset_path.exists():\n","        train_faces_dataset_path.mkdir()\n","        shutil.unpack_archive(\n","            gd_proj_path / train_faces_dataset_path.with_suffix('.zip').name,\n","            train_faces_dataset_path\n","        )"]},{"cell_type":"markdown","id":"352cbcf6-5f3c-4327-a919-3af93174b4fc","metadata":{"id":"352cbcf6-5f3c-4327-a919-3af93174b4fc","jp-MarkdownHeadingCollapsed":true},"source":["#### Creating a directory for feature files"]},{"cell_type":"code","execution_count":null,"id":"4c884b8a-84bb-4acb-80b3-f083c75e5c4d","metadata":{"id":"4c884b8a-84bb-4acb-80b3-f083c75e5c4d"},"outputs":[],"source":["if not skip:\n","\n","    if train_features_dataset_path.exists():\n","        shutil.rmtree(train_features_dataset_path)\n","    train_features_dataset_path.mkdir()"]},{"cell_type":"markdown","id":"b1cf985a-8780-46e3-8a53-2d38ef92bacd","metadata":{"id":"b1cf985a-8780-46e3-8a53-2d38ef92bacd","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["#### Extract and save features for the selected model"]},{"cell_type":"code","execution_count":null,"id":"b629c3b7-3af6-4c6f-907d-4762b8f55379","metadata":{"id":"b629c3b7-3af6-4c6f-907d-4762b8f55379"},"outputs":[],"source":["if not skip:\n","\n","    # Form a dataset with images of the optimal size for the model\n","    # Add the original images to the dataset\n","    dataset = utils.image_dataset_from_directory(\n","        train_faces_dataset_path,\n","        batch_size=params['batch_size'],\n","        label_mode='int',\n","        image_size=(image_size, image_size),\n","        shuffle=False)\n","\n","    # The number of images is determined by the length of the file list\n","    images_cnt = len(dataset.file_paths)\n","\n","    # Initialize the pre-production of the next batch\n","    dataset = dataset.prefetch(buffer_size=params['buffer_size'])\n","\n","    # Create arrays of features and labels\n","    features = np.zeros(shape=(images_cnt, feature_size))\n","    labels = np.zeros(shape=(images_cnt, 1))\n","\n","    # Create a base model trained on the ImageNet dataset\n","    base_model = build_base_model(\n","        base_model_name, weights='imagenet',\n","        image_size=image_size,\n","        pooling=BASE_MODEL_POOLINGS,\n","        include_preprocess_input=True,\n","        training=False\n","    )\n","\n","    # Create a Progress Bar to track the progress of feature extraction\n","    with tqdm(dataset, desc=f'{base_model_name}', unit='batch') as t:\n","\n","        # Slice indices for insertion into feature and label arrays\n","        start_index = 0\n","        end_index = 0\n","\n","        # We go through the batches of the image dataset, each of which consists of a batch of images and a batch of labels\n","        for batch, (batch_images, batch_labels) in enumerate(dataset.as_numpy_iterator()):\n","\n","            end_index += batch_images.shape[0]\n","\n","            # Extract features from a batch of images and insert them into an array\n","            batch_features = base_model(batch_images)\n","            features[start_index: end_index] = batch_features\n","\n","            # Transform the size of the batch of labels and insert it into the array\n","            batch_labels = batch_labels.reshape(-1, 1)\n","            labels[start_index: end_index] = batch_labels\n","\n","            start_index = end_index\n","\n","            # Updating the Progress Bar\n","            t.update()\n","\n","    # Write arrays of features and labels to a file\n","    feature_path = (train_features_dataset_path / base_model_name).with_suffix('.npz')\n","    np.savez(feature_path, features=features, labels=labels)"]},{"cell_type":"markdown","id":"7a5dedce-2a51-4a47-91b4-9396a338ba63","metadata":{"id":"7a5dedce-2a51-4a47-91b4-9396a338ba63","jp-MarkdownHeadingCollapsed":true},"source":["#### Saving the feature file archive to Google Drive"]},{"cell_type":"code","execution_count":null,"id":"8977cf6b-f240-496a-b6f3-c89c8e48c49a","metadata":{"id":"8977cf6b-f240-496a-b6f3-c89c8e48c49a"},"outputs":[],"source":["if not skip:\n","\n","    shutil.make_archive(gd_proj_path / train_features_dataset_path.name, 'zip', train_features_dataset_path)"]},{"cell_type":"markdown","id":"9a6efee7-cdb1-4f1b-853f-ec3ecd4c3e58","metadata":{"id":"9a6efee7-cdb1-4f1b-853f-ec3ecd4c3e58","tags":[]},"source":["#### Fixing the completion of the stage"]},{"cell_type":"code","execution_count":null,"id":"ed7dcfeb-e363-4993-9cec-5ba549315dcd","metadata":{"id":"ed7dcfeb-e363-4993-9cec-5ba549315dcd"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.complete_stage()"]},{"cell_type":"markdown","metadata":{"id":"5823ef54-c6e3-4936-b7eb-543cdb93db5a","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Extracting features from the test dataset"],"id":"5823ef54-c6e3-4936-b7eb-543cdb93db5a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ef41f9e9-179c-4709-bcd0-7b283ba07bda"},"outputs":[],"source":["pipeline.next_stage()\n","skip = pipeline.is_stage_skipped\n","params = pipeline.stage_params\n","test_features_dataset_path = Path(params['path'])"],"id":"ef41f9e9-179c-4709-bcd0-7b283ba07bda"},{"cell_type":"markdown","metadata":{"id":"53e10a4e-fcc6-4687-8305-a147613fb0f0"},"source":["#### Reading a test dataset of face images from the archive on Google Drive"],"id":"53e10a4e-fcc6-4687-8305-a147613fb0f0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4fabeb73-8853-4589-b78b-892926e4f426"},"outputs":[],"source":["if not skip:\n","\n","    if not test_faces_dataset_path.exists():\n","        test_faces_dataset_path.mkdir()\n","        shutil.unpack_archive(\n","            gd_proj_path / test_faces_dataset_path.with_suffix('.zip').name,\n","            test_faces_dataset_path\n","        )"],"id":"4fabeb73-8853-4589-b78b-892926e4f426"},{"cell_type":"markdown","metadata":{"id":"06904f1e-bfdd-486c-b2db-0056b905e19d"},"source":["#### Creating directories for feature files"],"id":"06904f1e-bfdd-486c-b2db-0056b905e19d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9c7ed495-51c9-4448-8180-054e8899aadb"},"outputs":[],"source":["if not skip:\n","\n","    if test_features_dataset_path.exists():\n","        shutil.rmtree(test_features_dataset_path)\n","    test_features_dataset_path.mkdir()"],"id":"9c7ed495-51c9-4448-8180-054e8899aadb"},{"cell_type":"markdown","metadata":{"id":"c45acefb-a4a0-4373-b9a7-a10ec1325e44","tags":[]},"source":["#### Extract and save features for the selected model"],"id":"c45acefb-a4a0-4373-b9a7-a10ec1325e44"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a126265f-e6d6-4f59-b0cb-bdaa589a681c"},"outputs":[],"source":["if not skip:\n","\n","    # Form a dataset with images of the optimal size for the model\n","    # Add the original images to the dataset\n","    dataset = utils.image_dataset_from_directory(\n","        test_faces_dataset_path,\n","        batch_size=params['batch_size'],\n","        label_mode=None,\n","        image_size=(image_size, image_size),\n","        shuffle=False)\n","\n","    # The number of images is determined by the length of the file list\n","    images_cnt = len(dataset.file_paths)\n","\n","    # Initialize the pre-production of the next batch\n","    dataset = dataset.prefetch(buffer_size=params['buffer_size'])\n","\n","    # Create arrays of features\n","    features = np.zeros(shape=(images_cnt, feature_size))\n","\n","    # Create a base model trained on the ImageNet dataset\n","    base_model = build_base_model(\n","        base_model_name, weights='imagenet',\n","        image_size=image_size,\n","        pooling=BASE_MODEL_POOLINGS,\n","        include_preprocess_input=True,\n","        training=False\n","    )\n","\n","    # Create a Progress Bar to track the progress of feature extraction\n","    with tqdm(dataset, desc=f'{base_model_name}', unit='batch') as t:\n","\n","        # Slice indices for insertion into feature and label arrays\n","        start_index = 0\n","        end_index = 0\n","\n","        # We go through the batches of the image dataset, each of which consists of a batch of images and a batch of labels\n","        for batch, batch_images in enumerate(dataset.as_numpy_iterator()):\n","\n","            end_index += batch_images.shape[0]\n","\n","            # Extract features from a batch of images and insert them into an array\n","            batch_features = base_model(batch_images)\n","            features[start_index: end_index] = batch_features\n","\n","            start_index = end_index\n","\n","            # Updating the Progress Bar\n","            t.update()\n","\n","    # Write arrays of features and labels to a file\n","    feature_path = (test_features_dataset_path / base_model_name).with_suffix('.npz')\n","    np.savez(feature_path, features=features)"],"id":"a126265f-e6d6-4f59-b0cb-bdaa589a681c"},{"cell_type":"markdown","metadata":{"id":"fddc6791-43dc-4a85-806c-6b8bde73250c"},"source":["#### Saving the feature file archive to Google Drive"],"id":"fddc6791-43dc-4a85-806c-6b8bde73250c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0696f2a-9c60-4be7-b1e9-940064d2e9f4"},"outputs":[],"source":["if not skip:\n","\n","    shutil.make_archive(gd_proj_path / test_features_dataset_path.name, 'zip', test_features_dataset_path)"],"id":"c0696f2a-9c60-4be7-b1e9-940064d2e9f4"},{"cell_type":"markdown","metadata":{"id":"a7def33f-2c37-4be8-a98e-03bb5ebfeec5"},"source":["#### Fixing the completion of the stage"],"id":"a7def33f-2c37-4be8-a98e-03bb5ebfeec5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2c1e566a-f8da-48d4-a84b-396ed48286ea"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.complete_stage()"],"id":"2c1e566a-f8da-48d4-a84b-396ed48286ea"},{"cell_type":"markdown","id":"e3922ac9-daec-4c9f-be91-02a4181ee515","metadata":{"id":"e3922ac9-daec-4c9f-be91-02a4181ee515","tags":[]},"source":["### Additional cleaning of the training dataset"]},{"cell_type":"code","execution_count":null,"id":"1186907f-870d-4116-81d4-5077b6a5a5f0","metadata":{"id":"1186907f-870d-4116-81d4-5077b6a5a5f0"},"outputs":[],"source":["pipeline.next_stage()\n","skip = pipeline.is_stage_skipped\n","params = pipeline.stage_params\n","train_faces_clean_features_path = Path(params['features_path'])\n","train_faces_clean_dataset_path = Path(params['dataset_path'])"]},{"cell_type":"markdown","id":"86705c20-0dc9-47e0-b22f-93fd3499442d","metadata":{"id":"86705c20-0dc9-47e0-b22f-93fd3499442d"},"source":["#### Creating directories for feature files"]},{"cell_type":"code","execution_count":null,"id":"be47b60a-d414-420b-82e8-3fa3e59aaeb6","metadata":{"id":"be47b60a-d414-420b-82e8-3fa3e59aaeb6"},"outputs":[],"source":["if not skip:\n","\n","    if train_faces_clean_features_path.exists():\n","        shutil.rmtree(train_faces_clean_features_path)\n","    train_faces_clean_features_path.mkdir()"]},{"cell_type":"markdown","id":"26ede546-a656-4e54-985d-2dd06f00415d","metadata":{"id":"26ede546-a656-4e54-985d-2dd06f00415d"},"source":["#### Creating the dataset folder structure"]},{"cell_type":"code","execution_count":null,"id":"36e3b49c-a481-497d-8631-8d7df51155f8","metadata":{"id":"36e3b49c-a481-497d-8631-8d7df51155f8"},"outputs":[],"source":["if not skip:\n","\n","    if train_faces_clean_dataset_path.exists():\n","        shutil.rmtree(train_faces_clean_dataset_path)\n","    train_faces_clean_dataset_path.mkdir()\n","    for emotion in EMOTIONS:\n","        (train_faces_clean_dataset_path / emotion).mkdir()"]},{"cell_type":"markdown","id":"d3667748-2e2f-46ce-9f44-8e76e55d35c2","metadata":{"id":"d3667748-2e2f-46ce-9f44-8e76e55d35c2","tags":[]},"source":["#### Reading training dataset of features from archive on Google Drive"]},{"cell_type":"code","execution_count":null,"id":"97240c0f-776c-43d3-bcda-1feae9e08444","metadata":{"id":"97240c0f-776c-43d3-bcda-1feae9e08444"},"outputs":[],"source":["if not skip:\n","\n","    if not train_features_dataset_path.exists():\n","        train_features_dataset_path.mkdir()\n","        shutil.unpack_archive(\n","            gd_proj_path / train_features_dataset_path.with_suffix('.zip').name,\n","            train_features_dataset_path\n","        )"]},{"cell_type":"markdown","id":"49535f24-5339-4b56-aca7-eed8f7d4e106","metadata":{"id":"49535f24-5339-4b56-aca7-eed8f7d4e106"},"source":["#### Reading training dataset face images from Google Drive archive"]},{"cell_type":"code","execution_count":null,"id":"7e1b02b4-8f4b-4d96-92d4-936b65d4e4d4","metadata":{"id":"7e1b02b4-8f4b-4d96-92d4-936b65d4e4d4"},"outputs":[],"source":["if not skip:\n","\n","    if not train_faces_dataset_path.exists():\n","        train_faces_dataset_path.mkdir()\n","        shutil.unpack_archive(\n","            gd_proj_path / train_faces_dataset_path.with_suffix('.zip').name,\n","            train_faces_dataset_path\n","        )"]},{"cell_type":"markdown","id":"34d97867-34c4-46ce-b73f-9d3d94c075f5","metadata":{"id":"34d97867-34c4-46ce-b73f-9d3d94c075f5"},"source":["#### Getting a list of training dataset face image files"]},{"cell_type":"code","execution_count":null,"id":"0f6d653e-c75a-4512-99f0-4fb3b1a8e5ff","metadata":{"id":"0f6d653e-c75a-4512-99f0-4fb3b1a8e5ff"},"outputs":[],"source":["if not skip:\n","\n","    file_paths = [Path(file_path).relative_to(train_faces_dataset_path).as_posix()\n","                  for file_path in utils.image_dataset_from_directory(train_faces_dataset_path, shuffle=False, batch_size=1).file_paths]"]},{"cell_type":"markdown","id":"b11ecc93-57cb-4945-914f-703b346bb27d","metadata":{"id":"b11ecc93-57cb-4945-914f-703b346bb27d"},"source":["#### Creating a process table"]},{"cell_type":"code","execution_count":null,"id":"23ee048f-424f-4002-aa8b-5772f76318ad","metadata":{"id":"23ee048f-424f-4002-aa8b-5772f76318ad"},"outputs":[],"source":["if not skip:\n","\n","    processing = pd.DataFrame(\n","        columns = ['file_path',\n","                   'similarity_max', 'similar_to', 'similarity', 'duplicated',\n","                   'similarity_median_min', 'similarity_median', 'different',\n","                   'failed']\n","    )\n","    processing['file_path'] = file_paths"]},{"cell_type":"markdown","id":"7e67b15d-7bde-4ca0-8703-b159ce98ed10","metadata":{"id":"7e67b15d-7bde-4ca0-8703-b159ce98ed10"},"source":["#### Creating a results table"]},{"cell_type":"code","execution_count":null,"id":"9162247a-aa09-4537-a46e-5660c6fb50e6","metadata":{"id":"9162247a-aa09-4537-a46e-5660c6fb50e6"},"outputs":[],"source":["if not skip:\n","\n","    train_cleaning = pd.DataFrame(\n","        columns = [\n","            'emotion',\n","            'duplicated_number',\n","            'different_number',\n","            'failed_number',\n","            'remain_number',\n","        ],\n","    )\n","    train_cleaning['emotion'] = list(EMOTIONS)\n","    train_cleaning.set_index('emotion', inplace=True)"]},{"cell_type":"markdown","id":"b4742b72-8f81-4716-9eae-83184104a280","metadata":{"id":"b4742b72-8f81-4716-9eae-83184104a280","tags":[]},"source":["#### Extraction training dataset of features from archive"]},{"cell_type":"code","execution_count":null,"id":"2fb57196-3a6d-44b5-8c18-780aa161e168","metadata":{"id":"2fb57196-3a6d-44b5-8c18-780aa161e168"},"outputs":[],"source":["if not skip:\n","\n","    file_path = (train_features_dataset_path / base_model_name).with_suffix('.npz')\n","    with np.load(file_path, allow_pickle=True) as data:\n","        train_features = data['features']\n","        train_labels = data['labels']"]},{"cell_type":"markdown","id":"4d220e80-d0cb-4714-845a-32b64dd7d4dd","metadata":{"id":"4d220e80-d0cb-4714-845a-32b64dd7d4dd","tags":[]},"source":["#### Identifying features that are too similar to each other and features that are too different from others"]},{"cell_type":"code","execution_count":null,"id":"a50fb06c-b2ad-4f61-aa55-9f65faccdefa","metadata":{"id":"a50fb06c-b2ad-4f61-aa55-9f65faccdefa"},"outputs":[],"source":["if not skip:\n","\n","    index = 0\n","    for emotion in EMOTIONS:\n","        file_paths = [Path(file_path).relative_to(train_faces_dataset_path).as_posix()\n","                      for file_path in utils.image_dataset_from_directory(train_faces_dataset_path / emotion, label_mode=None, shuffle=False, batch_size=1).file_paths]\n","        emotion_size = len(file_paths)\n","\n","        # Get a matrix of feature vectors\n","        features = train_features[index: index+emotion_size]\n","        # Form a matrix of similarity of features between themselves\n","        similarity_matrix = cosine_similarity(features, features)\n","        similarity_rows = [\n","            similarity_matrix[row, row + 1:]\n","            for row in range(similarity_matrix.shape[0])\n","        ]\n","        # Get flatten array of similarity between all combinations of two feature vectors\n","        similarity = np.concatenate(similarity_rows)\n","        # Find the upper bound of reliability\n","        similarity_max = np.mean(similarity) + np.std(similarity) * 3\n","        # Enter the similarity data into the process table\n","        indices = list(range(index, index + emotion_size))\n","        processing.loc[indices, 'similarity_max'] = similarity_max\n","        # Get a list of pairs of features that are too similar to each other\n","        similar_pairs = np.argwhere(similarity_matrix > similarity_max)\n","        for similar_pair in similar_pairs:\n","            if (similar_pair[0] >= similar_pair[1]):\n","                continue\n","            first_index = indices[similar_pair[0]]\n","            second_index = indices[similar_pair[1]]\n","            if processing.at[second_index, 'similar_to'] is not np.nan:\n","                continue\n","            processing.at[second_index, 'similar_to'] = processing.at[first_index, 'file_path']\n","            processing.at[second_index, 'similarity'] = similarity_matrix[similar_pair[0], similar_pair[1]]\n","        # Find the median values of similarity of each feature with other features\n","        similarity_medians = np.median(similarity_matrix, axis=0)\n","        # Find the upper bound of reliability\n","        similarity_median_min = np.mean(similarity_medians) - 3 * np.std(similarity_medians)\n","        # Enter the data on the difference into the process table\n","        processing.loc[indices, 'similarity_median_min'] = similarity_median_min\n","        processing.loc[indices, 'similarity_median'] = similarity_medians\n","        # Mark the signs that have very similar characteristics,\n","        # or are too different from other features\n","        processing.loc[indices, 'duplicated'] = processing.loc[indices, 'similarity'].notna()\n","        processing.loc[indices, 'different'] = processing.loc[indices, 'similarity_median'] < processing.loc[indices, 'similarity_median_min']\n","        processing.loc[indices, 'failed'] = processing.loc[indices, 'duplicated'] | processing.loc[indices, 'different']\n","        # Enter data into the results table\n","        train_cleaning.loc[emotion, 'duplicated_number'] = sum(processing.loc[indices, 'duplicated'])\n","        train_cleaning.loc[emotion, 'different_number'] = sum(processing.loc[indices, 'different'])\n","        train_cleaning.loc[emotion, 'failed_number'] = sum(processing.loc[indices, 'failed'])\n","        train_cleaning.loc[emotion, 'remain_number'] = emotion_size - train_cleaning.loc[emotion, 'failed_number']\n","        # First index of the next emotion\n","        index += emotion_size\n","\n","    processing = processing.convert_dtypes()"]},{"cell_type":"markdown","id":"37f8b129-7bb9-4aae-ba62-160f6f2c0588","metadata":{"id":"37f8b129-7bb9-4aae-ba62-160f6f2c0588"},"source":["#### Copying files of reliable face images of the training dataset"]},{"cell_type":"code","execution_count":null,"id":"047bb380-d66b-496d-a698-d1c3b444f5c4","metadata":{"id":"047bb380-d66b-496d-a698-d1c3b444f5c4"},"outputs":[],"source":["if not skip:\n","\n","    indices = processing.loc[~processing['failed']].index\n","    train_clean_features = train_features[indices]\n","    train_clean_labels = train_labels[indices]\n","    # Write arrays of features and labels to a file\n","    feature_path = (train_faces_clean_features_path / base_model_name).with_suffix('.npz')\n","    np.savez(feature_path, features=train_clean_features, labels=train_clean_labels)"]},{"cell_type":"markdown","id":"fcfcf6c8-af5b-4668-ba5f-de2c55bca77b","metadata":{"id":"fcfcf6c8-af5b-4668-ba5f-de2c55bca77b"},"source":["#### Copying files of reliable face images of the training dataset"]},{"cell_type":"code","execution_count":null,"id":"70caacb0-4c28-4aa5-b81f-98a051acfee9","metadata":{"id":"70caacb0-4c28-4aa5-b81f-98a051acfee9"},"outputs":[],"source":["if not skip:\n","\n","    with tqdm(processing, unit='file') as t:\n","        # We go through the iterations of augmentations\n","        for _, (file_path, failed) in processing[['file_path', 'failed']].iterrows():\n","            if not failed:\n","                source = train_faces_dataset_path / file_path\n","                dest = train_faces_clean_dataset_path / file_path\n","                shutil.copyfile(source, dest)\n","            # Updating the Progress Bar counter\n","            t.update()"]},{"cell_type":"markdown","id":"ddeeded9-ceef-4ddd-a6fc-4fae3487ec7c","metadata":{"id":"ddeeded9-ceef-4ddd-a6fc-4fae3487ec7c"},"source":["#### Saving the cleaned training dataset of face images and dataset of its features to a Google Drive archive"]},{"cell_type":"code","execution_count":null,"id":"19b4fb4d-ec6b-4ca1-a88c-3d09419159f7","metadata":{"id":"19b4fb4d-ec6b-4ca1-a88c-3d09419159f7"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.save_stage_processing(processing)\n","    shutil.make_archive(gd_proj_path / train_faces_clean_features_path.name, 'zip', train_faces_clean_features_path)\n","    shutil.make_archive(gd_proj_path / train_faces_clean_dataset_path.name, 'zip', train_faces_clean_dataset_path)"]},{"cell_type":"markdown","id":"45df321d-7bb7-4138-9c2e-bca5becfac4a","metadata":{"id":"45df321d-7bb7-4138-9c2e-bca5becfac4a"},"source":["#### Saving results to Google Drive"]},{"cell_type":"code","execution_count":null,"id":"97b3b111-2caa-42f9-8208-67521df5c2e4","metadata":{"id":"97b3b111-2caa-42f9-8208-67521df5c2e4"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.save_stage_result(train_cleaning)"]},{"cell_type":"markdown","id":"be5d966b-33ee-4b0d-b23c-4ff79effc8a8","metadata":{"id":"be5d966b-33ee-4b0d-b23c-4ff79effc8a8"},"source":["#### Fixing the completion of the stage"]},{"cell_type":"code","execution_count":null,"id":"775b26dd-c10f-46c4-b204-800c320d8448","metadata":{"id":"775b26dd-c10f-46c4-b204-800c320d8448"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.complete_stage()"]},{"cell_type":"markdown","id":"d4f466b5-dc8f-46b5-b705-443d9a9c5b67","metadata":{"id":"d4f466b5-dc8f-46b5-b705-443d9a9c5b67"},"source":["#### Loading results from Google Drive (done when step is skipped)"]},{"cell_type":"code","execution_count":null,"id":"c1503850-adda-491c-b315-541eeb343ad2","metadata":{"id":"c1503850-adda-491c-b315-541eeb343ad2"},"outputs":[],"source":["if skip:\n","\n","    if pipeline.is_stage_complete:\n","        train_cleaning = pipeline.load_stage_result().set_index('emotion')"]},{"cell_type":"markdown","id":"650e97e5-a89b-4952-93cb-7366ca8cb176","metadata":{"id":"650e97e5-a89b-4952-93cb-7366ca8cb176"},"source":["#### Output of results"]},{"cell_type":"code","execution_count":null,"id":"2061c8bb-79d6-4cca-af80-1b17929df78f","metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1743952734067,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"2061c8bb-79d6-4cca-af80-1b17929df78f","colab":{"base_uri":"https://localhost:8080/"},"outputId":"91e16c03-bd83-42b7-e60e-fa5312af0930"},"outputs":[{"output_type":"display_data","data":{"text/plain":["           duplicated_number  different_number  failed_number  remain_number\n","emotion                                                                     \n","anger                    816                68            877           6146\n","contempt                 102                35            137           2948\n","disgust                  465                28            493           2662\n","fear                    1349                26           1372           3672\n","happy                     93                61            154           5801\n","neutral                  172                80            250           6545\n","sad                     1155                52           1201           5539\n","surprise                 898                36            933           5390\n","uncertain                829                36            864           5063"],"text/html":["\n","  <div id=\"df-bb27e832-4939-4561-8d01-f057631d3a7a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>duplicated_number</th>\n","      <th>different_number</th>\n","      <th>failed_number</th>\n","      <th>remain_number</th>\n","    </tr>\n","    <tr>\n","      <th>emotion</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>anger</th>\n","      <td>816</td>\n","      <td>68</td>\n","      <td>877</td>\n","      <td>6146</td>\n","    </tr>\n","    <tr>\n","      <th>contempt</th>\n","      <td>102</td>\n","      <td>35</td>\n","      <td>137</td>\n","      <td>2948</td>\n","    </tr>\n","    <tr>\n","      <th>disgust</th>\n","      <td>465</td>\n","      <td>28</td>\n","      <td>493</td>\n","      <td>2662</td>\n","    </tr>\n","    <tr>\n","      <th>fear</th>\n","      <td>1349</td>\n","      <td>26</td>\n","      <td>1372</td>\n","      <td>3672</td>\n","    </tr>\n","    <tr>\n","      <th>happy</th>\n","      <td>93</td>\n","      <td>61</td>\n","      <td>154</td>\n","      <td>5801</td>\n","    </tr>\n","    <tr>\n","      <th>neutral</th>\n","      <td>172</td>\n","      <td>80</td>\n","      <td>250</td>\n","      <td>6545</td>\n","    </tr>\n","    <tr>\n","      <th>sad</th>\n","      <td>1155</td>\n","      <td>52</td>\n","      <td>1201</td>\n","      <td>5539</td>\n","    </tr>\n","    <tr>\n","      <th>surprise</th>\n","      <td>898</td>\n","      <td>36</td>\n","      <td>933</td>\n","      <td>5390</td>\n","    </tr>\n","    <tr>\n","      <th>uncertain</th>\n","      <td>829</td>\n","      <td>36</td>\n","      <td>864</td>\n","      <td>5063</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb27e832-4939-4561-8d01-f057631d3a7a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-bb27e832-4939-4561-8d01-f057631d3a7a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-bb27e832-4939-4561-8d01-f057631d3a7a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-01cd07f1-8924-450c-9de7-18b732066ff7\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-01cd07f1-8924-450c-9de7-18b732066ff7')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-01cd07f1-8924-450c-9de7-18b732066ff7 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_f20e0919-2649-47e2-97f0-5753756a962e\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train_cleaning')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_f20e0919-2649-47e2-97f0-5753756a962e button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('train_cleaning');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"train_cleaning","summary":"{\n  \"name\": \"train_cleaning\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"emotion\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"surprise\",\n          \"contempt\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"duplicated_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 466,\n        \"min\": 93,\n        \"max\": 1349,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          898,\n          102,\n          172\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"different_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19,\n        \"min\": 26,\n        \"max\": 80,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          35,\n          80,\n          68\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"failed_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 457,\n        \"min\": 137,\n        \"max\": 1372,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          933,\n          137,\n          250\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"remain_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1416,\n        \"min\": 2662,\n        \"max\": 6545,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          5390,\n          2948,\n          6545\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}],"source":["if pipeline.is_stage_complete:\n","    display(train_cleaning)"]},{"cell_type":"markdown","id":"858ec335-ba3e-4421-9cee-6e0bff560344","metadata":{"id":"858ec335-ba3e-4421-9cee-6e0bff560344","tags":[]},"source":["### Pipeline Execution Report"]},{"cell_type":"code","execution_count":null,"id":"8f6266af-5865-46b6-a144-66b24bab16b9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1743952734092,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"8f6266af-5865-46b6-a144-66b24bab16b9","outputId":"d1a62640-aa4c-4131-bcb4-298e2ab85e81"},"outputs":[{"output_type":"display_data","data":{"text/plain":["                                                                          params  \\\n","stage                                                                              \n","train_face_extraction          {'path': 'train_faces', 'classifier': 'haarcas...   \n","test_face_extraction           {'path': 'test_faces', 'classifier': 'haarcasc...   \n","train_face_feature_extraction  {'path': 'train_features', 'batch_size': 64, '...   \n","test_face_feature_extraction   {'path': 'test_features', 'batch_size': 64, 'b...   \n","train_cleaning                 {'features_path': 'train_clean_features', 'dat...   \n","\n","                              platform                  start_time  \\\n","stage                                                                \n","train_face_extraction            colab  2025-03-31 10:28:56.835055   \n","test_face_extraction             colab  2025-03-31 10:41:25.406164   \n","train_face_feature_extraction    colab  2025-03-31 10:42:47.223775   \n","test_face_feature_extraction     colab  2025-03-31 10:50:29.623191   \n","train_cleaning                   colab  2025-03-31 10:51:19.524240   \n","\n","                                              update_time     state  \n","stage                                                                \n","train_face_extraction          2025-03-31 10:41:25.374223  complete  \n","test_face_extraction           2025-03-31 10:42:47.195168  complete  \n","train_face_feature_extraction  2025-03-31 10:50:29.608719  complete  \n","test_face_feature_extraction   2025-03-31 10:51:19.507252  complete  \n","train_cleaning                 2025-03-31 10:56:25.587176  complete  "],"text/html":["\n","  <div id=\"df-3f3e9905-85bb-4bf6-bd78-0c0a4b070829\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>params</th>\n","      <th>platform</th>\n","      <th>start_time</th>\n","      <th>update_time</th>\n","      <th>state</th>\n","    </tr>\n","    <tr>\n","      <th>stage</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>train_face_extraction</th>\n","      <td>{'path': 'train_faces', 'classifier': 'haarcas...</td>\n","      <td>colab</td>\n","      <td>2025-03-31 10:28:56.835055</td>\n","      <td>2025-03-31 10:41:25.374223</td>\n","      <td>complete</td>\n","    </tr>\n","    <tr>\n","      <th>test_face_extraction</th>\n","      <td>{'path': 'test_faces', 'classifier': 'haarcasc...</td>\n","      <td>colab</td>\n","      <td>2025-03-31 10:41:25.406164</td>\n","      <td>2025-03-31 10:42:47.195168</td>\n","      <td>complete</td>\n","    </tr>\n","    <tr>\n","      <th>train_face_feature_extraction</th>\n","      <td>{'path': 'train_features', 'batch_size': 64, '...</td>\n","      <td>colab</td>\n","      <td>2025-03-31 10:42:47.223775</td>\n","      <td>2025-03-31 10:50:29.608719</td>\n","      <td>complete</td>\n","    </tr>\n","    <tr>\n","      <th>test_face_feature_extraction</th>\n","      <td>{'path': 'test_features', 'batch_size': 64, 'b...</td>\n","      <td>colab</td>\n","      <td>2025-03-31 10:50:29.623191</td>\n","      <td>2025-03-31 10:51:19.507252</td>\n","      <td>complete</td>\n","    </tr>\n","    <tr>\n","      <th>train_cleaning</th>\n","      <td>{'features_path': 'train_clean_features', 'dat...</td>\n","      <td>colab</td>\n","      <td>2025-03-31 10:51:19.524240</td>\n","      <td>2025-03-31 10:56:25.587176</td>\n","      <td>complete</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f3e9905-85bb-4bf6-bd78-0c0a4b070829')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3f3e9905-85bb-4bf6-bd78-0c0a4b070829 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3f3e9905-85bb-4bf6-bd78-0c0a4b070829');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-424a6f56-adbf-4f32-98f0-2414d51a39d8\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-424a6f56-adbf-4f32-98f0-2414d51a39d8')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-424a6f56-adbf-4f32-98f0-2414d51a39d8 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(pipeline\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"stage\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"test_face_extraction\",\n          \"train_cleaning\",\n          \"train_face_feature_extraction\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"params\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"{'path': 'test_faces', 'classifier': 'haarcascade_frontalface_default.xml', 'scale_factor': 1.1, 'min_neighbors': 3, 'flags': 6, 'face_min_ratio': 0.5, 'process_csv': 'test_face_extraction_process.csv', 'result_csv': 'test_face_extraction.csv'}\",\n          \"{'features_path': 'train_clean_features', 'dataset_path': 'train_clean_faces', 'process_csv': 'train_cleaning_process.csv', 'result_csv': 'train_cleaning.csv'}\",\n          \"{'path': 'train_features', 'batch_size': 64, 'buffer_size': 10}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"platform\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"colab\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start_time\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2025-03-31 10:41:25.406164\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"update_time\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2025-03-31 10:42:47.195168\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"state\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"complete\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}],"source":["display(pipeline.report)"]},{"cell_type":"markdown","id":"53d8c9c8-3457-4883-b80e-97951580774b","metadata":{"id":"53d8c9c8-3457-4883-b80e-97951580774b","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["## Model creation pipeline"]},{"cell_type":"markdown","id":"6281d05e-9f8f-431c-bb03-e48ac46de1f6","metadata":{"id":"6281d05e-9f8f-431c-bb03-e48ac46de1f6"},"source":["### Create/download pipeline from Google Drive"]},{"cell_type":"code","execution_count":null,"id":"993853c2-67cd-4695-ae8c-90956454967e","metadata":{"id":"993853c2-67cd-4695-ae8c-90956454967e"},"outputs":[],"source":["pipeline = Pipeline(config=MODEL_BUILDING_PIPELINE, proj_path=gd_proj_path, is_prev_complete=pipeline.is_complete, platform=platform)"]},{"cell_type":"markdown","id":"be9f8056-ef16-449c-8dc1-c4e2b5cab213","metadata":{"id":"be9f8056-ef16-449c-8dc1-c4e2b5cab213","tags":[]},"source":["### Selecting the best model on top"]},{"cell_type":"code","execution_count":null,"id":"c5bc50a3-3241-4f40-8293-b7d9e42d439d","metadata":{"id":"c5bc50a3-3241-4f40-8293-b7d9e42d439d"},"outputs":[],"source":["pipeline.next_stage()\n","skip = pipeline.is_stage_skipped\n","params = pipeline.stage_params\n","model_on_top_selection_path = Path(params['path'])\n","model_on_top_selection_logs_path = model_on_top_selection_path / 'logs'\n","model_on_top_selection_models_path = model_on_top_selection_path / 'models'\n","model_on_top_selection_predictions_path = model_on_top_selection_path / 'predictions'"]},{"cell_type":"markdown","id":"45f04178-fab9-44ab-8583-fc001dff55bb","metadata":{"id":"45f04178-fab9-44ab-8583-fc001dff55bb","tags":[]},"source":["#### Reading training dataset of features from archive on Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uYXIjCrofCGD"},"outputs":[],"source":["if not skip:\n","\n","    if not train_faces_clean_features_path.exists():\n","        train_faces_clean_features_path.mkdir()\n","        shutil.unpack_archive(\n","            gd_proj_path / train_faces_clean_features_path.with_suffix('.zip').name,\n","            train_faces_clean_features_path\n","        )"],"id":"uYXIjCrofCGD"},{"cell_type":"markdown","metadata":{"tags":[],"id":"K6jfUyMIfCGD"},"source":["#### Reading test dataset of features from archive on Google Drive"],"id":"K6jfUyMIfCGD"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dd9a898-43c7-4408-b42b-2e518f578aee"},"outputs":[],"source":["if not skip:\n","\n","    if not test_features_dataset_path.exists():\n","        test_features_dataset_path.mkdir()\n","        shutil.unpack_archive(\n","            gd_proj_path / test_features_dataset_path.with_suffix('.zip').name,\n","            test_features_dataset_path\n","        )"],"id":"0dd9a898-43c7-4408-b42b-2e518f578aee"},{"cell_type":"markdown","metadata":{"id":"lf5r1o657sUW"},"source":["#### Reading a test dataset of face images from the archive on Google Drive"],"id":"lf5r1o657sUW"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hl2uQhlZ7sUX"},"outputs":[],"source":["if not skip:\n","\n","    if not test_faces_dataset_path.exists():\n","        test_faces_dataset_path.mkdir()\n","        shutil.unpack_archive(\n","            gd_proj_path / test_faces_dataset_path.with_suffix('.zip').name,\n","            test_faces_dataset_path\n","        )"],"id":"Hl2uQhlZ7sUX"},{"cell_type":"markdown","metadata":{"id":"zGGIjKRE8IwH"},"source":["#### Getting a list of image files in the test dataset"],"id":"zGGIjKRE8IwH"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tr5TSsj88IwH"},"outputs":[],"source":["if not skip:\n","\n","    image_paths = [Path(file_path).relative_to(test_faces_dataset_path).as_posix()\n","                   for file_path in utils.image_dataset_from_directory(test_faces_dataset_path, labels=None, shuffle=False, batch_size=1).file_paths]"],"id":"Tr5TSsj88IwH"},{"cell_type":"markdown","id":"2abeb88e-ca1e-4457-8ba1-46ac7c92d5ca","metadata":{"id":"2abeb88e-ca1e-4457-8ba1-46ac7c92d5ca","tags":[]},"source":["#### Generate a list of possible hyperparameter combinations"]},{"cell_type":"code","execution_count":null,"id":"c4353216-2e6c-4894-8ede-7b870b0ca2bd","metadata":{"id":"c4353216-2e6c-4894-8ede-7b870b0ca2bd"},"outputs":[],"source":["if not skip:\n","\n","    # Using the enumeration method, we obtain a list of possible combinations of the regularization coefficient and the size of the fully connected layer\n","    dropout_rate_dense_units_combs = [(dropout_rate, dense_units) for dense_units in MODEL_ON_TOP_DENSE_UNITS for dropout_rate in MODEL_ON_TOP_DROPOUT_RATES]\n","    # Get a list of combinations\n","\n","    for dense_num in MODEL_ON_TOP_DENSE_NUMS:\n","        model_on_top_configs += set(itertools.permutations(dropout_rate_dense_units_combs, dense_num)).union(set(itertools.combinations_with_replacement(dropout_rate_dense_units_combs, dense_num)))\n","    model_on_top_configs = sorted(model_on_top_configs)\n","    model_on_top_config_strs = [', '.join([str(element) for element in model_on_top_config]) for model_on_top_config in model_on_top_configs]"]},{"cell_type":"markdown","id":"6ad54b09-98d5-4ee1-8f00-35819c5de420","metadata":{"id":"6ad54b09-98d5-4ee1-8f00-35819c5de420","tags":[]},"source":["#### Delete existing logs and model weights"]},{"cell_type":"code","execution_count":null,"id":"643ceda3-773c-47e8-904e-32fa8e445711","metadata":{"id":"643ceda3-773c-47e8-904e-32fa8e445711"},"outputs":[],"source":["if not skip:\n","\n","    shutil.rmtree(model_on_top_selection_path, ignore_errors=True)\n","    model_on_top_selection_path.mkdir()\n","    model_on_top_selection_models_path.mkdir()\n","    model_on_top_selection_predictions_path.mkdir()\n","    model_on_top_selection_logs_path.mkdir()\n","    for model_on_top_config_str in model_on_top_config_strs:\n","        (model_on_top_selection_models_path / model_on_top_config_str).mkdir()\n","        (model_on_top_selection_logs_path / model_on_top_config_str).mkdir()\n","        (model_on_top_selection_predictions_path / model_on_top_config_str).mkdir()"]},{"cell_type":"markdown","id":"cc8db036-519c-4059-ab08-80bf01a48479","metadata":{"id":"cc8db036-519c-4059-ab08-80bf01a48479","tags":[]},"source":["#### Creating a table of learning results"]},{"cell_type":"code","execution_count":null,"id":"1c3b841c-3d47-4bae-8e86-eb52f432df60","metadata":{"id":"1c3b841c-3d47-4bae-8e86-eb52f432df60"},"outputs":[],"source":["if not skip:\n","\n","    processing = pd.DataFrame(\n","        columns=[\n","            'model_on_top_config',\n","            'best_epoch',\n","            'loss',\n","            'metric',\n","            'submission',\n","            'public_score',\n","            'private_score',\n","            'score',\n","        ],\n","        dtype='float'\n","    )\n","    processing['model_on_top_config'] = model_on_top_config_strs\n","    processing.set_index('model_on_top_config', inplace=True)"]},{"cell_type":"markdown","id":"a61832b6-36b5-44c6-8fc2-06af7bc25ab7","metadata":{"id":"a61832b6-36b5-44c6-8fc2-06af7bc25ab7","tags":[]},"source":["#### Creating a results table"]},{"cell_type":"code","execution_count":null,"id":"1d026f8c-4d94-4792-9d84-bce2e3ea5806","metadata":{"id":"1d026f8c-4d94-4792-9d84-bce2e3ea5806"},"outputs":[],"source":["if not skip:\n","\n","    model_on_top_selection = pd.DataFrame(\n","        columns = [\n","            'model_on_top_config',\n","            'best_score',\n","        ],\n","        dtype=float\n","    )\n","    model_on_top_selection.set_index('model_on_top_config', inplace=True)"]},{"cell_type":"markdown","id":"95541198-b3a9-4e3d-88e7-afc53e5d1fa0","metadata":{"id":"95541198-b3a9-4e3d-88e7-afc53e5d1fa0","tags":[]},"source":["#### Creating training dataset"]},{"cell_type":"code","execution_count":null,"id":"ab5ee8f5-a04c-4058-8f49-9573100a406b","metadata":{"id":"ab5ee8f5-a04c-4058-8f49-9573100a406b"},"outputs":[],"source":["if not skip:\n","\n","    # Create a training and validating dataset\n","    train_dataset = build_feature_dataset(\n","        (train_faces_clean_features_path / base_model_name).with_suffix('.npz').as_posix(),\n","        batch_size=params['batch_size'],\n","        shuffle=True,\n","        seed=SEED\n","    )"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"UCDb7E-jfNOH"},"source":["#### Creating test dataset"],"id":"UCDb7E-jfNOH"},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKPSOEIBfNOH"},"outputs":[],"source":["if not skip:\n","\n","    # Create a training and validating dataset\n","    test_dataset = build_feature_dataset(\n","        (test_features_dataset_path / base_model_name).with_suffix('.npz').as_posix(),\n","        batch_size=params['batch_size'],\n","        shuffle=False,\n","        labeled=False\n","    )"],"id":"iKPSOEIBfNOH"},{"cell_type":"markdown","id":"6f7c4f6e-6fa4-4ce5-93fe-7dae396f6f2c","metadata":{"id":"6f7c4f6e-6fa4-4ce5-93fe-7dae396f6f2c"},"source":["#### Training of models on top"]},{"cell_type":"code","execution_count":null,"id":"47b1cd4f-e086-4fb3-b41d-fa03ed482628","metadata":{"id":"47b1cd4f-e086-4fb3-b41d-fa03ed482628"},"outputs":[],"source":["if not skip:\n","\n","    # Start logging for TensorBoard\n","    %tensorboard --logdir {model_on_top_selection_logs_path.as_posix()}"]},{"cell_type":"code","execution_count":null,"id":"f8131551-bc71-4304-808b-ec9e7e6e2563","metadata":{"id":"f8131551-bc71-4304-808b-ec9e7e6e2563"},"outputs":[],"source":["if not skip:\n","\n","    if isinstance(EMOTIONS, (list, tuple)):\n","        loss = 'sparse_categorical_crossentropy'\n","        metric = 'sparse_categorical_accuracy'\n","    else:\n","        loss = 'mean_absolute_error'\n","        metric = 'mean_absolute_percentage_error'\n","\n","    kaggle = Kaggle()\n","\n","    # Iterating through configurations\n","    with tqdm(model_on_top_configs, unit='config') as t:\n","\n","        for model_on_top_config, model_on_top_config_str in zip(model_on_top_configs, model_on_top_config_strs):\n","\n","            t.set_description(model_on_top_config_str)\n","\n","            # Create and compile the model on top\n","            model_on_top = build_model_on_top(\n","                feature_size=feature_size,\n","                config=model_on_top_config,\n","                training=True\n","            )\n","\n","            # Create an optimizer with exponential speed decay (every epoch)\n","            optimizer = getattr(optimizers, params['optimizer_name'])(learning_rate=params['initial_learning_rate'])\n","\n","            # Compile the model\n","            model_on_top.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n","\n","            # Reduced learning speed\n","            learning_rate_callback = LearningRateExpDecayScheduler(params['learning_rate_decay_rate'])\n","\n","            # Stop training if accuracy on test data stops growing\n","            earlystop_callback = callbacks.EarlyStopping(\n","                monitor=metric,\n","                patience=params['patience'],\n","                restore_best_weights=True\n","            )\n","\n","            # Displaying training graphs in TensorBoard\n","            tensorboard_callback = callbacks.TensorBoard(\n","                log_dir=model_on_top_selection_logs_path / model_on_top_config_str,\n","                update_freq=\"epoch\",\n","            )\n","\n","            # Update Progress Bar information\n","            epoch_end_callback = callbacks.LambdaCallback(\n","                on_epoch_end=lambda epoch, logs: t.set_description(\n","                    f\"{model_on_top_config_str} [#{epoch+1}]\"\n","                )\n","            )\n","\n","            # Train the model on top\n","            history = model_on_top.fit(\n","                train_dataset,\n","                epochs=params['epochs'],\n","                verbose=VERBOSE,\n","                callbacks=[learning_rate_callback, earlystop_callback, tensorboard_callback, epoch_end_callback]\n","            )\n","\n","            # model_on_top.set_weights(earlystop_callback.best_weights)\n","            model_on_top.save(model_on_top_selection_models_path / model_on_top_config_str / 'best_model.keras', include_optimizer=False)\n","\n","            # Using history, we find the best value of the accuracy metric on the test dataset and enter it into the process table\n","            processing.loc[model_on_top_config_str, 'best_epoch'] = earlystop_callback.best_epoch\n","            processing.loc[model_on_top_config_str, 'loss'] = history.history['loss'][earlystop_callback.best_epoch]\n","            processing.loc[model_on_top_config_str, 'metric'] = history.history[metric][earlystop_callback.best_epoch]\n","\n","            # Scoring the on top model on Kaggle\n","            predicts = model_on_top.predict(test_dataset, verbose=VERBOSE)\n","            if isinstance(EMOTIONS, (list, tuple)):\n","                labels = predicts.argmax(axis=1)\n","            else:\n","                errors = np.apply_along_axis(lambda a: np.linalg.norm(a - np.array(list(EMOTIONS.values())), axis=1), arr=predicts, axis=1)\n","                labels = errors.argmin(axis=1)\n","\n","            submission = f'{base_model_name}_{model_on_top_config_str}'.replace(', ', '_').replace('(', '').replace(')', '')\n","            file_path = model_on_top_selection_predictions_path / model_on_top_config_str / 'submission.csv'\n","\n","            df = pd.DataFrame(columns=['image_path', 'emotion'])\n","            df['image_path'] = image_paths\n","            df['emotion'] = [list(EMOTIONS)[label] for label in labels]\n","            df.to_csv(file_path, index=False)\n","\n","            kaggle.send_submission_files(descriptions=[submission], file_paths=[file_path])\n","            sleep(10)\n","            scores = kaggle.receive_submission_scores(descriptions=[submission]).loc[0, ['publicScore', 'privateScore']].convert_dtypes()\n","            scores['meanScore'] = (scores['publicScore'] + scores['privateScore']) / 2\n","            processing.loc[model_on_top_config_str, 'submission'] = submission\n","            processing.loc[model_on_top_config_str, ['public_score', 'private_score', 'score']] = scores.to_list()\n","\n","            # Updating the Progress Bar\n","            t.update()"]},{"cell_type":"markdown","id":"96654e10-251e-4b28-be5b-e30509ef6fd0","metadata":{"id":"96654e10-251e-4b28-be5b-e30509ef6fd0","tags":[]},"source":["#### Selecting the model on top configuration that provides the best accuracy on test data"]},{"cell_type":"code","execution_count":null,"id":"740ca667-a772-45b7-8b82-0771e07ebecd","metadata":{"id":"740ca667-a772-45b7-8b82-0771e07ebecd"},"outputs":[],"source":["if not skip:\n","\n","    processing['best_epoch'] = processing['best_epoch'].astype(int)\n","    best_model_on_top = processing['score'].idxmax()\n","    best_score = processing['score'].max()\n","    model_on_top_selection.loc[best_model_on_top] = [best_score]"]},{"cell_type":"markdown","id":"666d122b-c086-498e-94b5-dc997fdebb65","metadata":{"id":"666d122b-c086-498e-94b5-dc997fdebb65","tags":[]},"source":["#### Saving training logs and model weights in an archive on Google Drive"]},{"cell_type":"code","execution_count":null,"id":"ff0cdce2-1dda-450a-84f6-c527198dbc33","metadata":{"id":"ff0cdce2-1dda-450a-84f6-c527198dbc33"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.save_stage_processing(processing.convert_dtypes())\n","    shutil.make_archive(gd_proj_path / model_on_top_selection_path.name, 'zip', model_on_top_selection_path)"]},{"cell_type":"markdown","id":"73d13c09-a41a-4532-8a6d-1aabb01c24b2","metadata":{"id":"73d13c09-a41a-4532-8a6d-1aabb01c24b2","tags":[]},"source":["#### Saving learning results to Google Drive"]},{"cell_type":"code","execution_count":null,"id":"0a138404-9296-4647-992f-81cd0d17ba21","metadata":{"id":"0a138404-9296-4647-992f-81cd0d17ba21"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.save_stage_result(model_on_top_selection)"]},{"cell_type":"markdown","id":"48e1ee4b-66d6-4339-ae19-23c24909c349","metadata":{"id":"48e1ee4b-66d6-4339-ae19-23c24909c349","tags":[]},"source":["#### Fixing the completion of the stage"]},{"cell_type":"code","execution_count":null,"id":"ffd65a2f-5887-4a0e-ad1c-0ab23e5d7da7","metadata":{"id":"ffd65a2f-5887-4a0e-ad1c-0ab23e5d7da7"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.complete_stage()"]},{"cell_type":"markdown","id":"31aacead-4678-422c-a1c7-d622662b92b5","metadata":{"id":"31aacead-4678-422c-a1c7-d622662b92b5","tags":[]},"source":["#### Loading learning results from Google Drive (done when skipping a stage)"]},{"cell_type":"code","execution_count":null,"id":"36460b0f-21b0-434d-961c-ae49f40d1f9b","metadata":{"id":"36460b0f-21b0-434d-961c-ae49f40d1f9b"},"outputs":[],"source":["if skip:\n","\n","    if pipeline.is_stage_complete:\n","        model_on_top_selection = pipeline.load_stage_result().set_index('model_on_top_config')"]},{"cell_type":"markdown","id":"9e859942-6c96-4354-8113-231e15722829","metadata":{"id":"9e859942-6c96-4354-8113-231e15722829","tags":[]},"source":["#### Output of results"]},{"cell_type":"code","execution_count":null,"id":"667c6a40-f072-42dd-a697-411b11002443","metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1743952734203,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"667c6a40-f072-42dd-a697-411b11002443","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8358763e-df07-4001-bfa7-cd3b879980ee"},"outputs":[{"output_type":"display_data","data":{"text/plain":["'(0.0, 1024)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}}],"source":["if pipeline.is_stage_complete:\n","    model_on_top_config_str = model_on_top_selection.index[0]\n","    model_on_top_config_substrs = [element.split(', ') for element in model_on_top_config_str[1:-1].split('), (')]\n","    model_on_top_config = [(float(dropout_rate_str), int(dense_units_str)) for dropout_rate_str, dense_units_str in model_on_top_config_substrs]\n","    display(model_on_top_config_str)"]},{"cell_type":"markdown","id":"5b1ff925-ecef-4c4b-a06b-e001d296fc05","metadata":{"id":"5b1ff925-ecef-4c4b-a06b-e001d296fc05","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Fine tuning the model"]},{"cell_type":"code","execution_count":null,"id":"f1abf2e9-74a9-4bb6-9c94-091a12664489","metadata":{"id":"f1abf2e9-74a9-4bb6-9c94-091a12664489"},"outputs":[],"source":["pipeline.next_stage()\n","skip = pipeline.is_stage_skipped\n","params = pipeline.stage_params\n","model_fine_tuning_path = Path(params['path'])\n","model_fine_tuning_logs_path = model_fine_tuning_path / 'logs'\n","model_fine_tuning_models_path = model_fine_tuning_path / 'models'"]},{"cell_type":"markdown","id":"6130b97f-b60b-4d79-8c8f-68dad8061d8b","metadata":{"id":"6130b97f-b60b-4d79-8c8f-68dad8061d8b","tags":[]},"source":["#### Reading training dataset face images from Google Drive archive"]},{"cell_type":"code","execution_count":null,"id":"54981677-3d40-4e54-ad3a-de322f27733b","metadata":{"id":"54981677-3d40-4e54-ad3a-de322f27733b"},"outputs":[],"source":["if not skip:\n","\n","    if not train_faces_clean_dataset_path.exists():\n","        train_faces_clean_dataset_path.mkdir()\n","        shutil.unpack_archive(\n","            gd_proj_path / train_faces_clean_dataset_path.with_suffix('.zip').name,\n","            train_faces_clean_dataset_path\n","        )"]},{"cell_type":"markdown","id":"99985216-0a05-44b3-8a03-1e0febfb622c","metadata":{"id":"99985216-0a05-44b3-8a03-1e0febfb622c","tags":[]},"source":["#### Reading training logs and model weights of the model on top selection from an archive on Google Drive"]},{"cell_type":"code","execution_count":null,"id":"f91e2b54-9784-426a-8db1-e453bbd60028","metadata":{"id":"f91e2b54-9784-426a-8db1-e453bbd60028"},"outputs":[],"source":["if not skip:\n","\n","    if pipeline.is_stage_started and not model_on_top_selection_path.exists():\n","        model_on_top_selection_path.mkdir()\n","        shutil.unpack_archive(\n","            gd_proj_path / model_on_top_selection_path.with_suffix('.zip').name,\n","            model_on_top_selection_path\n","        )"]},{"cell_type":"markdown","id":"a3b37035-7b40-41a0-92b3-38a32bc57bad","metadata":{"id":"a3b37035-7b40-41a0-92b3-38a32bc57bad","tags":[]},"source":["#### Delete existing logs and model weights (done on first iteration)"]},{"cell_type":"code","execution_count":null,"id":"fd75c7f9-67b3-48df-b907-7bfa8bbd2dce","metadata":{"id":"fd75c7f9-67b3-48df-b907-7bfa8bbd2dce"},"outputs":[],"source":["if not skip:\n","\n","    # Delete the folder for logs and model weights on the first pass\n","    if pipeline.is_stage_started:\n","        shutil.rmtree(model_fine_tuning_path, ignore_errors=True)\n","\n","    # Create a folder for logs and model weights\n","    if not model_fine_tuning_path.exists():\n","        model_fine_tuning_path.mkdir()\n","        model_fine_tuning_logs_path.mkdir()\n","        model_fine_tuning_models_path.mkdir()"]},{"cell_type":"markdown","id":"372c4a0f-43e1-40d2-b27d-cb46ff522126","metadata":{"id":"372c4a0f-43e1-40d2-b27d-cb46ff522126","tags":[]},"source":["#### Loading logs and model weights from the previous iteration (done during the second and subsequent iterations)"]},{"cell_type":"code","execution_count":null,"id":"0a4accbe-914f-4150-947f-131f45cdf9fd","metadata":{"id":"0a4accbe-914f-4150-947f-131f45cdf9fd"},"outputs":[],"source":["if not skip:\n","\n","    if not pipeline.is_stage_started:\n","        # Read the archive of previous passes from the archive on Google Drive\n","        shutil.unpack_archive(\n","            gd_proj_path / model_fine_tuning_path.with_suffix('.zip').name,\n","            model_fine_tuning_path\n","        )"]},{"cell_type":"markdown","id":"63a295ed-a690-4554-814d-4a8fd6f471e7","metadata":{"id":"63a295ed-a690-4554-814d-4a8fd6f471e7","tags":[]},"source":["#### Creating a table of learning results"]},{"cell_type":"code","execution_count":null,"id":"66bb0f16-6600-444d-bd7d-db8d1ad4a438","metadata":{"id":"66bb0f16-6600-444d-bd7d-db8d1ad4a438"},"outputs":[],"source":["if not skip:\n","\n","    model_fine_tuning = pd.DataFrame(\n","        columns=[\n","            'base_model_name',\n","            'model_on_top_config',\n","            'best_epoch',\n","            'best_loss',\n","            'best_metric'\n","        ],\n","        dtype=np.int32\n","    )\n","    model_fine_tuning.loc[0, 'base_model_name'] = base_model_name\n","    model_fine_tuning.loc[0, 'model_on_top_config'] = model_on_top_config_str\n","    model_fine_tuning.set_index(['base_model_name', 'model_on_top_config'], inplace=True)"]},{"cell_type":"markdown","id":"3c7d8ba6-0163-4067-a427-be22b72ad91c","metadata":{"id":"3c7d8ba6-0163-4067-a427-be22b72ad91c","tags":[]},"source":["#### Initializing iteration"]},{"cell_type":"code","execution_count":null,"id":"c673d51d-04fc-4187-bf94-4d3db1642a24","metadata":{"id":"c673d51d-04fc-4187-bf94-4d3db1642a24"},"outputs":[],"source":["if not skip:\n","\n","    if pipeline.is_stage_started:\n","        # First iteration\n","        initial_epoch = 0\n","        initial_learning_rate = params['initial_learning_rate']\n","    else:\n","        # Second or subsequent iteration\n","        # Find the initial epoch number according to the protocol\n","        processing = pipeline.load_stage_processing().set_index('epoch')\n","        last_epoch = processing.index[-1]\n","        initial_epoch = last_epoch + 1\n","        initial_learning_rate = processing.loc[last_epoch, 'learning_rate']\n","        best_epoch = processing['metric'].idxmax()\n","        best_loss = processing.loc[best_epoch, 'loss']\n","        best_metric = processing.loc[best_epoch, 'metric']\n","        best_weights = models.load_model(model_fine_tuning_models_path / f'best_model.keras', safe_mode=False).get_weights()\n","        wait = last_epoch - best_epoch\n","\n","    # Calculate the last epoch number\n","    final_epoch = min(initial_epoch + params['epochs_per_run'], params['epochs'])"]},{"cell_type":"markdown","id":"0de9af2b-5079-44f5-876a-92eac43e3056","metadata":{"id":"0de9af2b-5079-44f5-876a-92eac43e3056","tags":[]},"source":["#### Creating training dataset"]},{"cell_type":"code","execution_count":null,"id":"e3390cb0-134e-4298-b3f2-a14d27577198","metadata":{"id":"e3390cb0-134e-4298-b3f2-a14d27577198"},"outputs":[],"source":["if not skip:\n","\n","    # If training is performed in Valence-Arousal mode, then emotion labels are replaced with pairs of values valence, arousal\n","    if isinstance(EMOTIONS, (list, tuple)):\n","        labels = 'inferred'\n","    else:\n","        file_paths = utils.image_dataset_from_directory(\n","            train_faces_clean_dataset_path, shuffle=False\n","        ).file_paths\n","        labels = [\n","            EMOTIONS[Path(file_path).parent.name]\n","            for file_path in file_paths\n","        ]\n","\n","    # Create a training dataset\n","    train_dataset = utils.image_dataset_from_directory(\n","        train_faces_clean_dataset_path,\n","        batch_size=params['batch_size'],\n","        labels=labels,\n","        # label_mode='int',\n","        image_size=(image_size, image_size),\n","        shuffle=True,\n","        seed=SEED,\n","        pad_to_aspect_ratio=True,\n","        verbose=VERBOSE\n","    )\n","\n","    # Initialize the pre-production of the next batch\n","    train_dataset = train_dataset.prefetch(buffer_size=params['buffer_size'])"]},{"cell_type":"markdown","id":"GpS8sUEUQ08i","metadata":{"id":"GpS8sUEUQ08i"},"source":["#### Create a model for fine-tuning (done in the first iteration)"]},{"cell_type":"code","execution_count":null,"id":"QvoRNdDDQ08j","metadata":{"id":"QvoRNdDDQ08j"},"outputs":[],"source":["if not skip:\n","\n","    if initial_epoch == 0:\n","        # Create the selected base model trained on the ImageNet dataset\n","        base_model = build_base_model(\n","            base_model_name,\n","            weights='imagenet',\n","            image_size=image_size,\n","            pooling=BASE_MODEL_POOLINGS,\n","            include_preprocess_input=True,\n","            training=True\n","        )\n","        base_model.trainable=False\n","        # Load the selected model on top\n","        model_on_top = models.load_model(\n","            model_on_top_selection_models_path / model_on_top_config_str / 'best_model.keras',\n","            safe_mode=False\n","        )\n","        # Create an augmentation model\n","        augment_model = build_augment_model(\n","            image_size=image_size,\n","            flip=params['flip'],\n","            rotation_factor=params['rotation_factor'],\n","            zoom_factor=params['zoom_factor'],\n","            contrast_factor=params['contrast_factor'],\n","            brightness_factor=params['brightness_factor'],\n","            training=True,\n","        )\n","        # We combine all the models created above into one\n","        model = build_model(augment_model=augment_model, base_model=base_model, model_on_top=model_on_top)\n","        model.trainable = False\n","        model.trainable = True"]},{"cell_type":"markdown","id":"71dde22b-aca8-4c4c-a856-f5ef51322e6c","metadata":{"id":"71dde22b-aca8-4c4c-a856-f5ef51322e6c","tags":[]},"source":["#### Load the latest model to train the model on top from the previous iteration (done on the second and subsequent iterations)"]},{"cell_type":"code","execution_count":null,"id":"b2019ba5-a46d-43d0-8e50-263f6b679dd7","metadata":{"id":"b2019ba5-a46d-43d0-8e50-263f6b679dd7"},"outputs":[],"source":["if not skip:\n","\n","    if initial_epoch > 0:\n","        model = models.load_model(model_fine_tuning_models_path / 'last_model.keras', safe_mode=False)"]},{"cell_type":"markdown","id":"e9d0803f-6953-4c27-b2c1-e1eada4142f8","metadata":{"id":"e9d0803f-6953-4c27-b2c1-e1eada4142f8","tags":[]},"source":["#### Compiling the model for fine-tuning"]},{"cell_type":"code","execution_count":null,"id":"881f9551-866a-4570-97f8-409980b3a302","metadata":{"id":"881f9551-866a-4570-97f8-409980b3a302"},"outputs":[],"source":["if not skip:\n","\n","    if isinstance(EMOTIONS, (list, tuple)):\n","        loss = 'sparse_categorical_crossentropy'\n","        metric = 'sparse_categorical_accuracy'\n","        mode = 'max'\n","    else:\n","        loss = 'mean_absolute_error'\n","        metric = 'mean_absolute_percentage_error'\n","        mode = 'min'\n","\n","    if initial_epoch == 0:\n","        # Create an optimizer with exponential speed decay (every epoch)\n","        optimizer = getattr(optimizers, params['optimizer_name'])(learning_rate=initial_learning_rate)\n","        # Compile the model\n","        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n","        model.summary()"]},{"cell_type":"markdown","id":"ac0afba6-304d-494d-9fb1-bba19568ce55","metadata":{"id":"ac0afba6-304d-494d-9fb1-bba19568ce55","tags":[]},"source":["#### Fine tuning the model"]},{"cell_type":"code","execution_count":null,"id":"42c25850-1dc9-469c-85d4-7667d471fa16","metadata":{"id":"42c25850-1dc9-469c-85d4-7667d471fa16"},"outputs":[],"source":["if not skip:\n","\n","    # Start logging for TensorBoard\n","    %tensorboard --logdir {model_fine_tuning_logs_path.as_posix()}"]},{"cell_type":"code","execution_count":null,"id":"23c6a72e-73cf-4a8d-84d6-f1602bf329c2","metadata":{"id":"23c6a72e-73cf-4a8d-84d6-f1602bf329c2"},"outputs":[],"source":["if not skip:\n","\n","    # Reduced learning speed\n","    learning_rate_callback = LearningRateExpDecayScheduler(params['learning_rate_decay_rate'])\n","\n","    # Stop training if loss on validation data stops decreasing\n","    if initial_epoch == 0:\n","        earlystop_callback = EarlyStoppingAtBestMetric(\n","            model=model,\n","            metric=metric,\n","            mode=mode,\n","            patience=params['patience'],\n","        )\n","    else:\n","        earlystop_callback = EarlyStoppingAtBestMetric(\n","            model=model,\n","            metric=metric,\n","            mode=mode,\n","            patience=params['patience'],\n","            best_epoch=best_epoch,\n","            best_loss=best_loss,\n","            best_metric=best_metric,\n","            best_weights=best_weights,\n","            wait=wait\n","        )\n","\n","    # Displaying training graphs in TensorBoard\n","    tensorboard_callback = callbacks.TensorBoard(\n","        log_dir=model_fine_tuning_logs_path,\n","        write_graph=True,\n","        update_freq=\"epoch\",\n","    )\n","\n","    # Progress Bar Update at the End of an Era\n","    def epoch_end(epoch, logs):\n","        t.set_description(\n","            f\"#{epoch+1}\"\n","        )\n","        t.update()\n","\n","    epoch_end_callback = callbacks.LambdaCallback(on_epoch_end=epoch_end)\n","\n","    # Train the model\n","    with tqdm(range(params['epochs']), initial=initial_epoch, desc='', unit='epoch') as t:\n","        history = model.fit(\n","            train_dataset,\n","            initial_epoch=initial_epoch,\n","            epochs=final_epoch,\n","            verbose=VERBOSE,\n","            callbacks=[learning_rate_callback, earlystop_callback, tensorboard_callback, epoch_end_callback],\n","        )\n","\n","        # Save the latest model\n","        model.save(model_fine_tuning_models_path / 'last_model.keras')\n","\n","        # Keep the weights of the best model\n","        model.set_weights(earlystop_callback.best_weights)\n","        model.save(model_fine_tuning_models_path / 'best_model.keras')"]},{"cell_type":"markdown","id":"e71d65b3-1709-4a6c-9b5c-06047b2ac765","metadata":{"id":"e71d65b3-1709-4a6c-9b5c-06047b2ac765","tags":[]},"source":["#### Saving detailed information about the learning process"]},{"cell_type":"code","execution_count":null,"id":"6be8b803-f54f-4679-84f8-f6cd6a22deab","metadata":{"id":"6be8b803-f54f-4679-84f8-f6cd6a22deab"},"outputs":[],"source":["if not skip:\n","\n","    history_df = pd.DataFrame(history.history, index=pd.Index(history.epoch, name='epoch'))\n","    history_df.rename(columns={metric: 'metric'}, inplace=True)\n","    if initial_epoch == 0:\n","        processing = history_df\n","    else:\n","        processing = pd.concat([processing, history_df], axis=0)\n","    pipeline.save_stage_processing(processing)\n","    shutil.make_archive(gd_proj_path / model_fine_tuning_path.name, 'zip', model_fine_tuning_path)"]},{"cell_type":"markdown","id":"f0b2b78e-d1a8-4e51-b49c-2ac3d034a37c","metadata":{"id":"f0b2b78e-d1a8-4e51-b49c-2ac3d034a37c","tags":[]},"source":["#### Saving results"]},{"cell_type":"code","execution_count":null,"id":"4c84711e-92b4-4026-8682-7c29e4af320e","metadata":{"id":"4c84711e-92b4-4026-8682-7c29e4af320e"},"outputs":[],"source":["if not skip:\n","\n","    if (final_epoch == params['epochs']) or model.stop_training:\n","        model_fine_tuning.loc[(base_model_name, model_on_top_config_str), 'best_epoch'] = earlystop_callback.best_epoch\n","        model_fine_tuning['best_epoch'] = model_fine_tuning['best_epoch'].astype(int)\n","        model_fine_tuning.loc[(base_model_name, model_on_top_config_str), 'best_loss'] = earlystop_callback.best_loss\n","        model_fine_tuning.loc[(base_model_name, model_on_top_config_str), 'best_metric'] = earlystop_callback.best_metric\n","        pipeline.save_stage_result(model_fine_tuning)\n","        # Save the final model\n","        final_model = models.Sequential(model.layers[1:])\n","        final_model.trainable = False\n","        final_model.save(gd_proj_path / 'final_model.keras', include_optimizer=False)"]},{"cell_type":"markdown","id":"82dcb42c-1137-4c0a-ab23-044f16bd64bf","metadata":{"id":"82dcb42c-1137-4c0a-ab23-044f16bd64bf","tags":[]},"source":["#### Committing completion of iteration/stage"]},{"cell_type":"code","execution_count":null,"id":"6312a96d-d61e-41e0-885b-4f331dcafb49","metadata":{"id":"6312a96d-d61e-41e0-885b-4f331dcafb49"},"outputs":[],"source":["if not skip:\n","\n","    if (final_epoch == params['epochs']) or model.stop_training:\n","        # Training completed\n","        pipeline.complete_stage()\n","    else:\n","        # Another iteration completed\n","        pipeline.complete_stage_run()"]},{"cell_type":"markdown","id":"8008a668-5418-45ff-b609-c949ae882a3d","metadata":{"id":"8008a668-5418-45ff-b609-c949ae882a3d","tags":[]},"source":["#### Loading results from Google Drive (done when step is skipped)"]},{"cell_type":"code","execution_count":null,"id":"caabb96d-5e61-4416-9fec-724f83fd1e73","metadata":{"id":"caabb96d-5e61-4416-9fec-724f83fd1e73"},"outputs":[],"source":["if skip:\n","\n","    if pipeline.is_stage_complete:\n","        model_fine_tuning = pipeline.load_stage_result().set_index(['base_model_name', 'model_on_top_config'])"]},{"cell_type":"markdown","id":"cfe4dece-8b56-48e0-89c1-bcfb85d7a45c","metadata":{"id":"cfe4dece-8b56-48e0-89c1-bcfb85d7a45c","tags":[]},"source":["#### Output of results"]},{"cell_type":"code","execution_count":null,"id":"f70ee62e-8874-4ff3-9c97-030c064caf95","metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1743952734312,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"f70ee62e-8874-4ff3-9c97-030c064caf95","colab":{"base_uri":"https://localhost:8080/"},"outputId":"46019417-c3f2-4290-832d-c84fb7fbf563"},"outputs":[{"output_type":"display_data","data":{"text/plain":["                                      best_epoch  best_loss  best_metric\n","base_model_name  model_on_top_config                                    \n","EfficientNetV2B0 (0.0, 1024)                  46   1.134255     0.591624"],"text/html":["\n","  <div id=\"df-d7018839-20f6-4345-85b9-3420e0493485\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>best_epoch</th>\n","      <th>best_loss</th>\n","      <th>best_metric</th>\n","    </tr>\n","    <tr>\n","      <th>base_model_name</th>\n","      <th>model_on_top_config</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>EfficientNetV2B0</th>\n","      <th>(0.0, 1024)</th>\n","      <td>46</td>\n","      <td>1.134255</td>\n","      <td>0.591624</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7018839-20f6-4345-85b9-3420e0493485')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d7018839-20f6-4345-85b9-3420e0493485 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d7018839-20f6-4345-85b9-3420e0493485');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_ea071481-e003-4df3-b7d4-355d387dfd95\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('model_fine_tuning')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_ea071481-e003-4df3-b7d4-355d387dfd95 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('model_fine_tuning');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"model_fine_tuning","summary":"{\n  \"name\": \"model_fine_tuning\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"best_epoch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 46,\n        \"max\": 46,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.1342546939849854,\n        \"max\": 1.1342546939849854,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.1342546939849854\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_metric\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.5916236639022827,\n        \"max\": 0.5916236639022827,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5916236639022827\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}],"source":["if pipeline.is_stage_complete:\n","    display(model_fine_tuning)"]},{"cell_type":"markdown","id":"0e52f9f0-c85f-4fde-925f-7a684eab1068","metadata":{"id":"0e52f9f0-c85f-4fde-925f-7a684eab1068","tags":[]},"source":["### Testing the model's operation"]},{"cell_type":"code","execution_count":null,"id":"71308f7f-e455-446c-ae9a-53014da0d257","metadata":{"id":"71308f7f-e455-446c-ae9a-53014da0d257"},"outputs":[],"source":["pipeline.next_stage()\n","skip = pipeline.is_stage_skipped\n","params = pipeline.stage_params\n","test_prediction_path = Path(params['path'])"]},{"cell_type":"markdown","id":"3c62ba70-6450-47be-b913-73528470da70","metadata":{"id":"3c62ba70-6450-47be-b913-73528470da70","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["#### Deleting existing prediction"]},{"cell_type":"code","execution_count":null,"id":"8e0435b9-ba6f-4e78-96fc-1848648b5643","metadata":{"id":"8e0435b9-ba6f-4e78-96fc-1848648b5643"},"outputs":[],"source":["if not skip:\n","\n","    shutil.rmtree(test_prediction_path, ignore_errors=True)\n","    test_prediction_path.mkdir()"]},{"cell_type":"code","execution_count":null,"id":"nSJh4E5TMBzX","metadata":{"id":"nSJh4E5TMBzX"},"outputs":[],"source":["if not skip:\n","\n","    test_dataset_path = Path(TEST_DATASET_PATH)\n","    if not test_dataset_path.exists():\n","        gdown.cached_download(\n","            url=TEST_DATASET_URL,\n","            path=f'temp.{TEST_DATASET_EXT}',\n","            postprocess=gdown.extractall,\n","            fuzzy=True)\n","        Path(f'temp.{TEST_DATASET_EXT}').unlink()"]},{"cell_type":"markdown","metadata":{"id":"QGfjSE6xbeIY"},"source":["#### Reading a test dataset of face images from the archive on Google Drive"],"id":"QGfjSE6xbeIY"},{"cell_type":"code","execution_count":null,"metadata":{"id":"RcsiXmlbbeIZ"},"outputs":[],"source":["if not skip:\n","\n","    if not test_faces_dataset_path.exists():\n","        test_faces_dataset_path.mkdir()\n","        shutil.unpack_archive(\n","            gd_proj_path / test_faces_dataset_path.with_suffix('.zip').name,\n","            test_faces_dataset_path\n","        )"],"id":"RcsiXmlbbeIZ"},{"cell_type":"markdown","id":"kV2xJhTjljbx","metadata":{"id":"kV2xJhTjljbx"},"source":["#### Getting a list of image files in the test dataset"]},{"cell_type":"code","execution_count":null,"id":"WxYeD00oljby","metadata":{"executionInfo":{"elapsed":341,"status":"ok","timestamp":1743952734661,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"WxYeD00oljby","colab":{"base_uri":"https://localhost:8080/"},"outputId":"964b6c86-260a-402e-c756-e627283c265c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5000 files.\n"]}],"source":["if not skip:\n","\n","    file_paths = [Path(file_path).relative_to(test_faces_dataset_path).as_posix()\n","                  for file_path in utils.image_dataset_from_directory(test_faces_dataset_path, label_mode=None, shuffle=False, batch_size=1).file_paths]"]},{"cell_type":"markdown","id":"f2b48e8f-7088-469f-97d4-c77e10cba1b6","metadata":{"id":"f2b48e8f-7088-469f-97d4-c77e10cba1b6","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["#### Creating a process table"]},{"cell_type":"code","execution_count":null,"id":"c8b94c43-51ba-4ba4-a204-8c0fd7dfe9e4","metadata":{"id":"c8b94c43-51ba-4ba4-a204-8c0fd7dfe9e4"},"outputs":[],"source":["if not skip:\n","\n","    if isinstance(EMOTIONS, (list, tuple)):\n","        processing = pd.DataFrame(\n","            columns=[\n","                'image_path',\n","                'probabilities',\n","                'label',\n","                'emotion'\n","            ]\n","        )\n","    else:\n","        processing = pd.DataFrame(\n","            columns=[\n","                'image_path',\n","                'valence',\n","                'arousal',\n","                'errors',\n","                'label',\n","                'emotion',\n","            ]\n","        )\n","    processing['image_path'] = file_paths\n","    processing.set_index('image_path', inplace=True)"]},{"cell_type":"markdown","id":"d124e738-5b43-4d5f-a43e-71f5abd1dc21","metadata":{"id":"d124e738-5b43-4d5f-a43e-71f5abd1dc21","tags":[]},"source":["#### Creating a results table"]},{"cell_type":"code","execution_count":null,"id":"94b3d376-136b-4d5b-824f-fc94e90e24ee","metadata":{"id":"94b3d376-136b-4d5b-824f-fc94e90e24ee"},"outputs":[],"source":["if not skip:\n","\n","    model_test = pd.DataFrame(\n","        columns=[\n","            'submission',\n","            'public_score',\n","            'private_score',\n","            'score',\n","        ]\n","    )\n","    model_test.set_index('submission', inplace=True)"]},{"cell_type":"markdown","id":"4d679e61-ecde-4074-8046-60f8bd4a9996","metadata":{"id":"4d679e61-ecde-4074-8046-60f8bd4a9996","tags":[]},"source":["#### Loading the final model"]},{"cell_type":"code","execution_count":null,"id":"aaf64169-b404-410b-accc-b517a5d8f926","metadata":{"executionInfo":{"elapsed":2730,"status":"ok","timestamp":1743952737448,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"aaf64169-b404-410b-accc-b517a5d8f926","colab":{"base_uri":"https://localhost:8080/"},"outputId":"418a6270-41fe-4801-8f29-0690aef6ca30"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n","  saveable.load_own_variables(weights_store.get(inner_path))\n"]}],"source":["if not skip:\n","\n","    model = models.load_model(gd_proj_path / 'final_model.keras', compile=False, safe_mode=False)"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"Y2YlLr4fqTH6"},"source":["#### Exporting the final model for deployment"],"id":"Y2YlLr4fqTH6"},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15767,"status":"ok","timestamp":1743952753216,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"colab":{"base_uri":"https://localhost:8080/"},"id":"2kW0Ea8IqTH7","outputId":"40fe46b7-1639-4a8a-97d7-d3ca182f6252"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saved artifact at '/content/drive/MyDrive/skillbox-computer-vision-project/final_model'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer')\n","Output Type:\n","  TensorSpec(shape=(None, 9), dtype=tf.float32, name=None)\n","Captures:\n","  139320894714320: TensorSpec(shape=(1, 1, 1, 3), dtype=tf.float32, name=None)\n","  139320894716048: TensorSpec(shape=(1, 1, 1, 3), dtype=tf.float32, name=None)\n","  139320894714128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894711632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894715472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894715664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894709328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894713552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894710288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894709136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894711440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894711824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894709520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894712784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894711056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894709712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894707984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894707216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894704912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894705488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894704528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894707408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894703760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894704720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894702608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894702992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894706448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894703952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894700624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894699088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894702224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894700432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894699856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894700240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894697744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894698512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894697552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894698896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894696784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894696208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894695056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894696976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894700048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894699472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894693520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894692368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894694096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894694864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894695824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894693328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894691600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894691024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894694672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894690064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894690832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894690448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894688720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894690640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894691792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894686800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894689296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894687952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894687184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894690256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894686224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894685648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894685840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894689872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894688336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894684624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894684048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894682896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894684432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894681360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894683856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894682512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894681744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894683280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894680784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894680400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894679248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894680976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894684240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894677328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894677712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894681168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894678096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894681936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894676560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894674832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894675984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894676176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894675600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894676752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894673488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894674256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894673296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894674640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894672528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894671952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894671568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894670608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894675792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894669840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894670032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894670992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894673680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894670416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894653456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894666512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894670224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894668240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894667664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894668048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894665168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894664016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894665744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894666896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894664784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894664400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894663248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894664976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894665552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894661520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894661712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894662864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894662096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894665936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894660560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894658448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894660176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894659792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894659600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894659216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894657872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894657296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894661904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894656144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894655952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894654800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894656336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894667280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894653264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894653648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894656528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894653072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894654032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895848272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895846736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894660752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894652496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895845200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895847888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895845392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895843856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895845584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895846928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895844624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895844240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895842896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895844816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895845008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895840976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895841360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895842512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895841744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895845776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895840208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895838672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895839632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895839824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895839440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895840400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895834064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895832720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895834448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895832912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895833488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895835792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895837136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895833680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895834256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895838480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895838096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895833872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895836176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895832144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895837904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895469712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895839056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895835984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895469520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895468176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895468752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895457040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895468560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895469904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895465872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895465296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895471056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895466064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895471440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895462800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895459728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895466256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895460112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895466448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895458576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895461840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895463952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895463184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895460304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895461072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895460880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895459344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895458960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895462032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895455312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895456464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895456272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895458000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895460688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319651342992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319237145616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319237148112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139322064418960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319237148304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139321432476560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139321432475984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319237148496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139321432476944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139321432476752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895473808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895476688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895473424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895471888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895472464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895477072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895478416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895472272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895473040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895479184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895479760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895478032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895477456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895474192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895481296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895481488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895480336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895480144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895479952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895471696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895483408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895486480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895483792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895482640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895484752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895484560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895486096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895484944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895485136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895487824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895485328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895783760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895785296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895784144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895786448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895787216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895785872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895786064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895785104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895785680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895788560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895794704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895788944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895788368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895790864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895790672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895794320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895796048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895791056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895797776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895795280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895795664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895798736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895789136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895798544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895796816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895798160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895795472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895817488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895816720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895817680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895818640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895817872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895831312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895819600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895819216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895820560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895817296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895831888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895823632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895821136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895819984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895821712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895818064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895822864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895826320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895824976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895822672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895824784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895823056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895827664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895826128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895825168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895824016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895827280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895826896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895828240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895827472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895825360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895831696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895829392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895828048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895825552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895829776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894834832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894835984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320895829200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894835216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894836944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894835024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894837328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894838288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894837520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894836176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894839248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894838864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894840208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894839440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894837136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894841168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894841360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894839632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894841744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894837712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894842896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894843664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894842512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894842320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894844624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894843088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894845008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894845968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894845200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894843856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894846928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894846544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894847888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894847120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894844816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894848848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894847312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894844240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139320894848464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319145136592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319145137552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319145138512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319145137744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319145136784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319145139472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319145137936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319145139856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319145140816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319145140048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319145138704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319145142160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319145143696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319145143120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  139319145144464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"]}],"source":["if not skip:\n","\n","    model.export(gd_proj_path / 'final_model')"],"id":"2kW0Ea8IqTH7"},{"cell_type":"markdown","id":"tdbFDfmYlG3P","metadata":{"id":"tdbFDfmYlG3P"},"source":["#### Creating test dataset"]},{"cell_type":"code","execution_count":null,"id":"cqV1bpgZlG3P","metadata":{"executionInfo":{"elapsed":151,"status":"ok","timestamp":1743952753368,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"cqV1bpgZlG3P","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b2349382-c076-405d-a0e0-472e73bfc3c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5000 files.\n"]}],"source":["if not skip:\n","\n","    test_dataset = utils.image_dataset_from_directory(\n","        test_faces_dataset_path,\n","        label_mode=None,\n","        shuffle=False,\n","        batch_size=params['batch_size'],\n","        image_size=(image_size, image_size),\n","        verbose=VERBOSE\n","    )\n","    # Initialize the pre-production of the next batch\n","    test_dataset = test_dataset.prefetch(buffer_size=params['buffer_size'])"]},{"cell_type":"markdown","id":"_hR6IkzSZe-G","metadata":{"id":"_hR6IkzSZe-G","tags":[]},"source":["#### Getting test predictions"]},{"cell_type":"code","execution_count":null,"id":"cctj6Z0QZe-G","metadata":{"executionInfo":{"elapsed":43804,"status":"ok","timestamp":1743952797173,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"cctj6Z0QZe-G","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0cba0103-325c-400d-ff80-b614f37f6ef5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m157/157\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 107ms/step\n"]}],"source":["if not skip:\n","\n","    predicts = model.predict(test_dataset, verbose=VERBOSE)\n","    if isinstance(EMOTIONS, (list, tuple)):\n","        labels = predicts.argmax(axis=1)\n","        processing['probabilities'] = predicts.tolist()\n","    else:\n","        errors = np.apply_along_axis(lambda a: np.linalg.norm(a - np.array(list(EMOTIONS.values())), axis=1), arr=predicts, axis=1)\n","        labels = errors.argmin(axis=1)\n","        processing[['valence', 'arousal']] = predicts.tolist()\n","        processing['errors'] = errors.tolist()\n","    processing['label'] = labels.tolist()\n","    processing['emotion'] = [list(EMOTIONS)[label] for label in labels]"]},{"cell_type":"markdown","id":"xs5QVQfqZOBe","metadata":{"id":"xs5QVQfqZOBe","tags":[]},"source":["#### Getting score from Kaggle"]},{"cell_type":"code","execution_count":null,"id":"pi2eBx55ZOBe","metadata":{"executionInfo":{"elapsed":14095,"status":"ok","timestamp":1743952811271,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"pi2eBx55ZOBe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1c7e691b-d861-467f-8092-3dd0d6973851"},"outputs":[{"output_type":"stream","name":"stdout","text":["[\"Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\", 'Successfully submitted to Skillbox. ÐÐ¸Ð¿Ð»Ð¾Ð¼Ð½Ð°Ñ ÑÐ°Ð±Ð¾ÑÐ° Ð¿Ð¾ ÐºÐ¾Ð¼Ð¿ÑÑÑÐµÑÐ½Ð¾Ð¼Ñ Ð·ÑÐµÐ½Ð¸Ñ']\n","Sended file model_test/submission.csv of submission test_EfficientNetV2B0_0.0_1024 to competition skillbox-computer-vision-project.\n","Received scores of submissions test_EfficientNetV2B0_0.0_1024 from competition skillbox-computer-vision-project.\n","         fileName                    date                     description  \\\n","0  submission.csv 2025-04-06 15:19:59.697  test_EfficientNetV2B0_0.0_1024   \n","\n","                      status  publicScore  privateScore  \n","0  SubmissionStatus.COMPLETE       0.5092          0.52  \n"]}],"source":["if not skip:\n","\n","    file_path = test_prediction_path / 'submission.csv'\n","    processing[['emotion']].to_csv(file_path, index=True)\n","    kaggle = Kaggle()\n","    submission = f'test_{base_model_name}_{model_on_top_config_str}'.replace(', ', '_').replace('(', '').replace(')', '')\n","    kaggle.send_submission_files(descriptions=[submission], file_paths=[file_path])\n","    sleep(10)\n","    scores = kaggle.receive_submission_scores(descriptions=[submission]).loc[0, ['publicScore', 'privateScore']].convert_dtypes()\n","    scores['meanScore'] = (scores['publicScore'] + scores['privateScore']) / 2"]},{"cell_type":"markdown","id":"XycYcxzScp5M","metadata":{"id":"XycYcxzScp5M","tags":[]},"source":["#### Saving predictions in an archive on Google Drive"]},{"cell_type":"code","execution_count":null,"id":"sOYGaF2Zcp5N","metadata":{"id":"sOYGaF2Zcp5N"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.save_stage_processing(processing)\n","    shutil.make_archive(gd_proj_path / test_prediction_path.name, 'zip', test_prediction_path)"]},{"cell_type":"markdown","id":"gl_-5XQycp5N","metadata":{"id":"gl_-5XQycp5N","tags":[]},"source":["#### Saving test results to Google Drive"]},{"cell_type":"code","execution_count":null,"id":"Tna5UKwNcp5N","metadata":{"id":"Tna5UKwNcp5N"},"outputs":[],"source":["if not skip:\n","\n","    model_test.loc[submission] = scores.to_list()\n","    pipeline.save_stage_result(model_test)"]},{"cell_type":"markdown","id":"cc751a19-5e6d-4158-b18d-de81c346093f","metadata":{"id":"cc751a19-5e6d-4158-b18d-de81c346093f","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["#### Fixing the completion of the stage"]},{"cell_type":"code","execution_count":null,"id":"50a30c6f-cc98-40ff-84eb-0023f553be09","metadata":{"id":"50a30c6f-cc98-40ff-84eb-0023f553be09"},"outputs":[],"source":["if not skip:\n","\n","    pipeline.complete_stage()"]},{"cell_type":"markdown","id":"239827ab-f903-4796-aec6-8e78ba866e95","metadata":{"id":"239827ab-f903-4796-aec6-8e78ba866e95","tags":[]},"source":["#### Loading test results from Google Drive (done when step is skipped)"]},{"cell_type":"code","execution_count":null,"id":"08b905b8-85f7-4103-b1fe-a766c7e76d22","metadata":{"id":"08b905b8-85f7-4103-b1fe-a766c7e76d22"},"outputs":[],"source":["if skip:\n","\n","    if pipeline.is_stage_complete:\n","        model_test = pipeline.load_stage_result().set_index('submission')"]},{"cell_type":"markdown","id":"c21c83c2-180e-43d0-ad86-250043236e11","metadata":{"id":"c21c83c2-180e-43d0-ad86-250043236e11","tags":[]},"source":["#### Output of results"]},{"cell_type":"code","execution_count":null,"id":"f1a6f808-e90b-44b2-a9cf-76f92b04bd97","metadata":{"executionInfo":{"elapsed":70,"status":"ok","timestamp":1743952814132,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"f1a6f808-e90b-44b2-a9cf-76f92b04bd97","colab":{"base_uri":"https://localhost:8080/","height":112},"outputId":"ac27a404-9aea-4122-f5b4-02e5b329e719"},"outputs":[{"output_type":"display_data","data":{"text/plain":["                                public_score  private_score   score\n","submission                                                         \n","test_EfficientNetV2B0_0.0_1024        0.5092           0.52  0.5146"],"text/html":["\n","  <div id=\"df-84af2244-794c-47b6-b85e-f5f9e41fe59c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>public_score</th>\n","      <th>private_score</th>\n","      <th>score</th>\n","    </tr>\n","    <tr>\n","      <th>submission</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>test_EfficientNetV2B0_0.0_1024</th>\n","      <td>0.5092</td>\n","      <td>0.52</td>\n","      <td>0.5146</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84af2244-794c-47b6-b85e-f5f9e41fe59c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-84af2244-794c-47b6-b85e-f5f9e41fe59c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-84af2244-794c-47b6-b85e-f5f9e41fe59c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_3905f283-e92b-4cb3-9cfc-9779b488bd5f\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('model_test')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_3905f283-e92b-4cb3-9cfc-9779b488bd5f button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('model_test');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"model_test","summary":"{\n  \"name\": \"model_test\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"submission\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"test_EfficientNetV2B0_0.0_1024\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"public_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.5092,\n        \"max\": 0.5092,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5092\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"private_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.52,\n        \"max\": 0.52,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.52\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.5146,\n        \"max\": 0.5146,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5146\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}],"source":["if pipeline.is_stage_complete:\n","\n","    display(model_test)"]},{"cell_type":"markdown","id":"cf7fcad3-d404-400b-a68c-af2868fc2f6b","metadata":{"id":"cf7fcad3-d404-400b-a68c-af2868fc2f6b","tags":[]},"source":["### Pipeline Execution Report"]},{"cell_type":"code","execution_count":null,"id":"23a96197-d618-4f82-8765-d4c160938612","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":87,"status":"ok","timestamp":1743952814221,"user":{"displayName":"ÐÐ»ÐµÐºÑÐµÐ¹ Ð¨ÐµÑÑÑÐ¾Ð±Ð¸ÑÐ¾Ð²","userId":"00010729100139413566"},"user_tz":-180},"id":"23a96197-d618-4f82-8765-d4c160938612","outputId":"13cd28b4-c139-46f5-9f2a-0b705118edf4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["                                                                   params  \\\n","stage                                                                       \n","model_on_top_selection  {'path': 'model_on_top_selection', 'batch_size...   \n","model_fine_tuning       {'path': 'model_fine_tuning', 'flip': 'horizon...   \n","model_test              {'path': 'model_test', 'batch_size': 32, 'buff...   \n","\n","                       platform                  start_time  \\\n","stage                                                         \n","model_on_top_selection    colab  2025-03-31 12:03:24.512772   \n","model_fine_tuning         colab  2025-03-31 15:50:49.324256   \n","model_test                colab  2025-04-06 15:18:54.527570   \n","\n","                                       update_time     state  \n","stage                                                         \n","model_on_top_selection  2025-03-31 15:10:55.404819  complete  \n","model_fine_tuning       2025-04-01 23:50:51.941296  complete  \n","model_test              2025-04-06 15:20:14.312456  complete  "],"text/html":["\n","  <div id=\"df-0889eeda-6415-43b7-a5a0-6904990a16f9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>params</th>\n","      <th>platform</th>\n","      <th>start_time</th>\n","      <th>update_time</th>\n","      <th>state</th>\n","    </tr>\n","    <tr>\n","      <th>stage</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>model_on_top_selection</th>\n","      <td>{'path': 'model_on_top_selection', 'batch_size...</td>\n","      <td>colab</td>\n","      <td>2025-03-31 12:03:24.512772</td>\n","      <td>2025-03-31 15:10:55.404819</td>\n","      <td>complete</td>\n","    </tr>\n","    <tr>\n","      <th>model_fine_tuning</th>\n","      <td>{'path': 'model_fine_tuning', 'flip': 'horizon...</td>\n","      <td>colab</td>\n","      <td>2025-03-31 15:50:49.324256</td>\n","      <td>2025-04-01 23:50:51.941296</td>\n","      <td>complete</td>\n","    </tr>\n","    <tr>\n","      <th>model_test</th>\n","      <td>{'path': 'model_test', 'batch_size': 32, 'buff...</td>\n","      <td>colab</td>\n","      <td>2025-04-06 15:18:54.527570</td>\n","      <td>2025-04-06 15:20:14.312456</td>\n","      <td>complete</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0889eeda-6415-43b7-a5a0-6904990a16f9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-0889eeda-6415-43b7-a5a0-6904990a16f9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-0889eeda-6415-43b7-a5a0-6904990a16f9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-2d97bddb-526b-4563-861d-f91f404e597c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2d97bddb-526b-4563-861d-f91f404e597c')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-2d97bddb-526b-4563-861d-f91f404e597c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(pipeline\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"stage\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"model_on_top_selection\",\n          \"model_fine_tuning\",\n          \"model_test\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"params\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"{'path': 'model_on_top_selection', 'batch_size': 64, 'optimizer_name': 'Adam', 'initial_learning_rate': 0.0001, 'learning_rate_decay_rate': 0.96, 'epochs': 20, 'patience': 3, 'process_csv': 'model_on_top_selection.csv', 'result_csv': 'selected_model_on_top.csv'}\",\n          \"{'path': 'model_fine_tuning', 'flip': 'horizontal', 'rotation_factor': 0.1, 'zoom_factor': 0.2, 'contrast_factor': 0.2, 'brightness_factor': 0.2, 'batch_size': 32, 'buffer_size': 100, 'optimizer_name': 'Adam', 'initial_learning_rate': 1e-05, 'learning_rate_decay_rate': 0.96, 'epochs': 50, 'epochs_per_run': 10, 'patience': 3, 'process_csv': 'model_fine_tuning.csv', 'result_csv': 'model.csv'}\",\n          \"{'path': 'model_test', 'batch_size': 32, 'buffer_size': 100, 'process_csv': 'test_prediction.csv', 'result_csv': 'test_scoring.csv'}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"platform\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"colab\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start_time\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2025-03-31 12:03:24.512772\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"update_time\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2025-03-31 15:10:55.404819\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"state\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"complete\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}],"source":["display(pipeline.report)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["c8c435e5-e282-4672-9beb-21a58c36c377","c6b687cf-1660-4b61-9f55-8ad76741bd8f","9f34e881-22b1-4f68-a64e-65471d6f61bb","18df7a45-badd-4a32-990e-7be0fb2e020d","dfdf7f1a-d8f3-48e7-80b0-7cf3af9d4c95","93df5dcf-949a-45a8-8a05-6fe9833fa6dc","30284640-db8e-4595-8406-078ce392c1ce","1b21d034-8b1f-4f10-a4cd-15838e29dec0","677f97c5-6cee-4267-9987-d42c91c15b9a","227088bf-3158-4743-9576-4309423063c8","697900d4-4b99-462d-bb60-7e769741ebf1","cb7e54b8-4425-469c-a758-ca804591944f","143536d9-de32-4b6a-a5e3-5c53d735ac96","4921d950-5597-43f6-a43d-4a75b72133c3","06340386-6d60-4172-bdb2-495db9218690","cf515457-5acc-47de-af87-6d6732ccb0ff","d66f6ba7-a65a-42b0-8c0a-63379c2c3a36","bfe3225a-5008-404f-8c7e-44c69bcd306b","1d83193c-d48e-4eca-99a5-cef351571004","560cd465-c102-495e-8adf-21a7905294b0","7e9d668c-4519-4bf9-8ccb-7aa0aad52790","0334070a-7597-4fe6-a5bf-b9dd280d403a","71e8b9fa-2ab1-45ca-bfc2-9e97c3d0d0bf","42129f3a-49a7-4b43-8eeb-fe3e6b2ea4d7","a0021e34-5451-4f7d-95c4-6b03309a4804","1c81247e-fff2-4f6f-800a-0d5bd75f9349","90c099c3-0632-43e8-9ab9-9e6cf6075ab7","1ef00c1f-b995-4508-a6f4-86ba97b8cfe6","e4058e38-8303-408b-848c-b4d1b493199b","baae31ad-bfc7-4076-9233-6cbeedf282d6","576d4baa-f44d-4f7d-8497-227f3d810a08","0232b656-9d83-4a79-ac00-65728a4b3103","050f80aa-3b70-4a1c-9bff-36ad1fd0c53c","7cac7ab6-e01a-4c09-a9a9-7bf5a0281078","bd034179-b872-4aa0-82c1-d92ed4e42630","66708202-6473-4142-8ad5-996314b611a7","8ae04f10-dcdd-4241-b85a-71cd473c27c5","3bc02057-9107-4d3b-a636-a204a5cc0f61","c8a972ba-5d9a-4938-9e1d-e99b36485b25","9679b7bf-fa52-4ee7-8b11-97f0eab24832","7892e9b3-193e-4586-bad0-133892c5e4ea","bc70230f-4975-4f73-9c50-008f2571c98e","f39bc921-ef0c-4d8b-a107-4f6a75256c7c","3d7ef9a4-15f3-4238-a231-07ce5abcd56a","0d641810-4ff6-4f9c-81fa-9dafc639bf22","5aac3fa5-9a53-4895-ad70-ad32138ccf14","fd582638-bf43-4e65-9a22-3566e658dd42","a1ad6275-b43a-4ef8-9880-b87b47e9b003","2d0a1a30-da90-4b29-b4df-dc7accdc9317","d3dd5015-b3d5-4c69-ab98-c65e40ee06fb","9ac8d13b-5894-4e7d-ac6d-01c1b6b3836e","232db2d2-fd02-4fad-a900-3a7dc367e09d","a6e27849-160e-41ac-b544-6d281747d793","b4d200ab-1133-44f6-92dc-f224ff026796","790cae71-a262-4517-8e83-b6b20e9367e0","2d0fe797-abc5-4080-baed-03e727b13711","0c47a32c-a1b8-4593-93e3-dc14085875db","772ad2a0-4fb8-4396-9e15-eba9054ebb15","352cbcf6-5f3c-4327-a919-3af93174b4fc","b1cf985a-8780-46e3-8a53-2d38ef92bacd","7a5dedce-2a51-4a47-91b4-9396a338ba63","9a6efee7-cdb1-4f1b-853f-ec3ecd4c3e58","53e10a4e-fcc6-4687-8305-a147613fb0f0","06904f1e-bfdd-486c-b2db-0056b905e19d","c45acefb-a4a0-4373-b9a7-a10ec1325e44","fddc6791-43dc-4a85-806c-6b8bde73250c","a7def33f-2c37-4be8-a98e-03bb5ebfeec5","86705c20-0dc9-47e0-b22f-93fd3499442d","26ede546-a656-4e54-985d-2dd06f00415d","d3667748-2e2f-46ce-9f44-8e76e55d35c2","49535f24-5339-4b56-aca7-eed8f7d4e106","34d97867-34c4-46ce-b73f-9d3d94c075f5","b11ecc93-57cb-4945-914f-703b346bb27d","7e67b15d-7bde-4ca0-8703-b159ce98ed10","b4742b72-8f81-4716-9eae-83184104a280","37f8b129-7bb9-4aae-ba62-160f6f2c0588","fcfcf6c8-af5b-4668-ba5f-de2c55bca77b","ddeeded9-ceef-4ddd-a6fc-4fae3487ec7c","45df321d-7bb7-4138-9c2e-bca5becfac4a","be5d966b-33ee-4b0d-b23c-4ff79effc8a8","d4f466b5-dc8f-46b5-b705-443d9a9c5b67","53d8c9c8-3457-4883-b80e-97951580774b","6281d05e-9f8f-431c-bb03-e48ac46de1f6","45f04178-fab9-44ab-8583-fc001dff55bb","K6jfUyMIfCGD","lf5r1o657sUW","zGGIjKRE8IwH","6ad54b09-98d5-4ee1-8f00-35819c5de420","cc8db036-519c-4059-ab08-80bf01a48479","a61832b6-36b5-44c6-8fc2-06af7bc25ab7","95541198-b3a9-4e3d-88e7-afc53e5d1fa0","UCDb7E-jfNOH","96654e10-251e-4b28-be5b-e30509ef6fd0","666d122b-c086-498e-94b5-dc997fdebb65","73d13c09-a41a-4532-8a6d-1aabb01c24b2","48e1ee4b-66d6-4339-ae19-23c24909c349","31aacead-4678-422c-a1c7-d622662b92b5","5b1ff925-ecef-4c4b-a06b-e001d296fc05","6130b97f-b60b-4d79-8c8f-68dad8061d8b","99985216-0a05-44b3-8a03-1e0febfb622c","a3b37035-7b40-41a0-92b3-38a32bc57bad","372c4a0f-43e1-40d2-b27d-cb46ff522126","63a295ed-a690-4554-814d-4a8fd6f471e7","3c7d8ba6-0163-4067-a427-be22b72ad91c","0de9af2b-5079-44f5-876a-92eac43e3056","GpS8sUEUQ08i","71dde22b-aca8-4c4c-a856-f5ef51322e6c","e9d0803f-6953-4c27-b2c1-e1eada4142f8","e71d65b3-1709-4a6c-9b5c-06047b2ac765","f0b2b78e-d1a8-4e51-b49c-2ac3d034a37c","82dcb42c-1137-4c0a-ab23-044f16bd64bf","8008a668-5418-45ff-b609-c949ae882a3d","cfe4dece-8b56-48e0-89c1-bcfb85d7a45c","0e52f9f0-c85f-4fde-925f-7a684eab1068","QGfjSE6xbeIY","kV2xJhTjljbx","f2b48e8f-7088-469f-97d4-c77e10cba1b6","d124e738-5b43-4d5f-a43e-71f5abd1dc21","4d679e61-ecde-4074-8046-60f8bd4a9996","tdbFDfmYlG3P","gl_-5XQycp5N","cc751a19-5e6d-4158-b18d-de81c346093f","cf7fcad3-d404-400b-a68c-af2868fc2f6b"],"gpuType":"T4","provenance":[{"file_id":"1at-2juJjPYOpVijAy3U4qmYGCpUfmqDP","timestamp":1743350547059}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":5}